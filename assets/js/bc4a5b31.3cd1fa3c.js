"use strict";(self.webpackChunkbyconity=self.webpackChunkbyconity||[]).push([[1842],{9613:(t,e,a)=>{a.d(e,{Zo:()=>u,kt:()=>k});var n=a(9496);function r(t,e,a){return e in t?Object.defineProperty(t,e,{value:a,enumerable:!0,configurable:!0,writable:!0}):t[e]=a,t}function l(t,e){var a=Object.keys(t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(t);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),a.push.apply(a,n)}return a}function i(t){for(var e=1;e<arguments.length;e++){var a=null!=arguments[e]?arguments[e]:{};e%2?l(Object(a),!0).forEach((function(e){r(t,e,a[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(a,e))}))}return t}function o(t,e){if(null==t)return{};var a,n,r=function(t,e){if(null==t)return{};var a,n,r={},l=Object.keys(t);for(n=0;n<l.length;n++)a=l[n],e.indexOf(a)>=0||(r[a]=t[a]);return r}(t,e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(t);for(n=0;n<l.length;n++)a=l[n],e.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(t,a)&&(r[a]=t[a])}return r}var s=n.createContext({}),m=function(t){var e=n.useContext(s),a=e;return t&&(a="function"==typeof t?t(e):i(i({},e),t)),a},u=function(t){var e=m(t.components);return n.createElement(s.Provider,{value:e},t.children)},p="mdxType",d={inlineCode:"code",wrapper:function(t){var e=t.children;return n.createElement(n.Fragment,{},e)}},c=n.forwardRef((function(t,e){var a=t.components,r=t.mdxType,l=t.originalType,s=t.parentName,u=o(t,["components","mdxType","originalType","parentName"]),p=m(a),c=r,k=p["".concat(s,".").concat(c)]||p[c]||d[c]||l;return a?n.createElement(k,i(i({ref:e},u),{},{components:a})):n.createElement(k,i({ref:e},u))}));function k(t,e){var a=arguments,r=e&&e.mdxType;if("string"==typeof t||r){var l=a.length,i=new Array(l);i[0]=c;var o={};for(var s in e)hasOwnProperty.call(e,s)&&(o[s]=e[s]);o.originalType=t,o[p]="string"==typeof t?t:r,i[1]=o;for(var m=2;m<l;m++)i[m]=a[m];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},3585:(t,e,a)=>{a.r(e),a.d(e,{assets:()=>s,contentTitle:()=>i,default:()=>d,frontMatter:()=>l,metadata:()=>o,toc:()=>m});var n=a(4778),r=(a(9496),a(9613));const l={title:"Import from Kafka",sidebar_position:3,tags:["Docs"]},i="Importing Data",o={unversionedId:"data-import/import-methods/import-from-kafka",id:"data-import/import-methods/import-from-kafka",title:"Import from Kafka",description:"Importing from Kafka",source:"@site/docs/data-import/import-methods/import-from-kafka.mdx",sourceDirName:"data-import/import-methods",slug:"/data-import/import-methods/import-from-kafka",permalink:"/docs/data-import/import-methods/import-from-kafka",draft:!1,editUrl:"https://github.com/ByConity/byconity.github.io/tree/main/docs/data-import/import-methods/import-from-kafka.mdx",tags:[{label:"Docs",permalink:"/docs/tags/docs"}],version:"current",sidebarPosition:3,frontMatter:{title:"Import from Kafka",sidebar_position:3,tags:["Docs"]},sidebar:"tutorialSidebar",previous:{title:"Import from HDFS",permalink:"/docs/data-import/import-methods/import-from-hdfs"},next:{title:"Direct Access via MySQL",permalink:"/docs/data-import/import-methods/direct-access-via-mysql"}},s={},m=[{value:"Importing from Kafka",id:"importing-from-kafka",level:2},{value:"User Guide",id:"user-guide",level:3},{value:"Table Creation",id:"table-creation",level:4},{value:"Virtual Column Support",id:"virtual-column-support",level:4},{value:"Setting Description",id:"setting-description",level:4},{value:"Modifying Consumption Parameters",id:"modifying-consumption-parameters",level:4},{value:"Manual Start/Stop Consumption",id:"manual-startstop-consumption",level:4},{value:"Resetting Offset",id:"resetting-offset",level:4},{value:"Reset to Special Positions",id:"reset-to-special-positions",level:4},{value:"Operation and Maintenance Manual",id:"operation-and-maintenance-manual",level:3},{value:"Common Consumption Performance Tuning",id:"common-consumption-performance-tuning",level:4},{value:"Adjusting max-block-size",id:"adjusting-max-block-size",level:4},{value:"Adjusting num_consumers",id:"adjusting-num_consumers",level:4},{value:"System Tables for Troubleshooting",id:"system-tables-for-troubleshooting",level:4},{value:"Field Description",id:"field-description",level:4},{value:"Event Table (event_table)",id:"event-table-event_table",level:5},{value:"Event Type Description (event_type)",id:"event-type-description-event_type",level:3},{value:"Consumption Status Table: system.cnch_kafka_tables",id:"consumption-status-table-systemcnch_kafka_tables",level:3},{value:"Common Queries for Troubleshooting Consumption Issues",id:"common-queries-for-troubleshooting-consumption-issues",level:3}],u={toc:m},p="wrapper";function d(t){let{components:e,...a}=t;return(0,r.kt)(p,(0,n.Z)({},u,a,{components:e,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"importing-data"},"Importing Data"),(0,r.kt)("h2",{id:"importing-from-kafka"},"Importing from Kafka"),(0,r.kt)("p",null,"CnchKafka is a table engine developed by ByConity based on the community ClickHouse Kafka table engine and optimized for cloud-native architectures. It efficiently and rapidly imports user data in real-time from Apache Kafka into ByConity. Its design and implementation are tailored to cloud-native infrastructures while enhancing certain functionalities beyond the community's implementations."),(0,r.kt)("h3",{id:"user-guide"},"User Guide"),(0,r.kt)("h4",{id:"table-creation"},"Table Creation"),(0,r.kt)("p",null,"Creating a CnchKafka consumer table is similar to creating a Kafka table in the community version. It requires configuring Kafka data sources and consumption parameters through the ",(0,r.kt)("inlineCode",{parentName:"p"},"Setting")," parameter. Here's an example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"CREATE TABLE kafka_test.cnch_kafka_consume\n(\n    `i` Int64,\n    `ts` DateTime\n)\nENGINE = CnchKafka()\nSETTINGS\nkafka_broker_list = '10.10.10.10:9092',  -- replace with your own broker list\nkafka_topic_list = 'my_kafka_test_topic', -- topic name to subscribe\nkafka_group_name = 'hansome_boy_consume_group', -- your consumer-group name\nkafka_format = 'JSONEachRow', -- always be json\nkafka_row_delimiter = '\\n', -- always be \\n\nkafka_num_consumers = 1\n")),(0,r.kt)("p",null,"(Please refer to the section below for explanations of the ",(0,r.kt)("inlineCode",{parentName:"p"},"Setting")," parameters and support for additional parameters.)"),(0,r.kt)("p",null,"Due to the design of Kafka consumption, three tables are required. Therefore, you need to create two additional tables."),(0,r.kt)("p",null,"First, create a storage table (using CnchMergeTree as an example):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"CREATE TABLE kafka_test.cnch_store_kafka\n(\n    `i` Int64,\n    `ts` DateTime\n)\nENGINE = CnchMergeTree\nPARTITION BY toDate(ts)\nORDER BY ts\n")),(0,r.kt)("p",null,"Finally, create a materialized view table (only after successfully creating the Kafka table and storage table):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"CREATE MATERIALIZED VIEW kafka_test.cnch_kafka_view\nTO kafka_test.cnch_store_kafka\n(\n    `i` Int64,\n    `ts` DateTime\n)\nAS\nSELECT * -- you can add virtual columns here if you need\nFROM kafka_test.cnch_kafka_consume\n")),(0,r.kt)("p",null,"Once you have the necessary permissions for the corresponding topic, consumption will automatically start executing after all three tables are created."),(0,r.kt)("h4",{id:"virtual-column-support"},"Virtual Column Support"),(0,r.kt)("p",null,"Sometimes, there's a business need to access metadata from Kafka messages (e.g., message partition, offset, etc.). In such cases, you can use the virtual columns feature to meet this requirement. Virtual columns don't need to be specified during table creation; they are inherent properties of the table engine. They can be included in the SELECT statement of the VIEW table and stored in the underlying table (provided the underlying table has the corresponding columns added):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"SELECT\n    _topic,    -- String\n    _partition,    -- UInt64\n    _key,    -- String\n    _offset,    -- UInt64\n    _content,  -- String: complete message content\n    *    -- Normal columns can be expanded with *, but virtual columns cannot\nFROM kafka_test.cnch_kafka_consume\n")),(0,r.kt)("h4",{id:"setting-description"},"Setting Description"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Table of Parameters")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Parameter Name")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Type")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Required/Default")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Description")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kafka_cluster / kafka_broker_list"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Required"),(0,r.kt)("td",{parentName:"tr",align:null},"Internal Kafka cluster for the company; for community version of Kafka, use the ",(0,r.kt)("inlineCode",{parentName:"td"},"kafka_broker_list")," parameter.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kafka_topic_list"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Required"),(0,r.kt)("td",{parentName:"tr",align:null},"Can be multiple, separated by commas.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kafka_group_name"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Required"),(0,r.kt)("td",{parentName:"tr",align:null},"Consumer group name.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kafka_format"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Required"),(0,r.kt)("td",{parentName:"tr",align:null},"Message format; currently, JSONEachRow is the most commonly used.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kafka_row_delimiter"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"'\\0'"),(0,r.kt)("td",{parentName:"tr",align:null},"Generally used as '\\n'.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kafka_num_consumers"),(0,r.kt)("td",{parentName:"tr",align:null},"UInt64"),(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},"Number of consumers, recommended not to exceed the maximum number of partitions in the topic.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kafka_max_block_size"),(0,r.kt)("td",{parentName:"tr",align:null},"UInt64"),(0,r.kt)("td",{parentName:"tr",align:null},"65536"),(0,r.kt)("td",{parentName:"tr",align:null},"Write block_size, with an upper limit of 1M.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kafka_max_poll_interval_ms"),(0,r.kt)("td",{parentName:"tr",align:null},"Milliseconds"),(0,r.kt)("td",{parentName:"tr",align:null},"7500"),(0,r.kt)("td",{parentName:"tr",align:null},"The maximum time to poll from the broker during each iteration.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kafka_schema"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},'""'),(0,r.kt)("td",{parentName:"tr",align:null},"Schema file setup parameter, set in the format of filename + colon + message name. E.g., ",(0,r.kt)("inlineCode",{parentName:"td"},"schema.proto:MyMessage"),".")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kafka_format_schema_path"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},'""'),(0,r.kt)("td",{parentName:"tr",align:null},"Remote schema file path (excluding filename) setup parameter, currently only supports hdfs. (If this parameter is not set, it will read from the default path set in the configuration file).")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kafka_protobuf_enable_multiple_message"),(0,r.kt)("td",{parentName:"tr",align:null},"bool"),(0,r.kt)("td",{parentName:"tr",align:null},"true"),(0,r.kt)("td",{parentName:"tr",align:null},"Set to true to indicate that multiple protobuf messages can be read from a single kafka message, separated by their respective lengths.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kafka_protobuf_default_length_parser"),(0,r.kt)("td",{parentName:"tr",align:null},"bool"),(0,r.kt)("td",{parentName:"tr",align:null},"false"),(0,r.kt)("td",{parentName:"tr",align:null},"Only effective when ",(0,r.kt)("inlineCode",{parentName:"td"},"kafka_protobuf_enable_multiple_message")," is true: true indicates that the message header has a variable record length; false indicates that a fixed 8 bytes are used as the header record length.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kafka_extra_librdkafka_config"),(0,r.kt)("td",{parentName:"tr",align:null},"Json format string"),(0,r.kt)("td",{parentName:"tr",align:null},'""'),(0,r.kt)("td",{parentName:"tr",align:null},"Other parameters supported by rdkafka, typically used for authentication (More params refer to ",(0,r.kt)("a",{parentName:"td",href:"https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md#:~:text=see%20dedicated%20API-,ssl.ca.location,-*"},"here"),").")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"cnch_vw_write"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},'"vw_write"'),(0,r.kt)("td",{parentName:"tr",align:null},"Configures consumption using Virtual WareHouse, where consumer tasks will be scheduled to execute on the configured Virtual Warehouse nodes.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kafka_cnch_schedule_mode"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},'"random"'),(0,r.kt)("td",{parentName:"tr",align:null},"Scheduling strategy adopted by ConsumeManager when scheduling consumer tasks. Currently supports: random, hash, and least_consumers; if using independent vw or with more than 10 consumers, least_consumers is recommended.")))),(0,r.kt)("h4",{id:"modifying-consumption-parameters"},"Modifying Consumption Parameters"),(0,r.kt)("p",null,"Supports quickly modifying Setting parameters through the ALTER command, primarily used for adjusting consumer count and other parameters to enhance consumption capacity."),(0,r.kt)("p",null,"Command:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"ALTER TABLE <cnch_kafka_name> MODIFY SETTING <name1> = <value1>, <name2> = <value2>\n")),(0,r.kt)("p",null,"Executing this command will automatically restart the consumption task."),(0,r.kt)("h4",{id:"manual-startstop-consumption"},"Manual Start/Stop Consumption"),(0,r.kt)("p",null,"In some scenarios, users may need to manually stop consumption and then resume it later; we provide corresponding SYSTEM commands to achieve this:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"SYSTEM START/STOP/RESTART CONSUME <cnch_kafka_name>\n")),(0,r.kt)("p",null,"Note: The START/STOP commands will persist the corresponding state to the Catalog. Therefore, after executing the STOP command, if START is not executed, even if the service is restarted, the consumption task will not resume."),(0,r.kt)("h4",{id:"resetting-offset"},"Resetting Offset"),(0,r.kt)("p",null,"Since CnchKafka manages and saves offsets internally, we have implemented SYSTEM commands for users to reset offsets when needed. Specifically, the following three methods are supported:"),(0,r.kt)("h4",{id:"reset-to-special-positions"},"Reset to Special Positions"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"By latest/earliest position")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'SYSTEM RESET CONSUME OFFSET \'{"database_name":"XXX", "table_name": "XXX", "offset_value":-1}\'\n')),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Possible values for special positions:")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    enum Offset {\n        OFFSET_BEGINNING = -2,\n        OFFSET_END = -1,\n        OFFSET_STORED = -1000,\n        OFFSET_INVALID = -1001\n    };\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Reset by timestamp")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'SYSTEM RESET CONSUME OFFSET \'{"database_name":"XXX", "table_name": "XXX", "timestamp":1646125258000}\'\n')),(0,r.kt)("p",null,"The timestamp value should be within the valid data retention period on the Kafka side and should be in milliseconds."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Specify offset value")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'system reset consume offset \'{"database_name":"XXX", "table_name": "XXX", "topic_name": "XXX", "offset_values":[{"partition":0, "offset":100}, {"partition":10, "offset":101}]}\'\n')),(0,r.kt)("p",null,"This is less common and used to specify a specific offset value for a particular topic partition."),(0,r.kt)("h3",{id:"operation-and-maintenance-manual"},"Operation and Maintenance Manual"),(0,r.kt)("h4",{id:"common-consumption-performance-tuning"},"Common Consumption Performance Tuning"),(0,r.kt)("p",null,"When there is a persistent lag in consumption, it is usually due to insufficient consumption capacity. By default, CnchKafka creates one consumer with a maximum block size of 65536 for writing in a single consumption. When the consumption capacity is insufficient, it is recommended to adjust the consumer and block-size parameters. Refer to the ",(0,r.kt)("strong",{parentName:"p"},"Modifying Consumption Parameters")," section above for adjustment methods."),(0,r.kt)("h4",{id:"adjusting-max-block-size"},"Adjusting max-block-size"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"This parameter directly affects consumption memory usage, and a larger value requires more memory. For consumption tables with larger individual data sizes, adjust this parameter carefully to avoid exceeding memory limits (the upper limit is 1M)."),(0,r.kt)("li",{parentName:"ul"},'When users have low requirements for data latency and have a large amount of data with sufficient memory, they can simultaneously adjust this parameter and the "kafka_max_poll_interval_ms" parameter to increase the consumption time for each round, making each written part larger, reducing MERGE pressure, and improving query performance.')),(0,r.kt)("h4",{id:"adjusting-num_consumers"},"Adjusting num_consumers"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"The upper limit for this parameter is the number of partitions corresponding to the consumed topic."),(0,r.kt)("li",{parentName:"ul"},"When there is no lag in consumption, it is recommended to minimize this parameter (i.e., avoid increasing it unnecessarily) to reduce resource usage, avoid generating too many small parts during consumption, reduce MERGE pressure, and facilitate queries.")),(0,r.kt)("h4",{id:"system-tables-for-troubleshooting"},"System Tables for Troubleshooting"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Consumption Events: cnch_system.cnch_kafka_log")),(0,r.kt)("p",null,"The kafka_log table records basic consumption events. To enable it, configure the kafka_log item in config.xml (required for both server and worker), and restart the service for the changes to take effect."),(0,r.kt)("p",null,"The kafka_log is written by the consumer task in the Virtual Warehouse and aggregated in real-time into the global cnch_system.cnch_kafka_log table, allowing you to view consumption records for all consumption tables from the server side."),(0,r.kt)("h4",{id:"field-description"},"Field Description"),(0,r.kt)("h5",{id:"event-table-event_table"},"Event Table (event_table)"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Column Name")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Type")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Description")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"event_type"),(0,r.kt)("td",{parentName:"tr",align:null},"Enum8"),(0,r.kt)("td",{parentName:"tr",align:null},"See the table below for details")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"event_date"),(0,r.kt)("td",{parentName:"tr",align:null},"Date"),(0,r.kt)("td",{parentName:"tr",align:null},"The date when the event occurred. It is a partition field and is recommended to be included in every query.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"event_time"),(0,r.kt)("td",{parentName:"tr",align:null},"DateTime"),(0,r.kt)("td",{parentName:"tr",align:null},"The time when the event occurred, in seconds")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"duration_ms"),(0,r.kt)("td",{parentName:"tr",align:null},"UInt64"),(0,r.kt)("td",{parentName:"tr",align:null},"The duration of the event, in milliseconds")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"cnch_database"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"The name of the CnchKafka database")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"cnch_table"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"The name of the CnchKafka table")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"database"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"The database name of the consumer task (currently the same as cnch_database)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"table"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"The table name of the consumer task (usually suffixed with a timestamp and consumer ID based on cnch_table)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"consumer"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"The consumer ID")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"metric"),(0,r.kt)("td",{parentName:"tr",align:null},"UInt64"),(0,r.kt)("td",{parentName:"tr",align:null},"The number of consumed rows")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"has_error"),(0,r.kt)("td",{parentName:"tr",align:null},"UInt8"),(0,r.kt)("td",{parentName:"tr",align:null},"1 indicates an exception; 0 indicates no exception")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"exception"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Exception description")))),(0,r.kt)("h3",{id:"event-type-description-event_type"},"Event Type Description (event_type)"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"UInt8 Value")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"String Value")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Description")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},"POLL"),(0,r.kt)("td",{parentName:"tr",align:null},"The metric indicates the number of data items consumed, and duration_ms covers the entire consumption process including WRITE time.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2"),(0,r.kt)("td",{parentName:"tr",align:null},"PARSE_ERROR"),(0,r.kt)("td",{parentName:"tr",align:null},"The metric represents the number of consumption entries with parsing errors. If multiple entries have errors, only one will be selected for printing.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"3"),(0,r.kt)("td",{parentName:"tr",align:null},"WRITE"),(0,r.kt)("td",{parentName:"tr",align:null},"The metric represents the number of data rows written, and duration_ms is basically equivalent to the data persistence time.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"4"),(0,r.kt)("td",{parentName:"tr",align:null},"EXCEPTION"),(0,r.kt)("td",{parentName:"tr",align:null},"Exceptions during the consumption process. Common examples include: authentication exceptions, data persistence failures, and VIEW SELECT execution failures.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"5"),(0,r.kt)("td",{parentName:"tr",align:null},"EMPTY_MESSAGE"),(0,r.kt)("td",{parentName:"tr",align:null},"The number of empty messages.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"6"),(0,r.kt)("td",{parentName:"tr",align:null},"FILTER"),(0,r.kt)("td",{parentName:"tr",align:null},"Data filtered during the write phase.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"7"),(0,r.kt)("td",{parentName:"tr",align:null},"COMMIT"),(0,r.kt)("td",{parentName:"tr",align:null},"Final transaction commit record. Only this record indicates successful data writing and can be used as a data audit standard.")))),(0,r.kt)("h3",{id:"consumption-status-table-systemcnch_kafka_tables"},"Consumption Status Table: system.cnch_kafka_tables"),(0,r.kt)("p",null,"The kafka_tables records the real-time status of CnchKafka tables, which are memory tables by default;"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Field Name")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Data Type")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Description")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"database"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Database name")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"name"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Kafka table name")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"uuid"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Unique identifier UUID for the kafka table")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"kafka_cluster"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Kafka cluster")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"topics"),(0,r.kt)("td",{parentName:"tr",align:null},"Array(String)"),(0,r.kt)("td",{parentName:"tr",align:null},"List of consumed topics")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"consumer_group"),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"Associated consumer group")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"num_consumers"),(0,r.kt)("td",{parentName:"tr",align:null},"UInt32"),(0,r.kt)("td",{parentName:"tr",align:null},"The current number of actively executing consumers")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"consumer_tables"),(0,r.kt)("td",{parentName:"tr",align:null},"Array(String)"),(0,r.kt)("td",{parentName:"tr",align:null},"Data table names corresponding to each consumer")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"consumer_hosts"),(0,r.kt)("td",{parentName:"tr",align:null},"Array(String)"),(0,r.kt)("td",{parentName:"tr",align:null},"Execution nodes assigned to each consumer")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"consumer_partitions"),(0,r.kt)("td",{parentName:"tr",align:null},"Array(String)"),(0,r.kt)("td",{parentName:"tr",align:null},"Partitions assigned to each consumer for consumption")))),(0,r.kt)("h3",{id:"common-queries-for-troubleshooting-consumption-issues"},"Common Queries for Troubleshooting Consumption Issues"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Checking the real-time status of CnchKafka consumption tables")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT * FROM system.cnch_kafka_tables\nWHERE database = '<database_name>' AND name = '<cnch_kafka_table>'\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Viewing recent consumption records")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT * FROM cnch_system.cnch_kafka_log\nWHERE event_date = today()\n AND cnch_database = '<database_name>'\n AND cnch_table = '<cnch_kafka_table>'\n AND event_time > now() - 600 -- Last 10 minutes\nORDER BY event_time\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Summarizing consumption records for the current day by hour")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT\n toHour(event_time) as hour,\n sumIf(metric, event_type = 'POLL') as poll_rows,\n sumIf(metric, event_type = 'PARSE_ERROR') as error_rows,\n sumIf(metric, event_type = 'COMMIT') as commit_rows\nFROM cnch_system.cnch_kafka_log\nWHERE event_date = today()\n AND cnch_database = '<database_name>'\n AND cnch_table = '<cnch_kafka_table>'\nGROUP BY hour\nORDER BY hour\n")))}d.isMDXComponent=!0}}]);