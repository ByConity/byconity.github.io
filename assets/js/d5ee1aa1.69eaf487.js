"use strict";(self.webpackChunkbyconity=self.webpackChunkbyconity||[]).push([[5295],{9613:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>h});var n=a(9496);function l(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){l(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function r(e,t){if(null==e)return{};var a,n,l=function(e,t){if(null==e)return{};var a,n,l={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(l[a]=e[a]);return l}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(l[a]=e[a])}return l}var s=n.createContext({}),u=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=u(e.components);return n.createElement(s.Provider,{value:t},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,l=e.mdxType,o=e.originalType,s=e.parentName,p=r(e,["components","mdxType","originalType","parentName"]),c=u(a),d=l,h=c["".concat(s,".").concat(d)]||c[d]||m[d]||o;return a?n.createElement(h,i(i({ref:t},p),{},{components:a})):n.createElement(h,i({ref:t},p))}));function h(e,t){var a=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var o=a.length,i=new Array(o);i[0]=d;var r={};for(var s in t)hasOwnProperty.call(t,s)&&(r[s]=t[s]);r.originalType=e,r[c]="string"==typeof e?e:l,i[1]=r;for(var u=2;u<o;u++)i[u]=a[u];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},5820:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>r,toc:()=>u});var n=a(4778),l=(a(9496),a(9613));const o={title:"Import Data",tags:["Docs"]},i="Import Data",r={unversionedId:"basic-guide/data-import",id:"basic-guide/data-import",title:"Import Data",description:"Document Type: Tutorial",source:"@site/docs/basic-guide/data-import.mdx",sourceDirName:"basic-guide",slug:"/basic-guide/data-import",permalink:"/docs/basic-guide/data-import",draft:!1,editUrl:"https://github.com/ByConity/byconity.github.io/tree/main/docs/basic-guide/data-import.mdx",tags:[{label:"Docs",permalink:"/docs/tags/docs"}],version:"current",frontMatter:{title:"Import Data",tags:["Docs"]},sidebar:"tutorialSidebar",previous:{title:"Export Data",permalink:"/docs/basic-guide/data-export"},next:{title:"Data Type",permalink:"/docs/basic-guide/data-type"}},s={},u=[{value:"Divide by scene",id:"divide-by-scene",level:2},{value:"Supported data formats",id:"supported-data-formats",level:2},{value:"Streaming import data (local files and memory data)",id:"streaming-import-data-local-files-and-memory-data",level:2},{value:"Method 1:",id:"method-1",level:3},{value:"Method 2:",id:"method-2",level:3},{value:"Method 3:",id:"method-3",level:3},{value:"Import data from external storage",id:"import-data-from-external-storage",level:2},{value:"Import data using kakfa",id:"import-data-using-kakfa",level:2},{value:"Function definition",id:"function-definition",level:3},{value:"Implementation principle",id:"implementation-principle",level:3},{value:"Overview",id:"overview",level:4},{value:"KafkaConsumeManager",id:"kafkaconsumemanager",level:4},{value:"KafkaConsumer",id:"kafkaconsumer",level:4},{value:"Exactly-Once",id:"exactly-once",level:4},{value:"Automatic fault tolerance implementation",id:"automatic-fault-tolerance-implementation",level:4},{value:"user&#39;s guidance",id:"users-guidance",level:3},{value:"Create table",id:"create-table",level:4},{value:"Virtual column support",id:"virtual-column-support",level:4},{value:"Setting Parameter Description",id:"setting-parameter-description",level:4},{value:"Modify consumption parameters",id:"modify-consumption-parameters",level:4},{value:"Manually start and stop consumption",id:"manually-start-and-stop-consumption",level:4},{value:"reset offset",id:"reset-offset",level:4},{value:"A is reset to a special position: latest position/start position",id:"a-is-reset-to-a-special-position-latest-positionstart-position",level:5},{value:"B reset by timestamp",id:"b-reset-by-timestamp",level:5},{value:"C specifies the offset specific value",id:"c-specifies-the-offset-specific-value",level:5},{value:"Operation Manual",id:"operation-manual",level:3},{value:"Common consumer performance tuning",id:"common-consumer-performance-tuning",level:4},{value:"Adjust max-block-size",id:"adjust-max-block-size",level:5},{value:"Adjust num_consumers",id:"adjust-num_consumers",level:5},{value:"System table for auxiliary troubleshooting",id:"system-table-for-auxiliary-troubleshooting",level:4},{value:"Consumption event: cnch_system.cnch_kafka_log",id:"consumption-event-cnch_systemcnch_kafka_log",level:5},{value:"Consumption status: system.cnch_kafka_tables",id:"consumption-status-systemcnch_kafka_tables",level:5},{value:"Common troubleshooting consumption exception records",id:"common-troubleshooting-consumption-exception-records",level:4},{value:"View the real-time status of CnchKafka consumption table",id:"view-the-real-time-status-of-cnchkafka-consumption-table",level:5},{value:"View recent consumption records",id:"view-recent-consumption-records",level:5},{value:"Statistics of consumption records of the day by hour",id:"statistics-of-consumption-records-of-the-day-by-hour",level:5},{value:"Import external data through Spark",id:"import-external-data-through-spark",level:2},{value:"Access external data sources through ByConity",id:"access-external-data-sources-through-byconity",level:2},{value:"Create Tables in MySQL",id:"create-tables-in-mysql",level:4},{value:"Create MySQL table in ByConity",id:"create-mysql-table-in-byconity",level:4},{value:"Test connection mysql table in ByConity",id:"test-connection-mysql-table-in-byconity",level:4},{value:"Hive",id:"hive",level:3},{value:"use:",id:"use",level:3},{value:"Example 1: Build a complete set of hive tables",id:"example-1-build-a-complete-set-of-hive-tables",level:4},{value:"Example 2: Building a subset of hive tables",id:"example-2-building-a-subset-of-hive-tables",level:4},{value:"Example 3: hive bucket table construction",id:"example-3-hive-bucket-table-construction",level:4},{value:"SETTINGS",id:"settings",level:4},{value:"Operation Manual",id:"operation-manual-1",level:4},{value:"Direct write mode tuning",id:"direct-write-mode-tuning",level:2}],p={toc:u},c="wrapper";function m(e){let{components:t,...o}=e;return(0,l.kt)(c,(0,n.Z)({},p,o,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"import-data"},"Import Data"),(0,l.kt)("p",null,"Document Type: Tutorial"),(0,l.kt)("p",null,"Document structure: tutorial purpose, pre-preparation, step-by-step explanation of principles & examples, and related document recommendations;"),(0,l.kt)("p",null,"Summary:"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"Which methods are supported to import data, and are there any suggestions on the import method (such as directly writing and merging part files after attaching)"),(0,l.kt)("li",{parentName:"ol"},"How to connect to upstream supported data sources"),(0,l.kt)("li",{parentName:"ol"},"How to check the data import status"),(0,l.kt)("li",{parentName:"ol"},"What to do for common import errors")),(0,l.kt)("h1",{id:"supported-data-sources"},"Supported data sources"),(0,l.kt)("p",null,"Various data import schemes are provided, and different data import methods can be selected for different data sources."),(0,l.kt)("h2",{id:"divide-by-scene"},"Divide by scene"),(0,l.kt)("p",null,"Data source"),(0,l.kt)("p",null,"Import method"),(0,l.kt)("p",null,"local files"),(0,l.kt)("p",null,"Streaming import data (local files and memory data)"),(0,l.kt)("p",null,"HDFS"),(0,l.kt)("p",null,"Import from external strorage data"),(0,l.kt)("p",null,"Kafka"),(0,l.kt)("p",null,"Import data from Kafka"),(0,l.kt)("p",null,"Import from spark"),(0,l.kt)("p",null,"Import external data from Spark"),(0,l.kt)("p",null,"Mysql\u3001Hive"),(0,l.kt)("p",null,"Access to external data sources via ByConity"),(0,l.kt)("h2",{id:"supported-data-formats"},"Supported data formats"),(0,l.kt)("p",null,"Import method"),(0,l.kt)("p",null,"Supported Data Formats"),(0,l.kt)("p",null,"HDFS"),(0,l.kt)("p",null,"Parquet\uff0cORC\uff0ccsv\uff0cgzip"),(0,l.kt)("p",null,"Local files and memory data"),(0,l.kt)("p",null,"snappy compression format"),(0,l.kt)("p",null,"json, csv, TSKV\uff0cParquet\uff0cORC"),(0,l.kt)("p",null,"Kafka"),(0,l.kt)("p",null,"csv, gzip,json"),(0,l.kt)("h1",{id:"import-method"},"Import Method"),(0,l.kt)("h2",{id:"streaming-import-data-local-files-and-memory-data"},"Streaming import data (local files and memory data)"),(0,l.kt)("h3",{id:"method-1"},"Method 1:"),(0,l.kt)("p",null,"Method 1 uses the regular syntax of the VALUES format, which is suitable for temporarily inserting a small amount of data for testing:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"INSERT INTO [db.]table [(c1, c2, c3...)] VALUES (v11, v12, v13), (v21, v22, v23), ...\n\n")),(0,l.kt)("p",null,"Among them, c1, c2, c3 are column field declarations, which can be ignored. Immediately after VALUES is the written data composed of tuples, which corresponds to the column field declaration through subscript bits. The data supports batch statement writing, and multiple lines of data are separated by commas."),(0,l.kt)("p",null,"For example, consider this table:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"CREATE TABLE test.insert_select_testtable\n(\n    `a` Int8,\n    `b` String,\n    `c` Int8,\n    `date` Date\n)\nENGINE = CnchMergeTree()\nPARTITION by toYYYYMM(date)\nORDER BY tuple()\n\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"INSERT INTO insert_select_testtable VALUES (1, 'a', 1,'2022-11-10');\n\n")),(0,l.kt)("p",null,"When writing data using the syntax of the VALUES format, expressions or functions are supported, for example:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"INSERT INTO insert_select_testtable VALUES (1, 'a', 1, now());\n\n")),(0,l.kt)("h3",{id:"method-2"},"Method 2:"),(0,l.kt)("p",null,"The second way is to use the syntax of the specified format:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"INSERT INTO [db.]table [(c1, c2, c3...)] FORMAT format_name data_set\n\n")),(0,l.kt)("p",null,"ByConity supports multiple data formats, taking the commonly used CSV format as an example:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"INSERT INTO insert_select_testtable FORMAT CSV \\\n1, 'a', 1, '2022-11-10'\\\n2, 'b', 2, '2022-11-11'\n\n")),(0,l.kt)("p",null,"At the same time, it also supports inserting data into tables from files. For example:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"INSERT INTO [db.]table [(c1, c2, c3)] FORMAT format_name INFILE file_name\n\n")),(0,l.kt)("p",null,"Use the above statement to read data from the client\u2019s file and insert it into the table. file_name and type are both String types, and the format of the input file must be set in the FORMAT statement."),(0,l.kt)("h3",{id:"method-3"},"Method 3:"),(0,l.kt)("p",null,"The third way is to use the form of the SELECT clause, which is suitable for the situation where the results of a certain table need to be saved and used for subsequent queries:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"INSERT INTO [db.]table [(c1, c2, c3...)] SELECT ...\n\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"The corresponding relationship with the SELECT column when writing is to use the position to correspond, although the names in the SELECT expression and INSERT are different. If necessary, the corresponding type conversion will be performed.\n")),(0,l.kt)("p",null,"The query result can be written into the data table through the SELECT clause. Assuming that the data of insert_select_testtable_1 needs to be written into insert_select_testtable, the following statement can be used:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"INSERT INTO insert_select_testtable SELECT * from insert_select_testtable_1\n\n")),(0,l.kt)("p",null,"When writing data through the SELECT clause, it also supports adding expressions or functions, for example:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"INSERT INTO insert_select_testtable SELECT 1, 'a', 1, now();\n\n")),(0,l.kt)("p",null,"Although both the forms of VALUES and SELECT clauses support declaring expressions or functions, expressions and functions will bring additional performance overhead, resulting in degraded write performance. So if you pursue the ultimate write performance. So if you pursue the ultimate write performance, you should avoid using them."),(0,l.kt)("h2",{id:"import-data-from-external-storage"},"Import data from external storage"),(0,l.kt)("p",null,"ByConity also supports importing data from local or HDFS, for example:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"INSERT INTO [db.]table [(c1, c2, c3)] FORMAT format_name INFILE 'hdfs://ip:port/file_name'\n\n")),(0,l.kt)("h2",{id:"import-data-using-kakfa"},"Import data using kakfa"),(0,l.kt)("h3",{id:"function-definition"},"Function definition"),(0,l.kt)("p",null,"CnchKafka is a table engine adapted to the cloud-native architecture implemented by ByConity based on the self-developed community ClickHouse Kafka table engine. It is used to efficiently and quickly import user data from Apache Kafka into ByConity in real time; At the same time, some functions have been enhanced based on the community implementation."),(0,l.kt)("p",null,"The main features of CnchKafka include:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Provide automatic fault tolerance based on the advantages of cloud-native architecture to reduce operation and maintenance costs;"),(0,l.kt)("li",{parentName:"ul"},"Scalable consumption capability: support to adjust the number of consumers through the SYSTEM command, and the maximum number of partitions corresponding to the topic can be adapted;"),(0,l.kt)("li",{parentName:"ul"},"Enhanced consumption semantics: Rely on Transaction guarantee, implement Exactly-Once consumption semantics through engine management offset;"),(0,l.kt)("li",{parentName:"ul"},"Consumption performance: largely depends on the schema complexity of the user table, usually the experience value is 2k-2M entries/second, and the throughput is about 15MiB/s;"),(0,l.kt)("li",{parentName:"ul"},"Support multiple data types, including but not limited to Json, ProtoBuf, CSV, etc.;"),(0,l.kt)("li",{parentName:"ul"},"Supports recording consumption logs, which not only facilitates troubleshooting, but also provides the ability to audit data.")),(0,l.kt)("h3",{id:"implementation-principle"},"Implementation principle"),(0,l.kt)("h4",{id:"overview"},"Overview"),(0,l.kt)("p",null,"CnchKafka inherits the basic design of the community, that is, realizes the entire consumption link through a <CnchKafka consumption table, Materialized View materialized view table, storage table> triplet, in which:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"CnchKafka consumption table: responsible for subscribing to Kafka topics and consuming messages; parsing the obtained messages and writing them as Blocks;"),(0,l.kt)("li",{parentName:"ul"},"Materialized View materialized view table: build a data path from the consumption table to the storage table, write the block consumed by CnchKafka into the storage table, and provide a simple filtering function;"),(0,l.kt)("li",{parentName:"ul"},"Storage table: Support various MergeTree storage tables of Cnch.")),(0,l.kt)("p",null,"The basic data path is as follows:"),(0,l.kt)("p",null,(0,l.kt)("img",{src:a(445).Z,width:"852",height:"492"})),(0,l.kt)("p",null,"The components in the figure are the components of the ByConity cloud-native architecture that involve CnchKafka. In order to avoid readers' ignorance, here is a brief description of each component; but due to space limitations and key points, please refer to the architecture document for more detailed architecture design details."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Server")),(0,l.kt)("li",{parentName:"ul"},"Application access layer, the entrance of all query and import tasks;"),(0,l.kt)("li",{parentName:"ul"},"Lightweight design, no specific query and import itself, mainly responsible for task scheduling forwarding and metadata access, including:"),(0,l.kt)("li",{parentName:"ul"},"Preprocess the query request, read the metadata from the Catalog, send the original data and query sql to the query node, and return the query result to the upper layer (call interface or user, etc.);"),(0,l.kt)("li",{parentName:"ul"},"Manage import tasks: select the import node to execute the import task;"),(0,l.kt)("li",{parentName:"ul"},"Interact with Catalog to query or update metadata;"),(0,l.kt)("li",{parentName:"ul"},"Interact with Resource Manager to select task execution nodes to ensure load balancing;"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Virtual Ware House")),(0,l.kt)("li",{parentName:"ul"},"Computing layer, the execution node of all query and import tasks, stateless service;"),(0,l.kt)("li",{parentName:"ul"},"Support exclusive use by tenants to achieve resource and data isolation;"),(0,l.kt)("li",{parentName:"ul"},"Support read-write separation, query and import can create and specify different Virtual WareHouse;"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Catalog")),(0,l.kt)("li",{parentName:"ul"},"KV database, used for metadata management, including database table meta information, part meta information, etc.;"),(0,l.kt)("li",{parentName:"ul"},"CnchKafka consumption offset is also stored in the catalog;"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"VFS")),(0,l.kt)("li",{parentName:"ul"},"Underlying storage, supporting multiple storage systems, including HDFS, S3, etc.")),(0,l.kt)("h4",{id:"kafkaconsumemanager"},"KafkaConsumeManager"),(0,l.kt)("p",null,"Each CnchKafka consumption table will start a Manager at the server layer to be responsible for scheduling and managing all consumer tasks. The Manager itself is a resident thread on the server side, and its service is guaranteed to be stable through the high availability of the server and DaemonManager."),(0,l.kt)("p",null,"The main implementation and functions of KafkaConsumeManager include:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Distribute the topic partition evenly to each consumer according to the configured number of consumers;"),(0,l.kt)("li",{parentName:"ul"},"Interact with the Catalog to obtain the offset consumed by the partition;"),(0,l.kt)("li",{parentName:"ul"},"Schedule the consumer to the configured Virtual Warehouse node for execution:"),(0,l.kt)("li",{parentName:"ul"},"Node selection supports multiple policy configurations to ensure load balancing;"),(0,l.kt)("li",{parentName:"ul"},"Regularly detect each consumer task to ensure the stability of task execution.")),(0,l.kt)("h4",{id:"kafkaconsumer"},"KafkaConsumer"),(0,l.kt)("p",null,"Each KafkaConsumer is implemented as a resident thread and executed on the Virtual Warehouse node. It is responsible for consuming data from the specified topic partition, converting it into parts and writing it to VFS, and submitting meta information back to the server side for writing to the Catalog. main feature:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Inherit the batch writing mode of the community (each consumption cycle defaults to 8 seconds);"),(0,l.kt)("li",{parentName:"ul"},"Each consumption process guarantees atomicity through Transaction:"),(0,l.kt)("li",{parentName:"ul"},"Create transactions by interacting with Server RPC;"),(0,l.kt)("li",{parentName:"ul"},"The transaction commit will submit the written part meta-information and the latest consumed offset at the same time.")),(0,l.kt)("p",null,"Refer to the figure below for the execution process of a single consumption:"),(0,l.kt)("p",null,(0,l.kt)("img",{src:a(603).Z,width:"852",height:"580"})),(0,l.kt)("h4",{id:"exactly-once"},"Exactly-Once"),(0,l.kt)("p",null,"Compared with the community implementation, the CnchKafka implementation has enhanced consumption semantics, that is, from the community's At-Least-Once semantics to Exactly-Once semantics. This is mainly due to the guarantee of the new architecture Transaction."),(0,l.kt)("p",null,"Since each round of consumption will go through transaction management, and the corresponding offset will be submitted at the same time as the data metadata information is submitted each time. Since the transaction guarantees the atomicity of the commit, the data metadata and offset are either submitted successfully at the same time, or both fail to be submitted."),(0,l.kt)("p",null,"This ensures that the data and offset are always consistent, and each restart of consumption will continue to consume from the last submitted offset position, thus realizing Exactly-Once."),(0,l.kt)("h4",{id:"automatic-fault-tolerance-implementation"},"Automatic fault tolerance implementation"),(0,l.kt)("p",null,"CnchKafka's overall fault-tolerant strategy adopts ",(0,l.kt)("strong",{parentName:"p"},"fast failure")," approach, namely:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"KafkaConsumeManager regularly detects consumer tasks, if the detection fails, a new consumer is immediately pulled;"),(0,l.kt)("li",{parentName:"ul"},"In each execution of KafkaConsumer, the two interactions with Server RPC (creating transaction and submitting transaction) will verify the validity of itself to the Manager. If the verification fails (for example, the Manager has pulled a new consumer, etc.), it will Take the initiative to kill yourself.")),(0,l.kt)("h3",{id:"users-guidance"},"user's guidance"),(0,l.kt)("h4",{id:"create-table"},"Create table"),(0,l.kt)("p",null,"Creating a CnchKafka consumption table is similar to creating a Kafka table natively in the community. You need to configure the Kafka data source and consumption parameters through the Setting parameter. Examples are as follows:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"CREATE TABLE kafka_test.cnch_kafka_consume\n(\n    `i` Int64,\n    `ts` DateTime\n)\nENGINE = CnchKafka()\nSETTINGS\nkafka_broker_list = '10.10.10.10:9092',  -- replace with your own broker list\nkafka_topic_list = 'my_kafka_test_topic', -- topic name to subcribe\nkafka_group_name = 'hansome_boy_consume_group', -- your consumer-group name\nkafka_format = 'JSONEachRow', -- always be json\nkafka_row_delimiter = '\\n', -- always be \\n\nkafka_num_consumers = 1\n\n")),(0,l.kt)("p",null,"(For setting parameter description and other more parameter support, please refer to the description below)"),(0,l.kt)("p",null,"Since the Kafka consumption design requires three tables, two other tables need to be created synchronously."),(0,l.kt)("p",null,"First create a storage table (take CnchMergeTree as an example):"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"CREATE TABLE kafka_test.cnch_store_kafka\n(\n    `i` Int64,\n    `ts` DateTime\n)\nENGINE = CnchMergeTree\nPARTITION BY toDate(ts)\nORDER BY ts\n\n")),(0,l.kt)("p",null,"Finally, create the materialized view table (it must be created after the Kafka table and storage table are successfully created):"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"CREATE MATERIALIZED VIEW kafka_test.cnch_kafka_view\nTO kafka_test.cnch_store_kafka\n(\n    `i` Int64,\n    `ts` DateTime\n)\nAS\nSELECT * -- you can add virtual columns here if you need\nFROM kafka_test.cnch_kafka_consume\n\n")),(0,l.kt)("p",null,"If you have the consumption permission corresponding to the topic, after the three tables are created, the consumption will start automatically."),(0,l.kt)("h4",{id:"virtual-column-support"},"Virtual column support"),(0,l.kt)("p",null,"Sometimes the business needs to obtain the metadata of Kafka messages (e.g. message partition, offset, etc.). At this time, you can use the virtual columns function to meet this requirement. Virtual columns do not need to be specified when creating a table, they are attributes of the table engine itself. It can be placed in the SELECT statement of the VIEW table and stored in the bottom table (when the corresponding column is added to the bottom table):"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"SELECT\n    _topic,    -- String\n    _partition,    -- UInt64\n    _key,    -- String\n    _offset,    -- UInt64\n    _content,  -- String: complete message content\n    *    -- Normal columns can be expanded by *, virtual columns cannot\nFROM kafka_test.cnch_kafka_consume\n\n")),(0,l.kt)("h4",{id:"setting-parameter-description"},"Setting Parameter Description"),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Parameter Name")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"type")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"required/default")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Description")),(0,l.kt)("p",null,"kafka_cluster / kafka_broker_list"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"required"),(0,l.kt)("p",null,"Kafka cluster within the company"),(0,l.kt)("p",null,"Community version Kafka please use ",(0,l.kt)("inlineCode",{parentName:"p"},"kafka_broker_list")," parameter"),(0,l.kt)("p",null,"kafka_topic_list"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"required"),(0,l.kt)("p",null,"Can be multiple, separated by commas"),(0,l.kt)("p",null,"kafka_group_name"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"required"),(0,l.kt)("p",null,"consumer group name, consumer group"),(0,l.kt)("p",null,"kafka_format"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"required"),(0,l.kt)("p",null,"Message format; currently most commonly used JSONEachRow"),(0,l.kt)("p",null,"kafka_row_delimiter"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"'\\0'"),(0,l.kt)("p",null,"common use '\\n'"),(0,l.kt)("p",null,"kafka_num_consumers"),(0,l.kt)("p",null,"UInt64"),(0,l.kt)("p",null,"1"),(0,l.kt)("p",null,"The number of consumers, it is recommended not to exceed the maximum number of partitions in the topic"),(0,l.kt)("p",null,"kafka_max_block_size"),(0,l.kt)("p",null,"UInt64"),(0,l.kt)("p",null,"65536"),(0,l.kt)("p",null,"write block_size\uff0cthe upper limit is 1M"),(0,l.kt)("p",null,"kafka_max_poll_interval_ms"),(0,l.kt)("p",null,"Milliseconds"),(0,l.kt)("p",null,"7500"),(0,l.kt)("p",null,"the max time to poll from broker each iteration"),(0,l.kt)("p",null,"kafka_schema"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,'""'),(0,l.kt)("p",null,"Schema file setting parameters, set in the format of file name + colon + message name"),(0,l.kt)("p",null,"Such as: ",(0,l.kt)("inlineCode",{parentName:"p"},"schema.proto:MyMessage")),(0,l.kt)("p",null,"kafka_format_schema_path"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,'""'),(0,l.kt)("p",null,"Remote schema file path (without file name) setting parameter, currently only supports hdfs."),(0,l.kt)("p",null,"(If this parameter is not set, it will be read from the default path set in the configuration file)"),(0,l.kt)("p",null,"kafka_protobuf_enable_multiple_message"),(0,l.kt)("p",null,"bool"),(0,l.kt)("p",null,"true"),(0,l.kt)("p",null,"If set to true, it means that multiple protobuf messages can be read from one kafka message, separated by their respective lengths"),(0,l.kt)("p",null,"kafka_protobuf_default_length_parser"),(0,l.kt)("p",null,"bool"),(0,l.kt)("p",null,"false"),(0,l.kt)("p",null,"It only takes effect when ",(0,l.kt)("inlineCode",{parentName:"p"},"kafka_protobuf_enable_multiple_message")," is true: true means that the message header has a variable record length; false means use a fixed 8 bytes as the header record length."),(0,l.kt)("p",null,"kafka_extra_librdkafka_config"),(0,l.kt)("p",null,"Json format string"),(0,l.kt)("p",null,'""'),(0,l.kt)("p",null,"(More params refer to ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md#:~:text=see%20dedicated%20API-,ssl.ca.location,-*"},"here"),")"),(0,l.kt)("p",null,"Other parameters supported by rdkafka, usually used for authentication"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"SCRAM"),': "{"sasl.mechanisms":"SCRAM-SHA-512","sasl.password":"',(0,l.kt)("strong",{parentName:"li"},"*",'","sasl.username":"bytehouse-dev","security.protocol":"sasl_ssl","ssl.ca.location":"'),'"}"'),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"PLAIN"),': "{"sasl.mechanisms":"PLAIN","sasl.password":"admin","sasl.username":"admin","security.protocol":"sasl_plaintext"}"')),(0,l.kt)("p",null,"cnch_vw_write"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,'"vw_write"'),(0,l.kt)("p",null,"Configure consumption to use Virtual Warehouse, and the consumer task will be scheduled to the configured Virtual Warehouse node for execution"),(0,l.kt)("p",null,"kafka_cnch_schedule_mode"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,'"random"'),(0,l.kt)("p",null,"The scheduling strategy used by ConsumeManager when scheduling consumer tasks currently supports: random, hash, and ",(0,l.kt)("strong",{parentName:"p"},"least_consumers; "),"If it is an independent vw or the number of consumers is greater than 10, it is recommended to use ",(0,l.kt)("strong",{parentName:"p"},"least_consumers")),(0,l.kt)("h4",{id:"modify-consumption-parameters"},"Modify consumption parameters"),(0,l.kt)("p",null,"Supports quick modification of Setting parameters through the ALTER command, which is mainly used to adjust the number of consumers and improve consumption capabilities."),(0,l.kt)("p",null,"command:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"ALTER TABLE <cnch_kafka_name> MODIFY SETTING <name1> = <value1>, <name2> = <value2>\n\n")),(0,l.kt)("p",null,"Execution of this command will automatically restart the consumption task."),(0,l.kt)("h4",{id:"manually-start-and-stop-consumption"},"Manually start and stop consumption"),(0,l.kt)("p",null,"In some scenarios, users may need to manually stop consumption, and then manually resume; we provide the corresponding SYSTEM command implementation:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"SYSTEM START/STOP/RESTART CONSUME <cnch_kafka_name>\n\n")),(0,l.kt)("p",null,"Note: The START/STOP command will persist the corresponding state to the Catalog, so after executing the STOP command, if you do not execute START, even if the service restarts, the consumption task will not resume."),(0,l.kt)("h4",{id:"reset-offset"},"reset offset"),(0,l.kt)("p",null,"Since CnchKafka's offset is managed and saved by the engine itself, when the user needs to restart the offset, we also implement the SYSTEM command operation. Specifically, the following three methods are supported:"),(0,l.kt)("h5",{id:"a-is-reset-to-a-special-position-latest-positionstart-position"},"A is reset to a special position: latest position/start position"),(0,l.kt)("p",null,"command:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},'SYSTEM RESET CONSUME OFFSET \'{"database_name":"XXX", "table_name": "XXX", "offset_value":-1}\'\n\n')),(0,l.kt)("p",null,"Possible position-specific value values:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"    enum Offset {\n        OFFSET_BEGINNING = -2,\n        OFFSET_END = -1,\n        OFFSET_STORED = -1000,\n        OFFSET_INVALID = -1001\n    };\n\n")),(0,l.kt)("h5",{id:"b-reset-by-timestamp"},"B reset by timestamp"),(0,l.kt)("p",null,"Version requirements >= cnch-1.4) command:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},'SYSTEM RESET CONSUME OFFSET \'{"database_name":"XXX", "table_name": "XXX", "timestamp":1646125258000}\'\n\n')),(0,l.kt)("p",null,"The value of timestamp should be the timestamp of a certain time within the validity period of the data on the Kafka side, and it is in milliseconds."),(0,l.kt)("h5",{id:"c-specifies-the-offset-specific-value"},"C specifies the offset specific value"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},'system reset consume offset \'{"database_name":"XXX", "table_name": "XXX", "topic_name": "XXX", "offset_values":[{"partition":0, "offset":100}, {"partition":10, "offset":101}]}\'\n\n')),(0,l.kt)("p",null,"It is relatively rare to assign a specific topic partition to a specific offset value."),(0,l.kt)("h3",{id:"operation-manual"},"Operation Manual"),(0,l.kt)("h4",{id:"common-consumer-performance-tuning"},"Common consumer performance tuning"),(0,l.kt)("p",null,"When consumption continues to lag, it is usually insufficient consumption capacity. CnchKafka creates a table with one consumer by default, and the maximum block size for a single consumption write is 65536. When the consumption capacity is insufficient, the consumer and block-size parameters should be adjusted first. For the adjustment method, refer to the above ",(0,l.kt)("strong",{parentName:"p"},"Modify Consumption Parameters")),(0,l.kt)("h5",{id:"adjust-max-block-size"},"Adjust max-block-size"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"This parameter directly affects the consumption memory usage, the larger the value is, the more memory is required. For some consumption tables with large single data, carefully adjust this parameter to avoid memory explosion. (up to 1M)"),(0,l.kt)("li",{parentName:"ul"},'When the user does not have high requirements for data delay and the amount of data is large and the memory is sufficient, this parameter and the "kafka_max_poll_interval_ms" parameter can be adjusted synchronously to increase the consumption time of each round, increase the part written each time, and reduce the pressure of MERGE , to improve query performance.')),(0,l.kt)("h5",{id:"adjust-num_consumers"},"Adjust num_consumers"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"The upper limit of this parameter is the number of partitions corresponding to the consumption topic."),(0,l.kt)("li",{parentName:"ul"},"In the case of no lag in consumption, reduce this parameter as much as possible (that is, avoid meaninglessly increasing this parameter), reduce resource usage, and avoid excessive consumption of fragmented parts, which increases MERGE pressure and is not conducive to queries.")),(0,l.kt)("h4",{id:"system-table-for-auxiliary-troubleshooting"},"System table for auxiliary troubleshooting"),(0,l.kt)("h5",{id:"consumption-event-cnch_systemcnch_kafka_log"},"Consumption event: cnch_system.cnch_kafka_log"),(0,l.kt)("p",null,"The kakfa_log table records some basic consumption events. To enable it, you need to configure the kafka_log item in config.xml (both server and worker need to be configured), and it will take effect after restarting."),(0,l.kt)("p",null,"The kafka_log is written by the consumer task in the Virtual Warehouse, and is aggregated into the global cnch_system.cnch_kafka_log table in real time, so that the consumption records of all consumption tables can be viewed from the server segment."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Field Description")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"column name")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"type")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"illustrate")),(0,l.kt)("p",null,"event_type"),(0,l.kt)("p",null,"Enum8"),(0,l.kt)("p",null,"see table below"),(0,l.kt)("p",null,"event_date"),(0,l.kt)("p",null,"date"),(0,l.kt)("p",null,"Date of occurrence of time. Partition field, it is recommended to bring it in every query."),(0,l.kt)("p",null,"event_time"),(0,l.kt)("p",null,"DateTime"),(0,l.kt)("p",null,"The time of occurrence of the time, in seconds"),(0,l.kt)("p",null,"duration_ms"),(0,l.kt)("p",null,"UInt64"),(0,l.kt)("p",null,"Event duration, in seconds"),(0,l.kt)("p",null,"cnch_database"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"CnchKafka library name"),(0,l.kt)("p",null,"cnch_table"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"CnchKafka table name"),(0,l.kt)("p",null,"database"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"consumer task library name (currently the same as cnch_database)"),(0,l.kt)("p",null,"table"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"consumer task table name (usually cnch_table plus timestamp and consumer number suffix)"),(0,l.kt)("p",null,"consumer"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"consumer number"),(0,l.kt)("p",null,"metric"),(0,l.kt)("p",null,"UInt64"),(0,l.kt)("p",null,"consumption line"),(0,l.kt)("p",null,"has_error"),(0,l.kt)("p",null,"UInt8"),(0,l.kt)("p",null,"1 means abnormal; 0 means no abnormal."),(0,l.kt)("p",null,"exception"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"exception description,"),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Event Description")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"UInt8 value")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"String value")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"illustrate")),(0,l.kt)("p",null,"1"),(0,l.kt)("p",null,"POLL"),(0,l.kt)("p",null,"Metric indicates how many pieces of data are consumed, and duration_ms covers a complete consumption process, including the time of WRITE."),(0,l.kt)("p",null,"2"),(0,l.kt)("p",null,"PARSE_ERROR"),(0,l.kt)("p",null,"The metric indicates the number of consumption items with parsing errors. If there are multiple parsing errors, only one will be selected and printed out."),(0,l.kt)("p",null,"3"),(0,l.kt)("p",null,"WRITE"),(0,l.kt)("p",null,"Metric indicates the number of rows written to the data, and duration_ms is basically equivalent to the data persistence time"),(0,l.kt)("p",null,"4"),(0,l.kt)("p",null,"EXCEPTION"),(0,l.kt)("p",null,"Abnormal consumption process. The common ones are: authentication exception, data persistence failure, and VIEW SELECT execution failure."),(0,l.kt)("p",null,"5"),(0,l.kt)("p",null,"EMPTY_MESSAGE"),(0,l.kt)("p",null,"The number of empty messages."),(0,l.kt)("p",null,"6"),(0,l.kt)("p",null,"FILTER"),(0,l.kt)("p",null,"The data to be filtered during the write phase."),(0,l.kt)("p",null,"7"),(0,l.kt)("p",null,"COMMIT"),(0,l.kt)("p",null,"The last transaction submission record, only this record indicates that the data was written successfully, which can be used as a data audit standard"),(0,l.kt)("h5",{id:"consumption-status-systemcnch_kafka_tables"},"Consumption status: system.cnch_kafka_tables"),(0,l.kt)("p",null,"kafka_tables records the real-time status of the CnchKafka table, which starts by default and is a memory table;"),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Field Description")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"field name")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"type of data")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"illustrate")),(0,l.kt)("p",null,"database"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"Database name"),(0,l.kt)("p",null,"name"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"Kafka table name"),(0,l.kt)("p",null,"uuid"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"Kafka table unique identifier UUID"),(0,l.kt)("p",null,"kafka_cluster"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"kafka cluster"),(0,l.kt)("p",null,"topics"),(0,l.kt)("p",null,"Array(String)"),(0,l.kt)("p",null,"Consume topic list"),(0,l.kt)("p",null,"consumer_group"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"Belonging to the consumer group"),(0,l.kt)("p",null,"num_consumers"),(0,l.kt)("p",null,"UInt32"),(0,l.kt)("p",null,"The number of consumers currently actually executing"),(0,l.kt)("p",null,"consumer_tables"),(0,l.kt)("p",null,"Array(String)"),(0,l.kt)("p",null,"The data table name corresponding to each consumer"),(0,l.kt)("p",null,"consumer_hosts"),(0,l.kt)("p",null,"Array(String)"),(0,l.kt)("p",null,"Execution nodes to which each consumer is distributed"),(0,l.kt)("p",null,"consumemr_partitions"),(0,l.kt)("p",null,"Array(String)"),(0,l.kt)("p",null,"The partitions that each consumer is assigned to consume"),(0,l.kt)("h4",{id:"common-troubleshooting-consumption-exception-records"},"Common troubleshooting consumption exception records"),(0,l.kt)("h5",{id:"view-the-real-time-status-of-cnchkafka-consumption-table"},"View the real-time status of CnchKafka consumption table"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"SELECT * FROM system.cnch_kafka_tables\nWHERE database = <database_name> AND name = <cnch_kafka_table>\n\n")),(0,l.kt)("h5",{id:"view-recent-consumption-records"},"View recent consumption records"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"SELECT * FROM cnch_system.cnch_kafka_log\nWHERE event_date = today()\n AND cnch_database = <database_name>\n AND cnch_table = <cnch_kafka_table>\n AND event_time > now() - 600 -- \u6700\u8fd1\u5341\u5206\u949f\nORDER BY event_time\n\n")),(0,l.kt)("h5",{id:"statistics-of-consumption-records-of-the-day-by-hour"},"Statistics of consumption records of the day by hour"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"SELECT\n toHour(event_time) as hour,\n sumIf(metric, event_type = 'POLL') as poll_rows,\n sumIf(metric, event_type = 'PARSE_ERROR') as error_rows,\n sumIf(metric, event_type = 'COMMIT') as commit_rows\nFROM cnch_system.cnch_kafka_log\nWHERE event_date = today()\n AND cnch_database = <database_name>\n AND cnch_table = <cnch_kafka_table>\nGROUP BY hour\nORDER BY hour\n\n")),(0,l.kt)("h2",{id:"import-external-data-through-spark"},"Import external data through Spark"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"   Use the part writer tool to import ByConity, and the part writer tool can directly convert data files into part files without going through the ByConity engine. Using the part writer can realize the separation of ByConity query and construction, alleviate the resource competition of data import and query to a certain extent, and improve query performance. At present, the development of the part writer and ByConity's corresponding function of loading part files is basically completed. The following describes how to use the part writer to import data into ByConity.\n\n    (1) Use part writer to generate part file\n")),(0,l.kt)("p",null,"The part writer receives a sql statement as a parameter, and the user specifies the source data file, data file format, data schema, part file storage path and other detailed information through the sql statement. The usage is as follows:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"./part_writer \"load CSV file '/path/to/data/test.csv' as table db.tablename(col1 UInt64, col2 String, col3 Nullable(String)) partition by col1 order by (col2, col3) location '/ path/to/dest/'\"\n\n")),(0,l.kt)("p",null,"In the example SQL,"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"'CSV' specifies the source data file format; in addition, the part writer can also use CSVWithNames, JSONEachRow and other data file formats natively supported by clickhouse."),(0,l.kt)("li",{parentName:"ol"},"'/path/to/data/test.csv' specifies the source data file; it supports reading source data from local and hdfs. If using hdfs data files, specify the path as: 'hdfs://host:port/path/to/data/file';"),(0,l.kt)("li",{parentName:"ol"},"'/path/to/dest/' specifies the destination folder where part files are written; it supports writing part files directly to hdfs, and ByConity can pull and load part files from hdfs."),(0,l.kt)("li",{parentName:"ol"},"as table specifies the schema information of the inserted data"),(0,l.kt)("li",{parentName:"ol"},"partition by and order by specify the partition key and sort key of the data table respectively. Multiple keys are separated by commas and need to be wrapped in parentheses, such as: partition by (name, id)."),(0,l.kt)("li",{parentName:"ol"},"The ByConity special option, settings cnch=1, is used to directly dump the generated part into the ByConity part format and write it into the hdfs path specified by the location option.")),(0,l.kt)("p",null,"(2) Import part files into ByConity"),(0,l.kt)("p",null,"The generated part file can be directly copied to the data file path corresponding to the ByConity table, and then loaded by restarting the ByConity server;"),(0,l.kt)("p",null,"You can also copy the part file directory to the detached directory of the table, and load the part file through the attach command, such as"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"alter table test attach part 'partfile'\n\n")),(0,l.kt)("p",null,"If you specify to upload directly to hdfs when using part writer to generate part files, you can execute the following command:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"system fetch parts into db.table 'hdfs://host:port/path/to/part/'\n\n")),(0,l.kt)("p",null,"ByConity will automatically pull the part file from the hdfs path and load it."),(0,l.kt)("p",null,"ByConity attach syntax:"),(0,l.kt)("p",null,"Used to import the parts dumped to hdfs into the target table:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"alter table test attach parts from '/hdfs/path/to/dumped/dir'\n\n")),(0,l.kt)("p",null,"At the same time, the fourth form also supports spark import:"),(0,l.kt)("p",null,"In the actual application scenario, it is necessary to import a large amount of data into the ByConity cluster, you can consider using spark. First read the data into the spark dataset from the outside; then repartition the dataset according to the sharding key to ensure that the data to be sent to different ByConity nodes falls on different partitions (it may be necessary to adjust the parameters of spark.sql.shuffle.partitions according to the actual situation Make the partition not less than the number of ByConity master nodes); for each partition, first generate a part file by calling the part writer, and specify the part file to upload to hdfs, and then notify ByConity to load the part file by sending an http request to the corresponding ByConity node. The data flow diagram is as follows:"),(0,l.kt)("p",null,"![](./assets/import data/boxcnlSkMX0zkWno7WT7250zU1f.png)"),(0,l.kt)("h2",{id:"access-external-data-sources-through-byconity"},"Access external data sources through ByConity"),(0,l.kt)("p",null,"###MySQL"),(0,l.kt)("p",null,"The ",(0,l.kt)("inlineCode",{parentName:"p"},"MySQL")," engine allows users to access MySQL tables through ByConity, and can make SELECT and INSERT queries."),(0,l.kt)("h4",{id:"create-tables-in-mysql"},"Create Tables in MySQL"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"create database")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"CREATE DATABASE db1;\n\n")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Create tables in mysql")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"CREATE TABLE db1.table1(\n    id Int,\n    column1 VARCHAR(255)\n);\n\n")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"insert some date")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"INSERT INTO db1.table1\n    (id, column1)\nvalues\n    (1, 'mysql-ab'),\n    (2, 'mysql-cd');\n\n")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Create a user in mysql to connect to mysql in ByConity")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"CREATE USER 'mysql_byconity'@'%' IDENTIFIED BY 'Password123!';\n\n")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Granted permission. (Here, for demonstration purposes, the ",(0,l.kt)("inlineCode",{parentName:"li"},"mysql_byconity")," user has been granted admin privileges)")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"GRANT ALL PRIVILEGES ON *.* TO 'mysql_byconity'@'%';\n\n")),(0,l.kt)("h4",{id:"create-mysql-table-in-byconity"},"Create MySQL table in ByConity"),(0,l.kt)("p",null,"Now let's create a ByConity table that uses the ",(0,l.kt)("strong",{parentName:"p"},"MySQL")," table engine:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"CREATE TABLE mysql_table1 (\n   idUInt64,\n   column1 String\n)\nENGINE = MySQL('mysql-host.domain.com','db1','table1','mysql_byconity','Password123!');\n\n")),(0,l.kt)("p",null,"The parameters of the ",(0,l.kt)("inlineCode",{parentName:"p"},"MySQL")," engine are as follows:"),(0,l.kt)("p",null,"parameter"),(0,l.kt)("p",null,"describe"),(0,l.kt)("p",null,"example"),(0,l.kt)("p",null,"host"),(0,l.kt)("p",null,"Domain name or IP:Port"),(0,l.kt)("p",null,"mysql-host.domain.com"),(0,l.kt)("p",null,"database"),(0,l.kt)("p",null,"mysql database name"),(0,l.kt)("p",null,"db1"),(0,l.kt)("p",null,"tabele"),(0,l.kt)("p",null,"mysql table name"),(0,l.kt)("p",null,"table1"),(0,l.kt)("p",null,"user"),(0,l.kt)("p",null,"user to connect to mysql"),(0,l.kt)("p",null,"mysql_byconity"),(0,l.kt)("p",null,"password"),(0,l.kt)("p",null,"Password to connect to mysql"),(0,l.kt)("p",null,"Password123!"),(0,l.kt)("h4",{id:"test-connection-mysql-table-in-byconity"},"Test connection mysql table in ByConity"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Test SELECT query")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"select * from mysql_table1;\n\n")),(0,l.kt)("p",null,"![](./assets/import data/boxcnJv89B9UChc3nI0EPHoba6c.png)"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Test INSERT query")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"INSERT INTO mysql_table1\n    (id, column1)\nVALUES\n    (3, 'byconity-test');\n\n")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Validate data inserted from ByConity in MySQL")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"mysql> select id, column1 from db1.table1;\n\n")),(0,l.kt)("p",null,"![](./assets/import data/boxcnQdV8Jpg8jcqEKwAOjONxkh.png)"),(0,l.kt)("h3",{id:"hive"},"Hive"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"CnchHive is a table engine provided by ByConity, which supports federated query in the form of external tables, and users can directly accelerate data query without importing data.\n")),(0,l.kt)("h3",{id:"use"},"use:"),(0,l.kt)("h4",{id:"example-1-build-a-complete-set-of-hive-tables"},"Example 1: Build a complete set of hive tables"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"--Create hive table\nCREATE TABLE t\n(\n   client_ip String,\n   request String,\n   status_code INT,\n   object_size INT,\n   date String\n)\nENGINE = CnchHive('psm', 'hive_database_name', 'hive_table_name')\nPARTITION BY date;\n\n--Parameter Description:\n--psm:hivemetastore-psm\n--hive_database_name: hive database name\n--hive_table_name: hive table name\n\n--Query hive external table\nselect * from  t where xxx;\n\n")),(0,l.kt)("h4",{id:"example-2-building-a-subset-of-hive-tables"},"Example 2: Building a subset of hive tables"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"CREATE TABLE t\n(\n  client_ip   String,\n  request     String,\n  date String\n)\nENGINE = CnchHive('psm', 'hive_database_name', 'hive_table_name')\nPARTITION BY date\n\n--Parameter Description:\n--psm\uff1ahivemetastore psm\n--hive_database_name\uff1ahive database name\n--hive_table_name\uff1ahive table name\n\n--Query hive external table\nselect * from  t where xxx;\n\n")),(0,l.kt)("h4",{id:"example-3-hive-bucket-table-construction"},"Example 3: hive bucket table construction"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"CREATE TABLE t\n(\n  client_ip   String,\n  request     String,\n  device_id   String,\n  server_time String,\n  date String\n)\nENGINE = CnchHive('psm', 'hive_database_name', 'hive_table_name')\nPARTITION BY date\nCLUSTER BY device_id INTO 65536 BUCKETS\nORDER BY server_time\nSETTINGS cnch_vw_default ='vw_default'\n\n--Parameter Description\uff1a\n--psm\uff1ahivemetastore psm\n--hive_database_name\uff1ahive database name\n--hive_table_name\uff1ahive table name\n\n--Query hive external table\nselect * from  t where xxx;\n\n")),(0,l.kt)("p",null,"illustrate:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"External column"),(0,l.kt)("li",{parentName:"ul"},"The column names need to correspond one-to-one with the hive table"),(0,l.kt)("li",{parentName:"ul"},"The order of the columns does not need to correspond one-to-one with hive"),(0,l.kt)("li",{parentName:"ul"},"You can select only some of the columns in the hive table, but the partition columns must include all of them."),(0,l.kt)("li",{parentName:"ul"},"The partition of the external column needs to be specified through the partition by statement, and it needs to be defined in the description list like ordinary columns."),(0,l.kt)("li",{parentName:"ul"},"When the Hive table is a bucket table, you need to specify the bucket column and the number of buckets when building the CnchHive engine. (CLUSTER BY xxx INTO xxx BUCKETS )"),(0,l.kt)("li",{parentName:"ul"},"When there is an ORDER BY field in the Hive table, the ORDER BY field needs to be specified when building the CnchHive engine."),(0,l.kt)("li",{parentName:"ul"},"ENGINE specified as CnchHive"),(0,l.kt)("li",{parentName:"ul"},"engine parameters"),(0,l.kt)("li",{parentName:"ul"},"psm:hivemetastore-psm"),(0,l.kt)("li",{parentName:"ul"},"hive_database_name: specifies the database in hive"),(0,l.kt)("li",{parentName:"ul"},"hive_table_name: specifies the table in hive, does not support view."),(0,l.kt)("li",{parentName:"ul"},"The supported column types correspond to the following table:")),(0,l.kt)("p",null,"hive column type"),(0,l.kt)("p",null,"CnchHive column type"),(0,l.kt)("p",null,"describe"),(0,l.kt)("p",null,"INT/INTERGER"),(0,l.kt)("p",null,"INT/INTERGER"),(0,l.kt)("p",null,"BIGINT"),(0,l.kt)("p",null,"BIGINT"),(0,l.kt)("p",null,"TIMESTAMP"),(0,l.kt)("p",null,"DateTime"),(0,l.kt)("p",null,"STRING"),(0,l.kt)("p",null,"String"),(0,l.kt)("p",null,"VARCHAR"),(0,l.kt)("p",null,"FixedString"),(0,l.kt)("p",null,"\u5185\u90e8\u8f6c\u6362\u4e3a FixedString"),(0,l.kt)("p",null,"CHAR"),(0,l.kt)("p",null,"FixedString"),(0,l.kt)("p",null,"Internally converted to FixedString"),(0,l.kt)("p",null,"DOUBLE"),(0,l.kt)("p",null,"DOUBLE"),(0,l.kt)("p",null,"FLOAT"),(0,l.kt)("p",null,"FLOAT"),(0,l.kt)("p",null,"DECIMAL"),(0,l.kt)("p",null,"DECIMAL"),(0,l.kt)("p",null,"MAP"),(0,l.kt)("p",null,"Map"),(0,l.kt)("p",null,"ARRAY"),(0,l.kt)("p",null,"Array"),(0,l.kt)("p",null,"Description:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"The hive table schema change will not be automatically synchronized, and the hive table needs to be rebuilt in Clickhouse"),(0,l.kt)("li",{parentName:"ul"},"The current hive storage format only supports Parquet"),(0,l.kt)("li",{parentName:"ul"},"Currently CnchHive does not support insert and alter operations")),(0,l.kt)("h4",{id:"settings"},"SETTINGS"),(0,l.kt)("p",null,"cnch_vw_default\uff1ato specify vw"),(0,l.kt)("p",null,"max_read_row_group_threads: Used to specify the number of concurrent reads of Parquet row groups."),(0,l.kt)("h4",{id:"operation-manual-1"},"Operation Manual"),(0,l.kt)("p",null,"keywords"),(0,l.kt)("p",null,"Solution"),(0,l.kt)("p",null,'DB::Exception: Can not insert NULL data into non-nullable column "name"'),(0,l.kt)("p",null,"Add Nullable attribute to column field."),(0,l.kt)("p",null,"DB::Exception: The hive type is not match in cnch."),(0,l.kt)("p",null,"CnchHive schema type does not match Hive schema."),(0,l.kt)("p",null,"DB::Exception: column name xxx doesn't match."),(0,l.kt)("p",null,"CnchHive schema name does not match Hive schema."),(0,l.kt)("p",null,"DB::Exception: CnchHive only support parquet format. Current format is org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat."),(0,l.kt)("p",null,"CnchHive currently only supports Parquet as the storage format."),(0,l.kt)("p",null,"DB::Exception: No available nnproxy xxx."),(0,l.kt)("p",null,"There is a problem with the psm of HiveMetastore, you need to check whether the psm of HiveMetastore is accessible."),(0,l.kt)("h1",{id:"common-errors-and-handling"},"Common errors and handling"),(0,l.kt)("p",null,"keywords"),(0,l.kt)("p",null,"reason"),(0,l.kt)("p",null,"Solution"),(0,l.kt)("p",null,"Too many map keys in table"),(0,l.kt)("p",null,"(more than 10000)"),(0,l.kt)("p",null,"The number of key types in the Map column exceeds 10,000"),(0,l.kt)("p",null,"If it exceeds 10000, it cannot be imported, please import data to reduce the number of map keys"),(0,l.kt)("p",null,"Memory limit (total)"),(0,l.kt)("p",null,"Memory limit exceeded during import"),(0,l.kt)("p",null,"Cannot parse JSON"),(0,l.kt)("p",null,"The data type in json does not match that of clickhouse"),(0,l.kt)("p",null,"Ask the user to check whether the upstream generated data type matches;"),(0,l.kt)("p",null,"Duplicate field found while parsing JSONEachRow format: hour"),(0,l.kt)("p",null,'The json data has repeated fields, and the repeated field here is hour, that is, the repeated field after "format:"'),(0,l.kt)("p",null,"Check whether the upstream data is correct and the configuration is correct"),(0,l.kt)("p",null,"HDFS json size xxx > 1099511627776"),(0,l.kt)("p",null,"Import data is too large (1T), import is prohibited"),(0,l.kt)("p",null,"Reduce the amount of imported data"),(0,l.kt)("p",null,"Unable to parse hdfs json file"),(0,l.kt)("p",null,"Data format error in hdfs"),(0,l.kt)("p",null,"Ask the user to check whether the file in hdfs is legal json"),(0,l.kt)("p",null,"DB::Exception: Error while reading parquet data: IOError: definition level exceeds maximum. Stack trace"),(0,l.kt)("p",null,"hdsf file error reading error, most of which are caused by missing blocks."),(0,l.kt)("p",null,"HDFS files need to be regenerated"),(0,l.kt)("p",null,"DB::Exception: Cannot parse string 'time=\"2021-11-12' as Date: syntax error at position 10 (parsed just 'time=\"2021'). Note: there are toDateOrZero and toDateOrNull functions, which returns zero/ NULL instead of throwing exception.: while pushing to view"),(0,l.kt)("p",null,"In this case, there is dirty data in the user topic; when the Kafka table is consumed, it is parsed normally according to the string; but the writing to the bottom table is found to be illegal through the conversion of functions such as toDate, resulting in writing failure and blocking consumption"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Temporarily modify the VIEW table to filter dirty data;")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"User upstream to add data cleaning and protection mechanism")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"The Kafka table analysis is consistent with the bottom table. It is not recommended to convert it during the writing phase. If Kafka analysis fails, dirty data can be discarded without blocking the entire consumption"))),(0,l.kt)("p",null,"Code: 1001, type: cppkafka::Exception, e.what() = Failed to create consumer handle: consumer regist error, please check output!"),(0,l.kt)("p",null,"Open the trace log to view the specific error information on the Kafka side"),(0,l.kt)("p",null,"Handle according to the error message"),(0,l.kt)("p",null,"Code: 49, e.displayText() = DB::Exception: Check dependencies failed for"),(0,l.kt)("p",null,"VIEW table not found"),(0,l.kt)("p",null,"Rebuild the VIEW table"),(0,l.kt)("p",null,"Code: 6001. DB::Exception: DB::Exception: Cannot get metadata of table XXX by UUID : XXX."),(0,l.kt)("p",null,"An error is reported when executing the ALTER TABLE command. The cnch table is bound to the server. This is because the table is not executed on the corresponding server."),(0,l.kt)("p",null,"First query system.cnch_table_host to obtain the server corresponding to the table, and then execute on the corresponding server"),(0,l.kt)("p",null,"No space left on device: while pushing to view"),(0,l.kt)("p",null,"disk full"),(0,l.kt)("p",null,"clean disk"),(0,l.kt)("h1",{id:"import-parameter-tuning"},"Import parameter tuning"),(0,l.kt)("h2",{id:"direct-write-mode-tuning"},"Direct write mode tuning"),(0,l.kt)("p",null,"When using INSERT VALUES, INSERT INFILE or PartWriter tools to write, the number of Parts generated at the end will affect the number of times written to HDFS and affect the overall writing time, so the number of Parts should be reduced as much as possible. The direct writing process is as follows:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"read part of file data"),(0,l.kt)("li",{parentName:"ul"},"Split this part of data according to PartitionBy"),(0,l.kt)("li",{parentName:"ul"},"Split this part of data according to ClusterBy"),(0,l.kt)("li",{parentName:"ul"},"Write the split data into a new Part and write it to HDFS")),(0,l.kt)("p",null,"Tuning means:"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"In order to reduce the number of Parts, we can arrange the data with the same partition and Bucket in the file together, so that after reading some new data each time, the number of Parts generated will be as small as possible. The data can be sorted according to the requirement that the partitions are the same and the buckets in the partitions are the same. The calculation rules of the buckets are:")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"If SPLIT_NUMBER is not specified, SipHash will be calculated for the column used by ClusterByKey and then the BucketNumber will be moduloed to get BucketNumber"),(0,l.kt)("li",{parentName:"ul"},"if SPLIT_NUMBER is specified"),(0,l.kt)("li",{parentName:"ul"},"Calculate SplitValue"),(0,l.kt)("li",{parentName:"ul"},"If a column is ClusterBy, use the dtspartition function to calculate the corresponding SplitValue"),(0,l.kt)("li",{parentName:"ul"},"If ClusterBy has multiple columns, use SipHash to calculate the corresponding SplitValue of these columns"),(0,l.kt)("li",{parentName:"ul"},"Calculate BucketNumber"),(0,l.kt)("li",{parentName:"ul"},"If it is WithRange, use SplitValue ","*"," BucketCount / SplitNumber to calculate the corresponding BucketNumber"),(0,l.kt)("li",{parentName:"ul"},"If not WithRange, use SplitValue % BucketCount to calculate the corresponding BucketNumber")),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"When reading a file"),(0,l.kt)("li",{parentName:"ol"},"If the data size of each row is not large, you can read a larger block at a time by increasing max_insert_block_size, thereby generating a larger Part"),(0,l.kt)("li",{parentName:"ol"},"If the file to be read is not HDFS/CFS and multiple files are matched using wildcards, you can also increase min_insert_block_size_rows and min_insert_block_size_bytes at the same time")))}m.isMDXComponent=!0},445:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/boxcnbeMipMsWmYggoqbR4DT88c-8f3b6d8f7092df4cbff90bfd5115de2c.png"},603:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/boxcnlEuwS4ogryA87WHPdDK1Md-d42dcff9e6716fe35060170db3994a06.png"}}]);