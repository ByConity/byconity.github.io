"use strict";(self.webpackChunkbyconity=self.webpackChunkbyconity||[]).push([[1691],{9613:(e,r,t)=>{t.d(r,{Zo:()=>i,kt:()=>m});var n=t(9496);function a(e,r,t){return r in e?Object.defineProperty(e,r,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[r]=t,e}function o(e,r){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);r&&(n=n.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),t.push.apply(t,n)}return t}function l(e){for(var r=1;r<arguments.length;r++){var t=null!=arguments[r]?arguments[r]:{};r%2?o(Object(t),!0).forEach((function(r){a(e,r,t[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(t,r))}))}return e}function s(e,r){if(null==e)return{};var t,n,a=function(e,r){if(null==e)return{};var t,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],r.indexOf(t)>=0||(a[t]=e[t]);return a}(e,r);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],r.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var c=n.createContext({}),u=function(e){var r=n.useContext(c),t=r;return e&&(t="function"==typeof e?e(r):l(l({},r),e)),t},i=function(e){var r=u(e.components);return n.createElement(c.Provider,{value:r},e.children)},p="mdxType",_={inlineCode:"code",wrapper:function(e){var r=e.children;return n.createElement(n.Fragment,{},r)}},d=n.forwardRef((function(e,r){var t=e.components,a=e.mdxType,o=e.originalType,c=e.parentName,i=s(e,["components","mdxType","originalType","parentName"]),p=u(t),d=a,m=p["".concat(c,".").concat(d)]||p[d]||_[d]||o;return t?n.createElement(m,l(l({ref:r},i),{},{components:t})):n.createElement(m,l({ref:r},i))}));function m(e,r){var t=arguments,a=r&&r.mdxType;if("string"==typeof e||a){var o=t.length,l=new Array(o);l[0]=d;var s={};for(var c in r)hasOwnProperty.call(r,c)&&(s[c]=r[c]);s.originalType=e,s[p]="string"==typeof e?e:a,l[1]=s;for(var u=2;u<o;u++)l[u]=t[u];return n.createElement.apply(null,l)}return n.createElement.apply(null,t)}d.displayName="MDXCreateElement"},4011:(e,r,t)=>{t.r(r),t.d(r,{assets:()=>c,contentTitle:()=>l,default:()=>_,frontMatter:()=>o,metadata:()=>s,toc:()=>u});var n=t(4778),a=(t(9496),t(9613));const o={title:"Monitor Cluster",tags:["Docs"]},l="Monitor Cluster",s={unversionedId:"basic-guide/monitoring",id:"basic-guide/monitoring",title:"Monitor Cluster",description:"Prometheus Monitoring Indicators\uff1a",source:"@site/docs/basic-guide/monitoring.mdx",sourceDirName:"basic-guide",slug:"/basic-guide/monitoring",permalink:"/docs/basic-guide/monitoring",draft:!1,editUrl:"https://github.com/ByConity/byconity.github.io/tree/main/docs/basic-guide/monitoring.mdx",tags:[{label:"Docs",permalink:"/docs/tags/docs"}],version:"current",frontMatter:{title:"Monitor Cluster",tags:["Docs"]},sidebar:"tutorialSidebar",previous:{title:"docker-wrapper",permalink:"/docs/basic-guide/docker-wrapper"},next:{title:"Package Deployment",permalink:"/docs/basic-guide/package-deployment"}},c={},u=[{value:"Prometheus Monitoring Indicators\uff1a",id:"prometheus-monitoring-indicators",level:2},{value:"VictoriaMetric Metric Aggregation\uff1a",id:"victoriametric-metric-aggregation",level:2},{value:"Important indicators",id:"important-indicators",level:3},{value:"Configure Grafana Kanban for the service node (Server)",id:"configure-grafana-kanban-for-the-service-node-server",level:3},{value:"Monitor TSO",id:"monitor-tso",level:2},{value:"important indicators",id:"important-indicators-1",level:3},{value:"Configure Grafana Kanban for TSO",id:"configure-grafana-kanban-for-tso",level:3},{value:"Other information that can be monitored",id:"other-information-that-can-be-monitored",level:2}],i={toc:u},p="wrapper";function _(e){let{components:r,...o}=e;return(0,a.kt)(p,(0,n.Z)({},i,o,{components:r,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"monitor-cluster"},"Monitor Cluster"),(0,a.kt)("h1",{id:"common-monitoring-indicators"},"Common Monitoring Indicators"),(0,a.kt)("h2",{id:"prometheus-monitoring-indicators"},"Prometheus Monitoring Indicators\uff1a"),(0,a.kt)("p",null,"The engine spits out monitoring items under the path of the HTTP interface ",(0,a.kt)("inlineCode",{parentName:"p"},"/metrics"),", the default port is 8123, and you can directly access the output of the corresponding port."),(0,a.kt)("p",null,"The corresponding metric output can be viewed through kubectl"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"kubectl port-forward -n cnch cnch-default-server-0 8123:8123\n# use port-forward function to proxy port\n\n")),(0,a.kt)("p",null,"After that, you can open localhost:8123/metrics with a browser, and you can view the indicator display as shown in the figure below. Each line corresponds to a specific index item, which conforms to the index format agreed by Prometheus.\n",(0,a.kt)("img",{src:t(4964).Z,width:"2208",height:"496"})),(0,a.kt)("h2",{id:"victoriametric-metric-aggregation"},"VictoriaMetric Metric Aggregation\uff1a"),(0,a.kt)("p",null,"Choose VictoriaMetric for the storage of indicators, which is convenient for horizontal expansion of storage and provides richer functions."),(0,a.kt)("p",null,"An important feature among these is the VMRule, which aggregates raw metrics. Because some of the original Prometheus indicators spit out by each component can be directly used to build monitoring alarms, the other part is more complicated and it is not easy to directly build monitoring dashboards and alarms, so it is aggregated through VWRule. The following is the rule configuration file cnch-metrics.yaml:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'# Source: victoria-rules/templates/cnch-metrics.yaml\napiVersion: operator.victoriametrics.com/v1beta1\nkind: VMRule\nmetadata:\n  name: release-name-victoria-rule-cnch-metrics\n  namespace: cnch-operator-default-system\n  labels:\n    app: victoria-rules\n\n    chart: victoria-rules-0.1.6\n    release: "release-name"\n    heritage: "Helm"\nspec:\n  groups:\n    - name: CnchMetricsLatency\n      rules:\n        # Histogram at VW level\n        - record: cnch:latency:queries_vw:pct95\n          expr: |-\n            histogram_quantile(0.95,\n              sum by (cluster, namespace, vw, le)(\n                rate(cnch_histogram_metrics_query_latency_bucket[5m])\n              )\n            )\n        # Histogram at Cluster level\n        - record: cnch:latency:queries_cluster:pct95\n          expr: |-\n            histogram_quantile(0.95,\n              sum by (cluster, namespace, le)(\n                rate(cnch_histogram_metrics_query_latency_bucket[5m])\n              )\n            )\n\n        # Trends Metrics\n        # Trend Latency VW level\n        - record: cnch:latency:queries_vw:pct95:avg_1d\n          expr: avg_over_time(cnch:latency:queries_vw:pct95[1d])\n\n        # Trend Latency Cluster level\n        - record: cnch:latency:queries_cluster:pct95:avg_1d\n          expr: avg_over_time(cnch:latency:queries_cluster:pct95[1d])\n        # Histogram at VW level\n        - record: cnch:latency:queries_vw:pct99\n          expr: |-\n            histogram_quantile(0.99,\n              sum by (cluster, namespace, vw, le)(\n                rate(cnch_histogram_metrics_query_latency_bucket[5m])\n              )\n            )\n        # Histogram at Cluster level\n        - record: cnch:latency:queries_cluster:pct99\n          expr: |-\n            histogram_quantile(0.99,\n              sum by (cluster, namespace, le)(\n                rate(cnch_histogram_metrics_query_latency_bucket[5m])\n              )\n            )\n\n        # Trends Metrics\n        # Trend Latency VW level\n        - record: cnch:latency:queries_vw:pct99:avg_1d\n          expr: avg_over_time(cnch:latency:queries_vw:pct99[1d])\n\n        # Trend Latency Cluster level\n        - record: cnch:latency:queries_cluster:pct99:avg_1d\n          expr: avg_over_time(cnch:latency:queries_cluster:pct99[1d])\n\n        # Trend Slow Q VW level\n        - record: cnch:latency:queries_vw:slow_ratio:avg_1d\n          expr: avg_over_time(cnch:latency:queries_vw:slow_ratio[1d])\n\n        # Trend Slow Q Cluster level\n        - record: cnch:latency:queries_cluster:slow_ratio:avg_1d\n          expr: avg_over_time(cnch:latency:queries_cluster:slow_ratio[1d])\n\n        # Slow Q VW level  (Percentage of query > 10s)\n        - record: cnch:latency:queries_vw:slow_ratio\n          expr: |-\n            sum by (cluster, namespace, vw)(\n              rate(cnch_histogram_metrics_query_latency_count[5m])\n              - on (namespace, pod, cluster, vw, instance) rate(cnch_histogram_metrics_query_latency_bucket{le="10000"}[5m])\n            )\n            /\n            sum by (cluster, namespace, vw)(\n              rate(cnch_histogram_metrics_query_latency_count[5m])\n            )\n\n        # Slow Q Cluster level (Percentage of query > 10s)\n        - record: cnch:latency:queries_cluster:slow_ratio\n          expr: |-\n            sum by (cluster, namespace)(\n              rate(cnch_histogram_metrics_query_latency_count[5m])\n              - on (namespace, pod, cluster, vw, instance) rate(cnch_histogram_metrics_query_latency_bucket{le="10000"}[5m])\n            )\n            /\n            sum by (cluster, namespace)(\n              rate(cnch_histogram_metrics_query_latency_count[5m])\n            )\n\n        # Slow Q Cluster level (count queries > 10s) used by OP portal\n        - record: cnch:latency:queries_cluster:slow_count\n          expr: |-\n            sum by (cluster, namespace)(\n              increase(cnch_histogram_metrics_query_latency_count[1h])\n              - on (namespace, pod, cluster, vw, instance) increase(cnch_histogram_metrics_query_latency_bucket{le="10000"}[1h])\n            )\n\n        # Todo check if this metric became server only\n        - record: cnch:latency:queries_timeout:rate5m\n          expr: |-\n            sum by (cluster, namespace, pod, workload) (\n              rate(cnch_profile_events_timed_out_query_total[5m])\n              * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel\n            )\n\n    - name: CnchMetricsQPS\n      rules:\n        # Trend WG workload level\n        # - record: cnch:profile_events:query:total_rate5m:avg_1d\n        #   expr: avg_over_time(sum by (cluster, namespace, workload, type) (cnch:profile_events:query:total_rate5m)[1d])\n\n        # Trend VW QPS VW level. server POV only\n        - record: cnch:profile_events:labelled_query_vw:total_rate5m:avg_1d\n          expr: avg_over_time(sum by (cluster, namespace, vw, query_type) (cnch:profile_events:labelled_query_vw:total_rate5m)[1d])\n\n        # VW QPS cluster level Todo use sum(avg_1d{vw != ""}) if no similar reenable this Trend\n        - record: cnch:profile_events:labelled_query_cluster:total_rate5m:avg_1d\n          expr: |-\n            avg_over_time(sum by (cluster, namespace, query_type) (cnch:profile_events:labelled_query_vw:total_rate5m)[1d])\n\n        # Trend VW Error Ratio VW level (can\'t sum burnrate % so we pre-recorded a burnrate summed at vw level)\n        - record: cnch:profile_events:labelled_query_vw_sum:error_burnrate5m:avg_1d\n          expr: |-\n            avg_over_time(cnch:profile_events:labelled_query_vw_sum:error_burnrate5m[1d])\n\n        # Number of workers in a WG that use more than 80% memory\n        - record: cnch:workers:high_mem_rss:80pct_count\n          expr: |-\n            (\n              count(\n                (sum(\n                    container_memory_rss{container!="", image!=""}\n                  * on(namespace,pod)\n                    group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{workload=~"cnch.*worker.*|vw.*"}\n                ) by (pod, namespace, workload)\n                / sum(\n                    kube_pod_container_resource_limits{resource="memory"}\n                  * on(namespace,pod)\n                    group_left(workload, workload_type) namespace_workload_pod:kube_pod_owner:relabel{workload=~"cnch.*worker.*|vw.*"}\n                ) by (pod, namespace, workload)) > 0.80\n              ) by (namespace, workload)\n            /\n              count(namespace_workload_pod:kube_pod_owner:relabel{workload=~"cnch.*worker.*|vw.*"}) by (namespace, workload)\n            )\n\n        # Byteyard Usage Profiler metrics\n        - record: cnch:vw:metrics:running_queries:time_milliseconds_total\n          expr: sum by (vw_id, cluster) (increase(cnch_internal_metrics_running_queries_time_milliseconds_total[30s]))\n        - record: cnch:vw:metrics:queued_queries:time_milliseconds_total\n          expr: sum by (vw_id, cluster) (increase(cnch_internal_metrics_queued_queries_time_milliseconds_total[30s]))\n\n      # Query Error Ratio over multiple intervals aka burn rate\n\n        # Record server POV, for vw only, the unlimited are only used for few dashboard and 1 alert rule\n        # Worker POV is used in workers dashboard only\n        - record: cnch:profile_events:labelled_query_vw:total_rate5m\n          expr: |-\n            sum(rate(cnch_profile_events_labelled_query_total{resource_type="vw"}[5m])) by (pod, cluster, namespace, query_type, vw, wg)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel{workload=~".*server.*"}\n\n        # Record workers POV, used by Byteyard autosuspend (server pov might not have direct insert) and workers graph\n        - record: cnch:profile_events:labelled_query_vw_workers:total_rate5m\n          expr: |-\n            sum(rate(cnch_profile_events_labelled_query_total{resource_type="vw"}[5m])) by (pod, cluster, namespace)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel{workload!~".*server.*"}\n        # TEMP until byteyard support cnch:profile_events:labelled_query_vw_workers\n        # TODO remove this\n        - record: cnch:profile_event:queries_vw_only:total_rate5m\n          expr: |-\n            cnch:profile_events:labelled_query_vw_workers:total_rate5m\n\n        - record: cnch:tso:requests:total_rate5m\n          expr: |-\n            sum(rate(cnch_profile_events_tso_request_total[5m])) by (pod, cluster, namespace)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel\n\n        # Err/s default to 0 if a request total exist (e.g. only success request) so it\'s included in availability\n        - record: cnch:profile_events:labelled_query_vw:error_rate5m\n          expr: |\n            ((\n              sum(rate(cnch_profile_events_queries_failed_total{failure_type!="QueriesFailedFromUser", resource_type="vw"}[5m])) by (pod, cluster, namespace, query_type, vw, wg)\n            )\n            or\n            (\n              0 * group by (pod, cluster, namespace, resource_type, query_type, vw, wg) (cnch:profile_events:labelled_query_vw:total_rate5m)\n            ))\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel{workload=~".*server.*"}\n        - record: cnch:tso:requests:error_rate5m\n          expr: |\n            ((\n              sum(rate(cnch_profile_events_tso_error_total[5m])) by (pod, cluster, namespace)\n            )\n            or\n            (\n              0 * group by (pod, cluster, namespace) (cnch:tso:requests:total_rate5m)\n            ))\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel\n\n        # Use WG level precision\n        - record: cnch:profile_events:labelled_query_vw:error_burnrate5m\n          expr: |\n            sum(cnch:profile_events:labelled_query_vw:error_rate5m) by (workload, cluster, namespace, vw, wg)\n            /\n            sum(cnch:profile_events:labelled_query_vw:total_rate5m) by (workload, cluster, namespace, vw, wg)\n\n        - record: cnch:tso:requests:error_burnrate5m\n          expr: |\n            sum(cnch:tso:requests:error_rate5m) by (workload, cluster, namespace)\n                /\n            sum(cnch:tso:requests:total_rate5m) by (workload, cluster, namespace)\n\n        # Record server POV, for vw only, the unlimited are only used for few dashboard and 1 alert rule\n        # Worker POV is used in workers dashboard only\n        - record: cnch:profile_events:labelled_query_vw:total_rate30m\n          expr: |-\n            sum(rate(cnch_profile_events_labelled_query_total{resource_type="vw"}[30m])) by (pod, cluster, namespace, query_type, vw, wg)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel{workload=~".*server.*"}\n\n        # Record workers POV, used by Byteyard autosuspend (server pov might not have direct insert) and workers graph\n        - record: cnch:profile_events:labelled_query_vw_workers:total_rate30m\n          expr: |-\n            sum(rate(cnch_profile_events_labelled_query_total{resource_type="vw"}[30m])) by (pod, cluster, namespace)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel{workload!~".*server.*"}\n        # TEMP until byteyard support cnch:profile_events:labelled_query_vw_workers\n        # TODO remove this\n        - record: cnch:profile_event:queries_vw_only:total_rate30m\n          expr: |-\n            cnch:profile_events:labelled_query_vw_workers:total_rate30m\n\n        - record: cnch:tso:requests:total_rate30m\n          expr: |-\n            sum(rate(cnch_profile_events_tso_request_total[30m])) by (pod, cluster, namespace)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel\n\n        # Err/s default to 0 if a request total exist (e.g. only success request) so it\'s included in availability\n        - record: cnch:profile_events:labelled_query_vw:error_rate30m\n          expr: |\n            ((\n              sum(rate(cnch_profile_events_queries_failed_total{failure_type!="QueriesFailedFromUser", resource_type="vw"}[30m])) by (pod, cluster, namespace, query_type, vw, wg)\n            )\n            or\n            (\n              0 * group by (pod, cluster, namespace, resource_type, query_type, vw, wg) (cnch:profile_events:labelled_query_vw:total_rate30m)\n            ))\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel{workload=~".*server.*"}\n        - record: cnch:tso:requests:error_rate30m\n          expr: |\n            ((\n              sum(rate(cnch_profile_events_tso_error_total[30m])) by (pod, cluster, namespace)\n            )\n            or\n            (\n              0 * group by (pod, cluster, namespace) (cnch:tso:requests:total_rate30m)\n            ))\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel\n\n        # Use WG level precision\n        - record: cnch:profile_events:labelled_query_vw:error_burnrate30m\n          expr: |\n            sum(cnch:profile_events:labelled_query_vw:error_rate30m) by (workload, cluster, namespace, vw, wg)\n            /\n            sum(cnch:profile_events:labelled_query_vw:total_rate30m) by (workload, cluster, namespace, vw, wg)\n\n        - record: cnch:tso:requests:error_burnrate30m\n          expr: |\n            sum(cnch:tso:requests:error_rate30m) by (workload, cluster, namespace)\n                /\n            sum(cnch:tso:requests:total_rate30m) by (workload, cluster, namespace)\n\n        # Record server POV, for vw only, the unlimited are only used for few dashboard and 1 alert rule\n        # Worker POV is used in workers dashboard only\n        - record: cnch:profile_events:labelled_query_vw:total_rate1h\n          expr: |-\n            sum(rate(cnch_profile_events_labelled_query_total{resource_type="vw"}[1h])) by (pod, cluster, namespace, query_type, vw, wg)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel{workload=~".*server.*"}\n\n        # Record workers POV, used by Byteyard autosuspend (server pov might not have direct insert) and workers graph\n        - record: cnch:profile_events:labelled_query_vw_workers:total_rate1h\n          expr: |-\n            sum(rate(cnch_profile_events_labelled_query_total{resource_type="vw"}[1h])) by (pod, cluster, namespace)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel{workload!~".*server.*"}\n        # TEMP until byteyard support cnch:profile_events:labelled_query_vw_workers\n        # TODO remove this\n        - record: cnch:profile_event:queries_vw_only:total_rate1h\n          expr: |-\n            cnch:profile_events:labelled_query_vw_workers:total_rate1h\n\n        - record: cnch:tso:requests:total_rate1h\n          expr: |-\n            sum(rate(cnch_profile_events_tso_request_total[1h])) by (pod, cluster, namespace)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel\n\n        # Err/s default to 0 if a request total exist (e.g. only success request) so it\'s included in availability\n        - record: cnch:profile_events:labelled_query_vw:error_rate1h\n          expr: |\n            ((\n              sum(rate(cnch_profile_events_queries_failed_total{failure_type!="QueriesFailedFromUser", resource_type="vw"}[1h])) by (pod, cluster, namespace, query_type, vw, wg)\n            )\n            or\n            (\n              0 * group by (pod, cluster, namespace, resource_type, query_type, vw, wg) (cnch:profile_events:labelled_query_vw:total_rate1h)\n            ))\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel{workload=~".*server.*"}\n        - record: cnch:tso:requests:error_rate1h\n          expr: |\n            ((\n              sum(rate(cnch_profile_events_tso_error_total[1h])) by (pod, cluster, namespace)\n            )\n            or\n            (\n              0 * group by (pod, cluster, namespace) (cnch:tso:requests:total_rate1h)\n            ))\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel\n\n        # Use WG level precision\n        - record: cnch:profile_events:labelled_query_vw:error_burnrate1h\n          expr: |\n            sum(cnch:profile_events:labelled_query_vw:error_rate1h) by (workload, cluster, namespace, vw, wg)\n            /\n            sum(cnch:profile_events:labelled_query_vw:total_rate1h) by (workload, cluster, namespace, vw, wg)\n\n        - record: cnch:tso:requests:error_burnrate1h\n          expr: |\n            sum(cnch:tso:requests:error_rate1h) by (workload, cluster, namespace)\n                /\n            sum(cnch:tso:requests:total_rate1h) by (workload, cluster, namespace)\n\n        # Record server POV, for vw only, the unlimited are only used for few dashboard and 1 alert rule\n        # Worker POV is used in workers dashboard only\n        - record: cnch:profile_events:labelled_query_vw:total_rate6h\n          expr: |-\n            sum(rate(cnch_profile_events_labelled_query_total{resource_type="vw"}[6h])) by (pod, cluster, namespace, query_type, vw, wg)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel{workload=~".*server.*"}\n\n        # Record workers POV, used by Byteyard autosuspend (server pov might not have direct insert) and workers graph\n        - record: cnch:profile_events:labelled_query_vw_workers:total_rate6h\n          expr: |-\n            sum(rate(cnch_profile_events_labelled_query_total{resource_type="vw"}[6h])) by (pod, cluster, namespace)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel{workload!~".*server.*"}\n        # TEMP until byteyard support cnch:profile_events:labelled_query_vw_workers\n        # TODO remove this\n        - record: cnch:profile_event:queries_vw_only:total_rate6h\n          expr: |-\n            cnch:profile_events:labelled_query_vw_workers:total_rate6h\n\n        - record: cnch:tso:requests:total_rate6h\n          expr: |-\n            sum(rate(cnch_profile_events_tso_request_total[6h])) by (pod, cluster, namespace)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel\n\n        # Err/s default to 0 if a request total exist (e.g. only success request) so it\'s included in availability\n        - record: cnch:profile_events:labelled_query_vw:error_rate6h\n          expr: |\n            ((\n              sum(rate(cnch_profile_events_queries_failed_total{failure_type!="QueriesFailedFromUser", resource_type="vw"}[6h])) by (pod, cluster, namespace, query_type, vw, wg)\n            )\n            or\n            (\n              0 * group by (pod, cluster, namespace, resource_type, query_type, vw, wg) (cnch:profile_events:labelled_query_vw:total_rate6h)\n            ))\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel{workload=~".*server.*"}\n        - record: cnch:tso:requests:error_rate6h\n          expr: |\n            ((\n              sum(rate(cnch_profile_events_tso_error_total[6h])) by (pod, cluster, namespace)\n            )\n            or\n            (\n              0 * group by (pod, cluster, namespace) (cnch:tso:requests:total_rate6h)\n            ))\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel\n\n        # Use WG level precision\n        - record: cnch:profile_events:labelled_query_vw:error_burnrate6h\n          expr: |\n            sum(cnch:profile_events:labelled_query_vw:error_rate6h) by (workload, cluster, namespace, vw, wg)\n            /\n            sum(cnch:profile_events:labelled_query_vw:total_rate6h) by (workload, cluster, namespace, vw, wg)\n\n        - record: cnch:tso:requests:error_burnrate6h\n          expr: |\n            sum(cnch:tso:requests:error_rate6h) by (workload, cluster, namespace)\n                /\n            sum(cnch:tso:requests:total_rate6h) by (workload, cluster, namespace)\n\n        # Record server POV, for vw only, the unlimited are only used for few dashboard and 1 alert rule\n        # Worker POV is used in workers dashboard only\n        - record: cnch:profile_events:labelled_query_vw:total_rate3d\n          expr: |-\n            sum(rate(cnch_profile_events_labelled_query_total{resource_type="vw"}[3d])) by (pod, cluster, namespace, query_type, vw, wg)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel{workload=~".*server.*"}\n\n        # Record workers POV, used by Byteyard autosuspend (server pov might not have direct insert) and workers graph\n        - record: cnch:profile_events:labelled_query_vw_workers:total_rate3d\n          expr: |-\n            sum(rate(cnch_profile_events_labelled_query_total{resource_type="vw"}[3d])) by (pod, cluster, namespace)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel{workload!~".*server.*"}\n        # TEMP until byteyard support cnch:profile_events:labelled_query_vw_workers\n        # TODO remove this\n        - record: cnch:profile_event:queries_vw_only:total_rate3d\n          expr: |-\n            cnch:profile_events:labelled_query_vw_workers:total_rate3d\n\n        - record: cnch:tso:requests:total_rate3d\n          expr: |-\n            sum(rate(cnch_profile_events_tso_request_total[3d])) by (pod, cluster, namespace)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel\n\n        # Err/s default to 0 if a request total exist (e.g. only success request) so it\'s included in availability\n        - record: cnch:profile_events:labelled_query_vw:error_rate3d\n          expr: |\n            ((\n              sum(rate(cnch_profile_events_queries_failed_total{failure_type!="QueriesFailedFromUser", resource_type="vw"}[3d])) by (pod, cluster, namespace, query_type, vw, wg)\n            )\n            or\n            (\n              0 * group by (pod, cluster, namespace, resource_type, query_type, vw, wg) (cnch:profile_events:labelled_query_vw:total_rate3d)\n            ))\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel{workload=~".*server.*"}\n        - record: cnch:tso:requests:error_rate3d\n          expr: |\n            ((\n              sum(rate(cnch_profile_events_tso_error_total[3d])) by (pod, cluster, namespace)\n            )\n            or\n            (\n              0 * group by (pod, cluster, namespace) (cnch:tso:requests:total_rate3d)\n            ))\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel\n\n        # Use WG level precision\n        - record: cnch:profile_events:labelled_query_vw:error_burnrate3d\n          expr: |\n            sum(cnch:profile_events:labelled_query_vw:error_rate3d) by (workload, cluster, namespace, vw, wg)\n            /\n            sum(cnch:profile_events:labelled_query_vw:total_rate3d) by (workload, cluster, namespace, vw, wg)\n\n        - record: cnch:tso:requests:error_burnrate3d\n          expr: |\n            sum(cnch:tso:requests:error_rate3d) by (workload, cluster, namespace)\n                /\n            sum(cnch:tso:requests:total_rate3d) by (workload, cluster, namespace)\n\n        # Use VW level precision only 5m timeframe used for dashboard only (trend avg_1d)\n        - record: cnch:profile_events:labelled_query_vw_sum:error_burnrate5m\n          expr: |\n            sum(cnch:profile_events:labelled_query_vw:error_rate5m) by (cluster, namespace, vw)\n            /\n            sum(cnch:profile_events:labelled_query_vw:total_rate5m) by (cluster, namespace, vw)\n\n        # Only used for few dashboard and 1 error alert rule, no need burn rate worker+ server POV\n        - record: cnch:profile_events:labelled_query_unlimited:total_rate5m\n          expr: |-\n            sum(rate(cnch_profile_events_labelled_query_total{resource_type="unlimited"}[5m])) by (pod, cluster, namespace, query_type)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel\n\n        - record: cnch:profile_events:labelled_query_unlimited:error_rate5m\n          expr: |-\n            sum(rate(cnch_profile_events_queries_failed_total{failure_type!="QueriesFailedFromUser", resource_type="unlimited"}[5m])) by (pod, cluster, namespace, query_type, vw, wg)\n            * on (pod, namespace) group_left(workload) namespace_workload_pod:kube_pod_owner:relabel\n\n    - name: CnchMetricsAvailability\n      rules:\n        - record: cnch:wg:availability\n          labels:\n            slo: error_rate\n          # 1 = available, 0 = unavailable\n          # min() check if any of the burn rate is firing (1, 1, 0) -> 0\n          # For any burn rate, both time window must be triggered (Multiwindow) so we use max() (1, 0) -> 1 avail\n          # TODO maybe change this with a ALERTS{alertstate="firing",severity="critical", alertname=~".*BudgetBurn"}\n          # As we can\'t have the \'for 15m\' here see: https://github.com/metalmatze/slo-libsonnet/issues/52\n          expr: |\n            min by (cluster, namespace, vw, wg) (\n              max by (cluster, namespace, vw, wg) (\n                cnch:profile_events:labelled_query_vw:error_burnrate5m{vw=~".*"} <= bool (14.40 * (1 - 0.99)),\n                cnch:profile_events:labelled_query_vw:error_burnrate1h{vw=~".*"} <= bool (14.40 * (1 - 0.99))\n              ),\n              max by (cluster, namespace, vw, wg) (\n                cnch:profile_events:labelled_query_vw:error_burnrate30m{vw=~".*"} <= bool (6.00 * (1 - 0.99)),\n                cnch:profile_events:labelled_query_vw:error_burnrate6h{vw=~".*"} <= bool (6.00 * (1 - 0.99))\n              ),\n              max by (cluster, namespace, vw, wg) (\n                cnch:profile_events:labelled_query_vw:error_burnrate6h{vw=~".*"} <= bool (1.00 * (1 - 0.99)),\n                cnch:profile_events:labelled_query_vw:error_burnrate3d{vw=~".*"} <= bool (1.00 * (1 - 0.99))\n              )\n            )\n        - record: cnch:cluster:availability\n          expr: |\n            1 - (sum by (cluster, namespace) (cnch:profile_events:labelled_query_vw:error_rate5m)\n            /\n            sum by (cluster, namespace) (cnch:profile_events:labelled_query_vw:total_rate5m))\n\n')),(0,a.kt)("p",null,"Use kubectl to execute the configuration to take effect:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"kubctl apply -f cnch-metrics.yaml # Configure the corresponding rule\n\n")),(0,a.kt)("p",null,"Monitoring service node (Server)"),(0,a.kt)("h3",{id:"important-indicators"},"Important indicators"),(0,a.kt)("p",null,"The more important indicators are excerpted below for explanation"),(0,a.kt)("table",null,(0,a.kt)("thead",{parentName:"table"},(0,a.kt)("tr",{parentName:"thead"},(0,a.kt)("th",{parentName:"tr",align:null},"Metric name (the ones in double quotes are VM-aggregated)\uff09"),(0,a.kt)("th",{parentName:"tr",align:null},"Description"))),(0,a.kt)("tbody",{parentName:"table"},(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"cnch:latency:queries_cluster:pct95 cnch:latency:queries_cluster:pct99"),(0,a.kt)("td",{parentName:"tr",align:null},"Query latency pct99 and pct55")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"cnch:latency:queries_cluster:slow_ratio"),(0,a.kt)("td",{parentName:"tr",align:null},"The proportion of slow queries longer than 10s")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"cnch:profile_events:labelled_query_vw:total_rate5m"),(0,a.kt)("td",{parentName:"tr",align:null},"Total QPS of all VW")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"cnch:profile_events:labelled_query_vw:error_rate5m"),(0,a.kt)("td",{parentName:"tr",align:null},"Failed QPS for all VWs")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"cnch_current_metrics_query"),(0,a.kt)("td",{parentName:"tr",align:null},"The value of the label name query_type is insert, which is the written query")))),(0,a.kt)("h3",{id:"configure-grafana-kanban-for-the-service-node-server"},"Configure Grafana Kanban for the service node (Server)"),(0,a.kt)("p",null,"Kanban content see screenshot"),(0,a.kt)("p",null,(0,a.kt)("img",{src:t(7511).Z,width:"3337",height:"9813"})),(0,a.kt)("p",null,"Important indicators:"),(0,a.kt)("table",null,(0,a.kt)("thead",{parentName:"table"},(0,a.kt)("tr",{parentName:"thead"},(0,a.kt)("th",{parentName:"tr",align:null},"Kanban Name"),(0,a.kt)("th",{parentName:"tr",align:null},"Expression"),(0,a.kt)("th",{parentName:"tr",align:null},"Description"))),(0,a.kt)("tbody",{parentName:"table"},(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"Queries Ducations"),(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},'cnch:latency:queries_cluster:pct95{namespace="$namespace", cluster="$cluster"}\u548ccnch:latency:queries_cluster:pct99{namespace="$namespace", cluster="$cluster"}')),(0,a.kt)("td",{parentName:"tr",align:null},"Query latency P99 \u548c P95")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"Slow Queries > 10s"),(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},'cnch:latency:queries_cluster:slow_ratio{namespace="$namespace", cluster="$cluster"}')),(0,a.kt)("td",{parentName:"tr",align:null},"The proportion of slow queries longer than 10s")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"Queries Per Second"),(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},'sum(cnch:profile_events:labelled_query_vw:total_rate5m{namespace="$namespace", cluster="$cluster", workload=~"$workload"})')),(0,a.kt)("td",{parentName:"tr",align:null},"Total QPS for all VWs")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"VW Queries Success"),(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},'1 - (sum by (pod) (cnch:profile_events:labelled_query_vw:error_rate5m{cluster="$cluster", namespace="$namespace", workload=~"$workload", pod=~"$pod"}) sum by (pod) (cnch:profile_events:labelled_query_vw:total_rate5m{cluster="$cluster", namespace="$namespace", workload=~"$workload", pod=~"$pod"}))')),(0,a.kt)("td",{parentName:"tr",align:null},"After subtracting and dividing error_rate5m and total_rate5m, the success rate is obtained")))),(0,a.kt)("p",null,"The complete Grafana configuration file of the Server is as follows, which can be imported in the Grafana UI\uff1a",(0,a.kt)("a",{target:"_blank",href:t(4690).Z},"cnch-server.json")),(0,a.kt)("h2",{id:"monitor-tso"},"Monitor TSO"),(0,a.kt)("h3",{id:"important-indicators-1"},"important indicators"),(0,a.kt)("p",null,"The following excerpts are some important indicators for TSO to illustrate:"),(0,a.kt)("table",null,(0,a.kt)("thead",{parentName:"table"},(0,a.kt)("tr",{parentName:"thead"},(0,a.kt)("th",{parentName:"tr",align:null},"Indicators"),(0,a.kt)("th",{parentName:"tr",align:null},"Description"))),(0,a.kt)("tbody",{parentName:"table"},(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"cnch:tso:requests:error_rate5m"),(0,a.kt)("td",{parentName:"tr",align:null},"Failed QPS for TSO components")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"cnch:tso:requests:total_rate5m"),(0,a.kt)("td",{parentName:"tr",align:null},"Total QPS for TSO components")))),(0,a.kt)("h3",{id:"configure-grafana-kanban-for-tso"},"Configure Grafana Kanban for TSO"),(0,a.kt)("p",null,"The screenshot of the board is as follows:"),(0,a.kt)("p",null,(0,a.kt)("img",{src:t(4886).Z,width:"3355",height:"5626"})),(0,a.kt)("p",null,"Important indicators:"),(0,a.kt)("table",null,(0,a.kt)("thead",{parentName:"table"},(0,a.kt)("tr",{parentName:"thead"},(0,a.kt)("th",{parentName:"tr",align:null},"Kanban Name"),(0,a.kt)("th",{parentName:"tr",align:null},"Expression"),(0,a.kt)("th",{parentName:"tr",align:null},"Description"))),(0,a.kt)("tbody",{parentName:"table"},(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"TSO Server Requests Per Sec"),(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},'cnch:tso:requests:total_rate5m{namespace="$namespace", cluster="$cluster", workload=~".*server.*"}')),(0,a.kt)("td",{parentName:"tr",align:null},"QPS of Server component for TSO query")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"TSO Worker Requests Per Sec"),(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},'cnch:tso:requests:total_rate5m{namespace="$namespace", cluster="$cluster", workload!~".*(server\\|**kafka**).*"}')),(0,a.kt)("td",{parentName:"tr",align:null},"Remove the server and kafka, and only look at the request QPS of each worker for TSO")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"TSO Servers Requests Server Rate"),(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},'cnch:tso:requests:error_rate5m{namespace="$namespace", cluster="$cluster", workload=~".*server.*"} cnch:tso:requests:total_rate5m{namespace="$namespace", cluster="$cluster", workload=~".*server.*"}')),(0,a.kt)("td",{parentName:"tr",align:null},"Divide error_rate and total_rate to filter the failure rate of TSO query")))),(0,a.kt)("p",null,"The complete configuration file of TSO is as follows:",(0,a.kt)("a",{target:"_blank",href:t(9416).Z},"cnch-tso.json")),(0,a.kt)("h2",{id:"other-information-that-can-be-monitored"},"Other information that can be monitored"),(0,a.kt)("p",null,"Other commonly used monitoring board configurations are listed here, no more screenshots"),(0,a.kt)("p",null,"Cluster Overview: Overview of the entire cluster ",(0,a.kt)("a",{target:"_blank",href:t(9416).Z},"cnch-cluster.json")),(0,a.kt)("p",null,"VW: detials of each Virtual Warehouse ",(0,a.kt)("a",{target:"_blank",href:t(754).Z},"cnch-vw.json")),(0,a.kt)("p",null,"DaemonManager: Components that manages background tasks such as Merge ",(0,a.kt)("a",{target:"_blank",href:t(8914).Z},"cnch-daemonmanager.json")))}_.isMDXComponent=!0},9416:(e,r,t)=>{t.d(r,{Z:()=>n});const n=t.p+"assets/files/cnch-cluster-defa551fd417f26c9d0c8e0c0e9c8613.json"},8914:(e,r,t)=>{t.d(r,{Z:()=>n});const n=t.p+"assets/files/cnch-daemonmanager-50a069af4bc2e5eb5bd58652959bb1f8.json"},4690:(e,r,t)=>{t.d(r,{Z:()=>n});const n=t.p+"assets/files/cnch-server-9c3a5e54b12f5d1f9fd04500abba5385.json"},754:(e,r,t)=>{t.d(r,{Z:()=>n});const n=t.p+"assets/files/cnch-vw-9aaaa58db804949d69e5f0f302c1546e.json"},4886:(e,r,t)=>{t.d(r,{Z:()=>n});const n=t.p+"assets/images/boxcn3CLoRUlpCEDJnEy8f6dxPe-9a6fde5cfab3aa39288f253a549a1c39.jpeg"},4964:(e,r,t)=>{t.d(r,{Z:()=>n});const n=t.p+"assets/images/boxcnMqU9e8xvq46v7IH9ORtLCf-69b89b581b04f353e3f545df9f414d45.png"},7511:(e,r,t)=>{t.d(r,{Z:()=>n});const n=t.p+"assets/images/boxcnvwueXWFISCRgsRJ66J2vKb-8988c2af21fed0034abda7d7d9f4154c.jpeg"}}]);