<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">4 posts tagged with &quot;docusaurus&quot; | ByConity</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://byconity.github.io/img/byconity-social-card.png"><meta data-rh="true" name="twitter:image" content="https://byconity.github.io/img/byconity-social-card.png"><meta data-rh="true" property="og:url" content="https://byconity.github.io/blog/tags/docusaurus"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="4 posts tagged with &quot;docusaurus&quot; | ByConity"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://byconity.github.io/blog/tags/docusaurus"><link data-rh="true" rel="alternate" href="https://byconity.github.io/blog/tags/docusaurus" hreflang="en"><link data-rh="true" rel="alternate" href="https://byconity.github.io/zh-cn/blog/tags/docusaurus" hreflang="zh-cn"><link data-rh="true" rel="alternate" href="https://byconity.github.io/blog/tags/docusaurus" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://Y8UJSN6KEB-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="ByConity RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="ByConity Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BYY7CCPJZ6"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-BYY7CCPJZ6",{})</script>


<link rel="search" type="application/opensearchdescription+xml" title="ByConity" href="/opensearch.xml"><link rel="stylesheet" href="/assets/css/styles.25c3bd5d.css">
<link rel="preload" href="/assets/js/runtime~main.722bd2dd.js" as="script">
<link rel="preload" href="/assets/js/main.8d48683e.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_Rzz1" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_hfek"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.png" alt="ByConity Logo" class="themedImage_DeRy themedImage--light_hbln"><img src="/img/logo.png" alt="ByConity Logo" class="themedImage_DeRy themedImage--dark_qgR1"></div><b class="navbar__title text--truncate">ByConity</b></a><a class="navbar__item navbar__link" href="/docs/introduction/what-is-byconity">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/community/become-maintainer">Community</a><a class="navbar__item navbar__link" href="/users">Users</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link" aria-haspopup="true" aria-expanded="false" role="button" href="/docs/introduction/what-is-byconity">0.4.x</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/introduction/what-is-byconity">0.4.x</a></li><li><a class="dropdown__link" href="/docs/0.3.0/introduction/what-is-byconity">0.3.0</a></li><li><a class="dropdown__link" href="/docs/0.2.0/introduction/main-principle-concepts">0.2.0</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">English</a><ul class="dropdown__menu"><li><a href="/blog/tags/docusaurus" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/zh-cn/blog/tags/docusaurus" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-cn">中文（中国）</a></li></ul></div><a href="https://github.com/ByConity/ByConity" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link"><span>GitHub</span></a><div class="searchBox_mmj0"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_NWrx"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_J5VD thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_ZkTY margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Jaez clean-list"><li class="sidebarItem_mMbS"><a class="sidebarItemLink_LJmi" href="/blog/byconity-elt">Detailed Explanation of ByConity ELT Principles</a></li><li class="sidebarItem_mMbS"><a class="sidebarItemLink_LJmi" href="/blog/byconity-benchmark">Performance Comparison Analysis of ByConity and Mainstream Open-Source OLAP Engines (ClickHouse, Doris, Presto)</a></li><li class="sidebarItem_mMbS"><a class="sidebarItemLink_LJmi" href="/blog/2023-05-24-byconity-announcement-opensources-its-cloudnative-data-warehouse">ByteDance Open Sources Its Cloud Native Data Warehouse ByConity</a></li><li class="sidebarItem_mMbS"><a class="sidebarItemLink_LJmi" href="/blog/byconity-an-opensource-cloudnative-data-warehouse-post">ByConity -- An Open source Cloud-native Data Warehouse</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>4 posts tagged with &quot;docusaurus&quot;</h1><a href="/blog/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_XMSB" itemprop="headline"><a itemprop="url" href="/blog/byconity-elt">Detailed Explanation of ByConity ELT Principles</a></h2><div class="container_fJtn margin-vert--md"><time datetime="2023-09-10T00:00:00.000Z" itemprop="datePublished">September 10, 2023</time> · <!-- -->10 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_YzU5"><div class="avatar margin-bottom--sm"><a href="https://github.com/WangTaoTheTonic" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/WangTaoTheTonic.png" alt="Tao Wang"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/WangTaoTheTonic" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Tao Wang</span></a></div><small class="avatar__subtitle" itemprop="description">ByConity maintainer</small></div></div></div><div class="col col--6 authorCol_YzU5"><div class="avatar margin-bottom--sm"><a href="https://github.com/tigerwangyb" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/tigerwangyb.png" alt="Yunbo Wang"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/tigerwangyb" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Yunbo Wang</span></a></div><small class="avatar__subtitle" itemprop="description">ByConity maintainer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="background">Background<a href="#background" class="hash-link" aria-label="Direct link to Background" title="Direct link to Background">​</a></h2><p>When it comes to data warehouses, the use of Extract-Transform-Load (ETL) or Extract-Load-Transform (ELT) is inevitable. It involves extracting data from different sources and in various formats into a data warehouse for processing. Traditionally, the data transformation process uses Extract-Transform-Load (ETL) to convert business data into a data model suitable for data warehouses. However, this relies on an ETL system independent of the data warehouse, resulting in high maintenance costs. As a cloud-native data warehouse, ByConity has gradually supported Extract-Load-Transform (ELT) since version 0.2.0, freeing users from maintaining multiple heterogeneous data systems. This article will introduce ByConity&#x27;s capabilities, implementation principles, and usage methods related to ELT.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="etl-scenarios-and-solutions">ETL Scenarios and Solutions<a href="#etl-scenarios-and-solutions" class="hash-link" aria-label="Direct link to ETL Scenarios and Solutions" title="Direct link to ETL Scenarios and Solutions">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="differences-between-elt-and-etl">Differences between ELT and ETL<a href="#differences-between-elt-and-etl" class="hash-link" aria-label="Direct link to Differences between ELT and ETL" title="Direct link to Differences between ELT and ETL">​</a></h3><ul><li>ETL: Describes the process of extracting data from a source, transforming it, and loading it into a destination (data warehouse). The Transform phase typically describes the preprocessing of data within the data warehouse.</li></ul><p><img loading="lazy" src="/assets/images/elt1-ccfe04dd9abfa7d620e7c93b024bbc2c.png" width="1080" height="388" class="img_astN"></p><ul><li>ELT focuses on loading minimally processed data into the data warehouse, leaving most of the transformation operations to the analysis phase. Compared to ETL, it requires less data modeling and provides analysts with more flexibility. ELT has become the norm in big data processing today, posing many new requirements for data warehouses.</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="challenges-of-resource-duplication">Challenges of Resource Duplication<a href="#challenges-of-resource-duplication" class="hash-link" aria-label="Direct link to Challenges of Resource Duplication" title="Direct link to Challenges of Resource Duplication">​</a></h3><p><img loading="lazy" src="/assets/images/elt2-caa5b7ef03a16466c984978e04735732.png" width="1048" height="402" class="img_astN"></p><p>A typical data pipeline is as follows: We ingest behavioral data, logs, clickstreams, etc., into a storage system using MQ/Kafka/Flink. The storage system can be further divided into on-premises HDFS and cloud-based OSS&amp;S3 remote storage systems. Then, a series of ETL operations are performed on the data warehouse to provide data for OLAP systems for analysis and query. However, some businesses need to branch off from the above storage, exporting data from the overall pipeline at a certain stage of data analysis to perform ETL operations different from the main pipeline, resulting in duplicate data storage. Additionally, two different ETL logics may emerge during this process.</p><p>As the amount of data increases, the cost pressure brought by computational and storage redundancy also increases. Meanwhile, the expansion of storage space makes elastic scaling inconvenient.</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="industry-solutions">Industry Solutions<a href="#industry-solutions" class="hash-link" aria-label="Direct link to Industry Solutions" title="Direct link to Industry Solutions">​</a></h3><p>In the industry, there are several approaches to address the above issues:</p><ul><li>Data pre-calculation school: Tools like Kylin. If report generation in Hadoop systems is slow or aggregation capabilities are poor, data pre-calculation can be performed to calculate cubes or views in advance for configured indicators. During actual SQL queries, the cubes or views can be directly used for substitution and returned.</li><li>Streaming and batch integration school: Tools like Flink, Risingwave. Data is aggregated directly in memory for reports or large screens as it flows in. After aggregation, the results are written to HBase or MySQL for retrieval and display. Flink also exposes interfaces for intermediate states, i.e., queryable state, to enable users to better utilize state data. However, the final results still need to be reconciled with batch computation results, and if inconsistencies are found, backtracking operations may be required. The entire process tests the skills of operation and maintenance/development teams.</li><li>Data lake and warehouse integration &amp; HxxP: Combining data lakes with data warehouses.</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="elt-in-byconity">ELT in ByConity<a href="#elt-in-byconity" class="hash-link" aria-label="Direct link to ELT in ByConity" title="Direct link to ELT in ByConity">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="overall-execution-flow">Overall Execution Flow<a href="#overall-execution-flow" class="hash-link" aria-label="Direct link to Overall Execution Flow" title="Direct link to Overall Execution Flow">​</a></h3><p><img loading="lazy" src="/assets/images/elt3-9500640cd18aa26d9bc3357e5dd627c9.png" width="1280" height="828" class="img_astN"></p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="system-requirements-for-elt-tasks">System Requirements for ELT Tasks:<a href="#system-requirements-for-elt-tasks" class="hash-link" aria-label="Direct link to System Requirements for ELT Tasks:" title="Direct link to System Requirements for ELT Tasks:">​</a></h3><ol><li>Overall scalability: Importing and transforming often require significant resources, and the system needs to scale horizontally to meet rapid data growth.</li><li>Reliability and fault tolerance: Large numbers of jobs can be scheduled orderly; in case of occasional task failures (OOM), container failures, etc., retries can be triggered; able to handle a certain degree of data skewness.</li><li>Efficiency and performance: Effective utilization of multi-core and multi-machine concurrency; fast data import; efficient memory usage (memory management); CPU optimization (vectorization, codegen).</li><li>Ecosystem and observability: Compatible with various tools; task status awareness; task progress awareness; failed log query; certain visualization capabilities.</li></ol><p>Based on the requirements of ELT tasks and the difficulties encountered in current scenarios, ByConity has added the following features and optimizations.</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="stage-level-scheduling">Stage-level Scheduling<a href="#stage-level-scheduling" class="hash-link" aria-label="Direct link to Stage-level Scheduling" title="Direct link to Stage-level Scheduling">​</a></h3><p><img loading="lazy" src="/assets/images/elt4-494e04491f26b62b894e7bcea61beb89.png" width="830" height="528" class="img_astN"></p><h4 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="principle-analysis">Principle Analysis<a href="#principle-analysis" class="hash-link" aria-label="Direct link to Principle Analysis" title="Direct link to Principle Analysis">​</a></h4><ul><li>The current SQL execution process in ClickHouse is as follows:<ul><li>In the first stage, the Coordinator receives a distributed table query and converts it into a local table query for each shard node.</li><li>In the second stage, the Coordinator aggregates the results from each node and returns them to the client.</li></ul></li><li>ClickHouse converts the right table in Join operations into a subquery, which brings several issues that are difficult to resolve:<ul><li>Complex queries have multiple subqueries, resulting in high conversion complexity.</li><li>When the Join table is large, it can easily cause OOM in worker nodes.</li><li>Aggregation occurs at the Coordinator, putting pressure on it and easily becoming a performance bottleneck.</li></ul></li></ul><p><img loading="lazy" src="/assets/images/elt5-feb79e6e129f3aeb9ab86aa2b92767f4.png" width="443" height="521" class="img_astN"> <img loading="lazy" src="/assets/images/elt6-8481e54f07ead8899937be2f068395c6.png" width="395" height="536" class="img_astN"> <img loading="lazy" src="/assets/images/elt7-306fde6a5dad86ac0bb435ce6909f7bc.png" width="320" height="231" class="img_astN"></p><p>Unlike ClickHouse, we have implemented optimization for the execution of complex queries in ByConity. By splitting the execution plan, we transform the previous two-stage execution model into stage-level execution. During the logical plan phase, exchange operators are inserted based on operator types. During the execution phase, the entire execution plan is DAG-split based on exchange operators, and scheduling is performed stage by stage. The exchange operators between stages are responsible for data transmission and exchange.
Key nodes:</p><ol><li>Insertion of exchange nodes</li><li>Splitting of stages</li><li>Stage scheduler</li><li>Segment executer</li><li>Exchange manager</li></ol><p><img loading="lazy" src="/assets/images/elt8-91b30f3fc765792bb11ada8747fd2ec2.png" width="583" height="529" class="img_astN"></p><p>Here, we focus on the exchange perspective. As you can see in the figure above, at the top level is the query plan. When converting it to a physical plan, we transform it into different operators based on different data distribution requirements. The source layer, which receives data, is mostly unified and called ExchangeSource. Sinks have different implementations, such as BroadcastSink, Local, PartitionSink, etc., which run as part of map tasks. For cross-node data operations, we use a unified brpc streaming data transmission at the bottom level, and for local operations, we use memory queues. We have made very detailed optimizations for different points:</p><ul><li>Data transmission layer<ul><li>In-process communication uses memory queues, without serialization, zero copy</li><li>Inter-process communication uses brpc stream RPC, ensuring order preservation, connection reuse, status code transmission, compression, etc.</li></ul></li><li>Operator layer<ul><li>Batch sending</li><li>Thread reuse, reducing the number of threads</li></ul></li></ul><h4 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="benefits">Benefits<a href="#benefits" class="hash-link" aria-label="Direct link to Benefits" title="Direct link to Benefits">​</a></h4><p>Because ByConity fully adopts a multi-stage query execution approach, there are significant overall benefits:</p><ul><li>More stable and efficient Coordinator<ul><li>Aggregation and other operators are split to worker nodes for execution</li><li>The Coordinator node only needs to aggregate the final results</li></ul></li><li>Reduced Worker OOM<ul><li>Stage splitting makes each stage&#x27;s computation relatively simple</li><li>The addition of exchange operators reduces memory pressure</li></ul></li><li>More stable and efficient network connections<ul><li>Effective transmission by exchange operators</li><li>Reuse of connection pools</li></ul></li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="adaptive-scheduler">Adaptive Scheduler<a href="#adaptive-scheduler" class="hash-link" aria-label="Direct link to Adaptive Scheduler" title="Direct link to Adaptive Scheduler">​</a></h3><p>The Adaptive Scheduler is a feature we have implemented for stability. In OLAP scenarios, it may be found that some data is incomplete or data queries timeout, often due to the fact that each worker is shared by all queries. Once a worker is slow, it can affect the execution of the entire query.</p><p><img loading="lazy" src="/assets/images/elt9-5832a9a799604adcf46d919e33a0fd48.png" width="360" height="360" class="img_astN"></p><p>Issues with shared computation nodes:</p><ul><li>The load on the node where Scan occurs is related to the amount of scan data required by different queries, and it cannot be perfectly averaged.</li><li>The resource requirements vary greatly among Plan Segments.
This leads to severe load imbalance among worker nodes. Heavily loaded worker nodes can affect the overall progress of the query. Therefore, we have implemented the following optimization solutions:</li><li>Establishment of a Worker health mechanism. The server side establishes a Worker health management class that can quickly obtain health information about the Worker Group, including CPU, memory, the number of running queries, etc.</li><li>Adaptive scheduling: Each SQL dynamically selects and controls the concurrency of computation nodes based on Worker health.</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="query-queue-mechanism">Query Queue Mechanism<a href="#query-queue-mechanism" class="hash-link" aria-label="Direct link to Query Queue Mechanism" title="Direct link to Query Queue Mechanism">​</a></h3><p><img loading="lazy" src="/assets/images/elt10-00e36ff14589366f30019c9a22871ed0.png" width="1280" height="1024" class="img_astN"> <img loading="lazy" src="/assets/images/elt11-7a76adabaa507e6b8502fef8764e9333.png" width="1280" height="1047" class="img_astN"></p><p>Our clusters may also experience full load situations, where all workers are unhealthy or overloaded/overloaded. In such cases, we use a query queue for optimization.
We directly implemented a manager on the server side. Each time a query is made, the manager checks the cluster&#x27;s resources and holds a lock. If resources are insufficient, it waits for resources to be released before waking up the lock. This avoids the server issuing unbounded computation tasks, leading to worker node overloads and crashes.
The current implementation is relatively simple. The server is multi-instanced, and each server instance has its own queue, providing a local perspective lacking a global resource perspective. Additionally, the query status in each queue is not persisted but simply cached in memory.
In the future, we will add coordination between servers to limit query concurrency from a global perspective. We will also persist</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="async-execution">Async Execution<a href="#async-execution" class="hash-link" aria-label="Direct link to Async Execution" title="Direct link to Async Execution">​</a></h3><p><img loading="lazy" src="/assets/images/elt12-8734ea2851ceceb9d629659aecd68ac5.png" width="1280" height="708" class="img_astN"></p><p>A typical characteristic of ELT tasks is that their running time is relatively long compared to real-time analysis. Generally, ELT tasks can take minutes or even hours to execute.
Currently, ClickHouse&#x27;s client queries are returned in a blocking manner. This results in the client remaining in a waiting state for an extended period, during which it needs to maintain a connection with the server. In unstable network conditions, the connection between the client and server may be disconnected, leading to task failures on the server side.
To reduce such unnecessary failures and reduce the complexity of maintaining connections for the client, we have developed an asynchronous execution feature, which is implemented as follows:</p><ol><li>User-specified asynchronous execution. Users can specify asynchronous execution on a per-query basis by using <code>settings enable_async_query = 1</code>. Alternatively, they can set it at the session level using <code>set enable_async_query = 1</code>.</li><li>If the query is asynchronous, it is placed in a background thread pool for execution.</li><li>Silent I/O. When an asynchronous query is executing, its interaction with the client, such as log output, needs to be severed.</li></ol><p>Initialization of the query still occurs in the session&#x27;s synchronous thread. Once initialization is complete, the query state is written to the metastore, and an async query ID is returned to the client. The client can use this ID to query the status of the query. After the async query ID is returned, it indicates the completion of the interaction for this query. In this mode, if the statement is a <code>SELECT</code>, subsequent results cannot be sent back to the client. In such cases, we recommend users use a combination of async query and <code>SELECT...INTO OUTFILE</code> to meet their needs.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="future-plans">Future Plans<a href="#future-plans" class="hash-link" aria-label="Direct link to Future Plans" title="Direct link to Future Plans">​</a></h2><p>Regarding ELT mixed loads, the ByConity 0.2.0 version is just the beginning. In subsequent versions, we will continue to optimize query-related capabilities, with ELT as the core focus. The plans are as follows:</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="fault-recovery-capabilities">Fault Recovery Capabilities<a href="#fault-recovery-capabilities" class="hash-link" aria-label="Direct link to Fault Recovery Capabilities" title="Direct link to Fault Recovery Capabilities">​</a></h3><ul><li>Operator Spill<ul><li>Spill for Sort, Agg, and Join operators;</li><li>Exchange Spill capability;</li></ul></li><li>Recoverability<ul><li>Operator execution recovery: When ELT tasks run for a long time, occasional failures of intermediate tasks can lead to the failure of the entire query. Supporting task-level retries can significantly reduce occasional failures caused by environmental factors;</li><li>Stage retries: When a node fails, stage-level retries can be performed;</li><li>Ability to save queue job states;</li></ul></li><li>Remote Shuffle Service: Currently, open-source shuffle services in the industry are often tailored for Spark and lack generic clients, such as C++ clients. We will supplement this capability in the future.</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="resources">Resources<a href="#resources" class="hash-link" aria-label="Direct link to Resources" title="Direct link to Resources">​</a></h3><ul><li>Specifiable computational resources: Users can specify the computational resources required for a query;</li><li>Computational resource estimation/reservation: Dynamically estimate the computational resources required for a query and allocate them through reservation;</li><li>Dynamic resource allocation: Currently, workers are permanent processes/nodes. Dynamic resource allocation can improve utilization;</li><li>Fine-grained resource isolation: Reduce the mutual influence between queries through worker group or process-level isolation;</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_HAaO padding--none margin-left--sm"><li class="tag_Ut5_"><a class="tag_sQub tagRegular_dfhI" href="/blog/tags/video-introduction">video introduction</a></li><li class="tag_Ut5_"><a class="tag_sQub tagRegular_dfhI" href="/blog/tags/docusaurus">docusaurus</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_XMSB" itemprop="headline"><a itemprop="url" href="/blog/byconity-benchmark">Performance Comparison Analysis of ByConity and Mainstream Open-Source OLAP Engines (ClickHouse, Doris, Presto)</a></h2><div class="container_fJtn margin-vert--md"><time datetime="2023-07-21T00:00:00.000Z" itemprop="datePublished">July 21, 2023</time> · <!-- -->9 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_YzU5"><div class="avatar margin-bottom--sm"><a href="https://github.com/tigerwangyb" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/tigerwangyb.png" alt="Yunbo Wang"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/tigerwangyb" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Yunbo Wang</span></a></div><small class="avatar__subtitle" itemprop="description">ByConity maintainer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2><p>As the amount and complexity of data continue to increase, more and more companies are using OLAP (Online Analytical Processing) engines to process large-scale data and provide instant analysis results. Performance is a crucial factor when selecting an OLAP engine. Therefore, this article will compare the performance of four open-source OLAP engines: ClickHouse, Doris, Presto, and ByConity, using the 99 query statements from the TPC-DS benchmark test. The aim is to provide a reference for companies to choose a suitable OLAP engine.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="introduction-to-tpc-ds-benchmark-test">Introduction to TPC-DS Benchmark Test<a href="#introduction-to-tpc-ds-benchmark-test" class="hash-link" aria-label="Direct link to Introduction to TPC-DS Benchmark Test" title="Direct link to Introduction to TPC-DS Benchmark Test">​</a></h2><p>TPC-DS (Transaction Processing Performance Council Decision Support Benchmark) is a benchmark test designed for decision support systems (DSS). Developed by the TPC organization, it simulates multidimensional analysis and decision support scenarios, providing 99 query statements to evaluate the performance of database systems in complex multidimensional analysis scenarios. Each query is designed to simulate complex decision support scenarios, including joins across multiple tables, aggregations and groupings, subqueries, and other advanced SQL techniques.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="introduction-to-olap-engines">Introduction to OLAP Engines<a href="#introduction-to-olap-engines" class="hash-link" aria-label="Direct link to Introduction to OLAP Engines" title="Direct link to Introduction to OLAP Engines">​</a></h2><p>ClickHouse, Doris, Presto, and ByConity are currently popular open-source OLAP engines known for their high performance and scalability.</p><ul><li>ClickHouse is a column-based database management system developed by Yandex, a Russian search engine company. It focuses on fast query and analysis of large-scale data.</li><li>Doris is a distributed column-based storage and analysis system that supports real-time query and analysis and can integrate with big data technologies such as Hadoop, Spark, and Flink.</li><li>Presto is a distributed SQL query engine developed by Facebook that enables fast query and analysis on large-scale datasets.</li><li>ByConity is a cloud-native data warehouse open-sourced by ByteDance. It adopts a storage-compute separation architecture, achieves tenant resource isolation, elastic scaling, and strong consistency in data read and write. It supports mainstream OLAP engine optimization techniques and exhibits excellent read and write performance.</li></ul><p>This article will test the performance of these four OLAP engines using the 99 query statements from the TPC-DS benchmark test and compare their performance differences in different types of queries.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="test-environment-and-methodology">Test Environment and Methodology<a href="#test-environment-and-methodology" class="hash-link" aria-label="Direct link to Test Environment and Methodology" title="Direct link to Test Environment and Methodology">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="test-environment-configuration">Test Environment Configuration<a href="#test-environment-configuration" class="hash-link" aria-label="Direct link to Test Environment Configuration" title="Direct link to Test Environment Configuration">​</a></h3><ul><li><p><strong>Environment Configuration</strong></p><ul><li>Memory: 256GB</li><li>Disk: ATA, 7200rpm, partitioned:gpt</li><li>System: Linux 4.14.81.bm.30-amd64 x86_64, Debian GNU/Linux 9</li></ul></li><li><p><strong>Test Data Volume</strong></p><ul><li>Using 1TB of data tables, equivalent to 2.8 billion rows of data</li></ul></li></ul><table><thead><tr><th>Software Name</th><th>Version</th><th>Release Date</th><th>Number of Nodes</th><th>Other Configurations</th></tr></thead><tbody><tr><td>ClickHouse</td><td>23.4.1.1943</td><td>2023-04-26</td><td>5 Workers</td><td>distributed_product_mode = &#x27;global&#x27;, partial_merge_join_optimizations = 1</td></tr><tr><td>Doris</td><td>1.2.4.1</td><td>2023-04-27</td><td>5 BEs, 1 FE</td><td>Bucket configuration: Dimension table 1, returns table 10-20, sales table 100-200</td></tr><tr><td>Presto</td><td>0.28.0</td><td>2023-03-16</td><td>5 Workers, 1 Coordinator</td><td>Hive Catalog, ORC format, Xmx200GB, enable_optimizer=1, dialect_type=&#x27;ANSI&#x27;</td></tr><tr><td>ByConity</td><td>0.1.0-GA</td><td>2023-03-15</td><td>5 Workers</td><td>enable_optimizer=1, dialect_type=&#x27;ANSI&#x27;</td></tr></tbody></table><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="server-configuration">Server Configuration<a href="#server-configuration" class="hash-link" aria-label="Direct link to Server Configuration" title="Direct link to Server Configuration">​</a></h3><div class="codeBlockContainer_Dtwu theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_sgmo"><pre tabindex="0" class="prism-code language-text codeBlock_zjcW thin-scrollbar"><code class="codeBlockLines_u4eB"><span class="token-line" style="color:#393A34"><span class="token plain">Architecture:          x86_64</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">CPU op-mode(s):        32-bit, 64-bit</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Byte Order:            Little Endian</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">CPU(s):                48</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">On-line CPU(s) list:   0-47</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Thread(s) per core:    2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Core(s) per socket:    12</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Socket(s):             2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUMA node(s):          2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Vendor ID:             GenuineIntel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">CPU family:            6</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model:                 79</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Model name:            Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Stepping:              1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">CPU MHz:               2494.435</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">CPU max MHz:           2900.0000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">CPU min MHz:           1200.0000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">BogoMIPS:              4389.83</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Virtualization:        VT-x</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">L1d cache:             32K</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">L1i cache:             32K</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">L2 cache:              256K</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">L3 cache:              30720K</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUMA node0 CPU(s):     0-11,24-35</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NUMA node1 CPU(s):     12-23,36-47</span><br></span></code></pre><div class="buttonGroup_uzaf"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_MWsZ" aria-hidden="true"><svg class="copyButtonIcon_utg0" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_YlgY" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="test-methodology">Test Methodology<a href="#test-methodology" class="hash-link" aria-label="Direct link to Test Methodology" title="Direct link to Test Methodology">​</a></h3><ul><li>Use the 99 query statements from the TPC-DS benchmark test and 1TB (2.8 billion rows) of data to test the performance of the four OLAP engines.</li><li>Use the same test dataset in each engine and maintain the same configuration and hardware environment.</li><li>Execute each query multiple times and take the average value to reduce measurement errors, with a query timeout set to 500 seconds.</li><li>Record details of query execution, such as query execution plans, I/O and CPU usage.</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="performance-test-results">Performance Test Results<a href="#performance-test-results" class="hash-link" aria-label="Direct link to Performance Test Results" title="Direct link to Performance Test Results">​</a></h2><p>We used the same dataset and hardware environment to test the performance of these four OLAP engines. The test dataset size is 1TB, and the hardware and software environments are as described above. We conducted three consecutive tests on each of the four OLAP engines using the 99 query statements from the TPC-DS benchmark test and took the average of the three results. Among them, ByConity successfully ran all 99 query tests. Doris crashed on SQL15 and had four timeouts, specifically SQL54, SQL67, SQL78, and SQL95. Presto had timeouts only on SQL67 and SQL72, while all other queries ran successfully. ClickHouse only ran 50% of the query statements, with some timing out and others reporting system errors. The analysis revealed that ClickHouse does not effectively support multi-table join queries, requiring manual rewriting and splitting of such SQL statements for execution. Therefore, we temporarily exclude ClickHouse from the comparison of total execution time. The total execution time for the TPC-DS tests of the other three OLAP engines is shown in Figure 1. As seen in Figure 1, the query performance of the open-source ByConity is significantly better than the other engines, approximately 3-4 times faster. (Note: The vertical axis units of all charts below are in seconds.)</p><p><img loading="lazy" alt="Figure 1: Total Execution Time for 99 Queries in TPC-DS" src="/assets/images/benchmark1-b501a6894b07e84ebbcbea991548633f.png" width="1280" height="780" class="img_astN"></p><p>Based on the 99 query statements from the TPC-DS benchmark test, we will categorize them according to different query scenarios, such as basic queries, join queries, aggregation queries, subqueries, and window function queries. We will use these categories to analyze and compare the performance of ClickHouse, Doris, Presto, and ByConity:</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="basic-query-scenario">Basic Query Scenario<a href="#basic-query-scenario" class="hash-link" aria-label="Direct link to Basic Query Scenario" title="Direct link to Basic Query Scenario">​</a></h3><p>This scenario involves simple query operations, such as retrieving data from a single table, filtering, and sorting results. The performance test of basic queries mainly focuses on the ability to process individual queries. Among them, ByConity performs best, with Presto and Doris also showing good performance. This is because basic queries usually involve only a small number of tables and fields, allowing Presto and Doris to fully utilize their distributed query features and in-memory computing capabilities. ClickHouse struggles with multi-table joins, resulting in some queries timing out. Specifically, SQL5, 8, 11, 13, 14, 17, and 18 all timed out. We calculated these timeouts as 500 seconds but truncated them to 350 seconds for clearer display. Figure 2 shows the average query time for the four engines in the basic query scenario:</p><p><img loading="lazy" alt="Figure 2: Performance Comparison of Basic Queries in TPC-DS" src="/assets/images/benchmark2-00a78c3b1d2b94ce8eb03ecc0f3f506a.png" width="1280" height="788" class="img_astN"></p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="join-query-scenario">Join Query Scenario<a href="#join-query-scenario" class="hash-link" aria-label="Direct link to Join Query Scenario" title="Direct link to Join Query Scenario">​</a></h3><p>Join queries are common multi-table query scenarios that typically use JOIN statements to connect multiple tables and retrieve data based on specified conditions. As seen in Figure 3, ByConity performs best, mainly due to its optimized query optimizer, which introduces cost-based optimization capabilities (CBO) and performs optimization operations such as re-ordering during multi-table joins. Presto and Doris follow closely behind, while ClickHouse performs relatively poorly in multi-table joins and does not support many complex statements well.</p><p><img loading="lazy" alt="Figure 3: Performance Comparison of Join Queries in TPC-DS" src="/assets/images/benchmark3-1acbfa4ea42175c44667256a6ced5206.png" width="1280" height="787" class="img_astN"></p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="aggregation-query-scenario">Aggregation Query Scenario<a href="#aggregation-query-scenario" class="hash-link" aria-label="Direct link to Aggregation Query Scenario" title="Direct link to Aggregation Query Scenario">​</a></h3><p>Aggregation queries involve statistical calculations on data, such as testing the use of aggregate functions like SUM, AVG, COUNT. ByConity continues to perform exceptionally well, followed by Doris and Presto. ClickHouse experienced four timeouts. To facilitate comparison, we truncated the timeout value to 250 seconds.</p><p><img loading="lazy" alt="Figure 4: Performance Comparison of Aggregation Queries in TPC-DS" src="/assets/images/benchmark4-fb103d3e66cdefc5d8dd4e5cff4388e0.png" width="1280" height="787" class="img_astN"></p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="subquery-scenario">Subquery Scenario<a href="#subquery-scenario" class="hash-link" aria-label="Direct link to Subquery Scenario" title="Direct link to Subquery Scenario">​</a></h3><p>Subqueries are nested within SQL statements and often serve as conditions or constraints for the main query. As shown in Figure 5, ByConity performs best due to its rule-based optimization (RBO) capabilities. ByConity optimizes complex nested queries holistically through techniques like operator pushdown, column pruning, and partition pruning, eliminating all subqueries and transforming common operators into Join+Agg formats. Doris and Presto also perform relatively well, but Presto experienced timeouts on SQL68 and SQL73, and Doris experienced timeouts on three SQL queries. ClickHouse also had some timeouts and system errors, as mentioned earlier. For easier comparison, we truncated the timeout value to 250 seconds.</p><p><img loading="lazy" alt="Figure 5: Performance Comparison of Subqueries in TPC-DS" src="/assets/images/benchmark5-0ae53bb8245ec87a9ac998d1b0b5614f.png" width="1280" height="796" class="img_astN"></p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="window-function-query-scenario">Window Function Query Scenario<a href="#window-function-query-scenario" class="hash-link" aria-label="Direct link to Window Function Query Scenario" title="Direct link to Window Function Query Scenario">​</a></h3><p>Window function queries are advanced SQL query scenarios that enable ranking, grouping, sorting, and other operations within query results. As shown in Figure 6, ByConity exhibits the best performance, followed by Presto. Doris experienced a single timeout, and ClickHouse still had some TPC-DS tests that did not complete successfully.</p><p><img loading="lazy" alt="Figure 6: Performance Comparison of Window Function Queries in TPC-DS" src="/assets/images/benchmark6-334f3bbe8cdd726ae7f6131839b71d43.png" width="1280" height="792" class="img_astN"></p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>This article analyzes and compares the performance of four OLAP engines - ClickHouse, Doris, Presto, and ByConity - using the 99 query statements from the TPC-DS benchmark test. We found that the performance of the four engines varies under different query scenarios. ByConity performs exceptionally well in all 99 TPC-DS query scenarios, surpassing the other three OLAP engines. Presto and Doris perform relatively well in join queries, aggregation queries, and window function queries. However, ClickHouse&#x27;s design and implementation are not specifically optimized for join queries, resulting in subpar performance in multi-table join scenarios.</p><p>It is important to note that performance test results depend on multiple factors, including data structure, query type, and data model. In practical applications, it is necessary to consider various factors comprehensively to select the most suitable OLAP engine.</p><p>When selecting an OLAP engine, other factors such as scalability, usability, and stability should also be considered. In practical applications, it is essential to select based on specific business needs and configure and optimize the engine reasonably to achieve optimal performance.</p><p>In summary, ClickHouse, Doris, Presto, and ByConity are all excellent OLAP engines with different strengths and applicable scenarios. In practical applications, it is necessary to select based on specific business needs and configure and optimize the engine reasonably to achieve optimal performance. At the same time, it is essential to select representative query scenarios and datasets and conduct testing and analysis for different query scenarios to comprehensively evaluate the engine&#x27;s performance.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="join-us">Join Us<a href="#join-us" class="hash-link" aria-label="Direct link to Join Us" title="Direct link to Join Us">​</a></h2><p>The ByConity community has a large number of users and is a very open community. We invite everyone to discuss and contribute together. We have established an issue on Github: <a href="https://github.com/ByConity/ByConity/issues/26" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/ByConity/issues/26</a>. You can also join our Feishu group, Slack, or Discord to participate in the discussion.</p><p><img loading="lazy" src="/assets/images/benchmark7-14e580dc14c6dfd29f801716c262ad3b.png" width="900" height="400" class="img_astN"></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_HAaO padding--none margin-left--sm"><li class="tag_Ut5_"><a class="tag_sQub tagRegular_dfhI" href="/blog/tags/video-introduction">video introduction</a></li><li class="tag_Ut5_"><a class="tag_sQub tagRegular_dfhI" href="/blog/tags/docusaurus">docusaurus</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_XMSB" itemprop="headline"><a itemprop="url" href="/blog/2023-05-24-byconity-announcement-opensources-its-cloudnative-data-warehouse">ByteDance Open Sources Its Cloud Native Data Warehouse ByConity</a></h2><div class="container_fJtn margin-vert--md"><time datetime="2023-05-24T00:00:00.000Z" itemprop="datePublished">May 24, 2023</time> · <!-- -->12 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_YzU5"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/vinijaiswal" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/vinijaiswal.png" alt="Vini Jaiswal"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/vinijaiswal" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Vini Jaiswal</span></a></div><small class="avatar__subtitle" itemprop="description">ByConity maintainer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>ByConity is an open-source cloud-native data warehouse developed by ByteDance. It utilizes a computing-storage separation architecture and offers various essential features, including the separation of computing and storage, elastic scalability, tenant resource isolation, and strong consistency in data reading and writing. By leveraging optimizations from popular OLAP engines like column storage, vectorized execution, MPP execution, and query optimization, ByConity delivers exceptional read and write performance.</p><h1>History of ByConity</h1><p>The origins of ByConity can be traced back to 2018 when ByteDance initially implemented ClickHouse for internal use. As the business grew, the data volume increased significantly to cater to a large user base. However, ClickHouse&#x27;s Shared-Nothing architecture, where each node operates independently without sharing storage resources, posed certain challenges during its usage. Here are some of the issues encountered:</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="expansion-and-contraction">Expansion and contraction:<a href="#expansion-and-contraction" class="hash-link" aria-label="Direct link to Expansion and contraction:" title="Direct link to Expansion and contraction:">​</a></h2><p>Due to the tight coupling of computing and storage resources in ClickHouse, scaling the system incurred higher costs and involved data migration. This prevented real-time on-demand scalability, resulting in inefficient resource utilization.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="multi-tenancy-and-shared-cluster-environment">Multi-tenancy and shared cluster environment:<a href="#multi-tenancy-and-shared-cluster-environment" class="hash-link" aria-label="Direct link to Multi-tenancy and shared cluster environment:" title="Direct link to Multi-tenancy and shared cluster environment:">​</a></h2><p>ClickHouse&#x27;s tightly coupled architecture led to interactions among multiple tenants in a shared cluster environment. Since reading and writing operations were performed on the same node, they often interfered with each other, impacting overall performance.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="performance-limitations">Performance limitations:<a href="#performance-limitations" class="hash-link" aria-label="Direct link to Performance limitations:" title="Direct link to Performance limitations:">​</a></h2><p>ClickHouse&#x27;s support for complex queries, such as multi-table join operations, was not optimal, which hindered the system&#x27;s ability to handle such queries efficiently.</p><p>To address these pain points, ByteDance undertook an architectural upgrade of ClickHouse. In 2020, we initiated the ByConity project internally. After releasing the Beta version in January 2023, the project was officially made available to the public at the end of May 2023.</p><p><img loading="lazy" alt="Figure 1 ByteDance ClickHouse usage" src="/assets/images/f1-byte-clickhouse-usage-cefbe6a269f39c214219df180aef1560.png" width="1652" height="896" class="img_astN"></p><p>Figure 1: ByteDance ClickHouse Usage</p><h1>Features of ByConity</h1><p>ByConity implements a computing and storage separation architecture that transforms the original local management of computing and storage on individual nodes. Instead, it adopts a unified management approach for all data across the entire cluster using distributed storage. This transformation results in stateless computing nodes, enabling dynamic expansion and contraction by leveraging the scalability of distributed storage and the stateless nature of computing nodes. ByConity offers several crucial features that enhance its functionality and performance:</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="storage-computing-separation">Storage-Computing Separation<a href="#storage-computing-separation" class="hash-link" aria-label="Direct link to Storage-Computing Separation" title="Direct link to Storage-Computing Separation">​</a></h2><p>One of the critical advantages of ByConity is its storage-computing separation architecture, which enables read-write separation and elastic scaling. This architecture ensures that read and write operations do not affect each other, and computing resources and storage resources can be independently expanded and contracted on demand, ensuring efficient resource utilization. ByConity also supports multi-tenant resource isolation, making it suitable for multi-tenant environments.</p><p><img loading="lazy" alt="Figure 2: ByConity storage-computing separation to achieve multi-tenant isolation" src="/assets/images/f2-storage-computing-separation-51a4fd7a3073479cf431a48278204cd5.png" width="1203" height="561" class="img_astN">
Figure 2: ByConity storage-computing separation to achieve multi-tenant isolation</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="resource-isolation">Resource Isolation<a href="#resource-isolation" class="hash-link" aria-label="Direct link to Resource Isolation" title="Direct link to Resource Isolation">​</a></h2><p>ByConity provides resource isolation, ensuring that different tenants have separate and independent resources. This prevents interference or impact between tenants, promoting data privacy and efficient multi-tenancy support.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="elastic-scaling">Elastic Scaling<a href="#elastic-scaling" class="hash-link" aria-label="Direct link to Elastic Scaling" title="Direct link to Elastic Scaling">​</a></h2><p> ByConity supports elastic expansion and contraction, allowing for real-time and on-demand scaling of computing resources. This flexibility ensures efficient resource utilization and enables the system to adapt to changing workload requirements.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="strong-data-consistency">Strong Data Consistency<a href="#strong-data-consistency" class="hash-link" aria-label="Direct link to Strong Data Consistency" title="Direct link to Strong Data Consistency">​</a></h2><p>ByConity ensures strong consistency of data read and write operations. This ensures that data is always up-to-date and eliminates any inconsistencies between read and write operations, guaranteeing data integrity and accuracy.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="high-performance">High Performance<a href="#high-performance" class="hash-link" aria-label="Direct link to High Performance" title="Direct link to High Performance">​</a></h2><p>ByConity incorporates optimization techniques from mainstream OLAP engines, such as column storage, vectorized execution, MPP execution, and query optimization. These optimizations enhance read and write performance, enabling faster and more efficient data processing and analysis.</p><h1>Technical Architecture of ByConity</h1><p>ByConity follows a three-layer architecture consisting of:</p><ol><li>Service access layer: The service access layer, represented by ByConity Server, handles client data and service access. </li><li>Computing layer: The computing layer comprises multiple computing groups, where each Virtual Warehouse (VW) functions as a computing group.  </li><li>Data storage layer: The data storage layer utilizes distributed file systems like HDFS and S3.</li></ol><p><img loading="lazy" alt="Figure 3: ByConity&amp;#39;s architecture" src="/assets/images/f3-three-layer-architecture-403f695f8cd48cb936283a4c7b86553d.png" width="1483" height="809" class="img_astN">
Figure 3: ByConity&#x27;s architecture</p><h1>Working Principle of ByConity</h1><p>ByConity is a powerful open-source cloud-native data warehouse that adopts a storage-computing separation architecture. In this section, we will examine the working principle of ByConity and the interaction process of each component of ByConity through the complete life cycle of a SQL.</p><p><img loading="lazy" alt="Figure 4: ByConity internal component interaction diagram" src="/assets/images/f4-internal-component-interaction-301b3d1e7d179142c3b1ce535f1ce105.png" width="1280" height="1210" class="img_astN">
Figure 4: ByConity internal component interaction diagram</p><p>Figure 4 depicts the interaction diagram of ByConity&#x27;s components. In the figure, the dotted line represents the inflow of a SQL query, the double-headed arrow indicates component interaction, and the one-way arrow represents data processing and output to the client. Let&#x27;s explore the interaction process of each component in ByConity throughout the complete lifecycle of a SQL query.</p><p>ByConity&#x27;s working principle can be divided into three stages:</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="stage-1-query-request">Stage 1: Query Request<a href="#stage-1-query-request" class="hash-link" aria-label="Direct link to Stage 1: Query Request" title="Direct link to Stage 1: Query Request">​</a></h2><p>The client submits a Query request to the server. The server initially performs <strong>parsing</strong> and subsequently analyzes and optimizes the query through the <strong>Query Analyzer and Optimizer</strong> to generate an efficient executable plan. To access the required <strong>metadata</strong>, which is stored in a distributed key-value (KV) store, ByConity leverages <strong>FoundationDB</strong> and reads the metadata from the <strong>Catalog</strong>.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="stage-2-plan-scheduling">Stage 2: Plan Scheduling<a href="#stage-2-plan-scheduling" class="hash-link" aria-label="Direct link to Stage 2: Plan Scheduling" title="Direct link to Stage 2: Plan Scheduling">​</a></h2><p>ByConity passes the optimized executable plan to the <strong>Plan Scheduler</strong> component. The scheduler accesses the <strong>Resource Manager</strong> to obtain available computing resources and determines which nodes to schedule the query tasks for execution.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="stage-3-query-execution">Stage 3: Query Execution<a href="#stage-3-query-execution" class="hash-link" aria-label="Direct link to Stage 3: Query Execution" title="Direct link to Stage 3: Query Execution">​</a></h2><p>The Query request is executed on <strong>ByConity&#x27;s Workers</strong>. The Workers read data from the underlying <strong>Cloud Storage</strong> and perform computations by establishing a Pipeline. The server then aggregates the calculation results from multiple Workers and returns them to the client.</p><p>Additionally, ByConity includes two main components: <strong>Time-stamp Oracle</strong> and <strong>Daemon Manager</strong>. The time-stamp oracle supports transaction processing, while the daemon manager manages and schedules subsequent tasks.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="main-component-library">Main Component Library<a href="#main-component-library" class="hash-link" aria-label="Direct link to Main Component Library" title="Direct link to Main Component Library">​</a></h2><p>To better understand the working principle of ByConity, let&#x27;s take a look at the main components of ByConity:</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="metadata-management">Metadata Management<a href="#metadata-management" class="hash-link" aria-label="Direct link to Metadata Management" title="Direct link to Metadata Management">​</a></h3><p>ByConity offers a highly available and high-performance metadata read and write service called the Catalog Server. It supports complete transaction semantics (ACID). Furthermore, we have designed the Catalog Server with a flexible architecture, allowing for the pluggability of backend storage systems. Currently, we support Apple&#x27;s open-source FoundationDB, and there is potential for extending support to other backend storage systems in the future.</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="query-optimizer">Query Optimizer<a href="#query-optimizer" class="hash-link" aria-label="Direct link to Query Optimizer" title="Direct link to Query Optimizer">​</a></h3><p>The query optimizer plays a crucial role in the performance of a database system. A well-designed optimizer can significantly enhance query performance, particularly in complex query scenarios, where it can achieve performance improvements ranging from several times to hundreds of times. ByConity&#x27;s self-developed optimizer focuses on improving optimization capabilities through two main approaches:</p><ul><li>RBO (Rule-Based Optimization): This capability encompasses various optimizations such as column pruning, partition pruning, expression simplification, subquery dissociation, predicate pushdown, redundant operator elimination, Outer-Join to Inner-Join conversion, operator pushdown storage, distributed operator splitting, and other heuristic optimization techniques.</li><li>CBO (Cost-Based Optimization): ByConity&#x27;s optimizer also includes cost-based optimization capabilities. This includes support for join reorder, outer-join reorder, join/agg reorder, common table expressions (CTE), materialized views, dynamic filter push-down, magic set optimization, and other cost-based techniques. Additionally, it integrates property enforcement for distributed planning.</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="query-scheduling">Query Scheduling<a href="#query-scheduling" class="hash-link" aria-label="Direct link to Query Scheduling" title="Direct link to Query Scheduling">​</a></h3><p>ByConity currently supports two query scheduling strategies: Cache-aware scheduling and Resource-aware scheduling.</p><ul><li>The <strong>cache-aware scheduling</strong> focuses on scenarios where computing and storage are separated. Its objective is to maximize cache utilization and minimize cold reads. This strategy aims to schedule tasks to nodes that have corresponding data caches, enabling computations to leverage the cache and improve read and write performance. Additionally, considering the dynamic expansion and contraction of the system, cache-aware scheduling strives to minimize the impact of cache failure on query performance when the computing group&#x27;s topology changes.</li><li><strong>Resource-aware scheduling</strong> analyzes the resource usage of different nodes within the computing group across the entire cluster. It performs targeted scheduling to optimize resource utilization. Moreover, resource-aware scheduling incorporates flow control mechanisms to ensure rational resource utilization and prevent negative effects caused by overload, such as system downtime.</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="computing-group">Computing Group<a href="#computing-group" class="hash-link" aria-label="Direct link to Computing Group" title="Direct link to Computing Group">​</a></h3><p>ByConity enables different tenants to utilize distinct computing resources, as depicted in Figure 5. With ByConity&#x27;s architecture, implementing features like multi-tenant isolation and read-write separation becomes straightforward. Each tenant can leverage separate computing groups to achieve multi-tenant isolation and support read-write separation. The computing groups can be dynamically expanded and contracted on-demand, ensuring efficient resource utilization. During periods of low resource utilization, resource sharing can be employed, allowing computing groups to be allocated to other tenants to maximize resource utilization and minimize costs.</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="virtual-file-system">Virtual File System<a href="#virtual-file-system" class="hash-link" aria-label="Direct link to Virtual File System" title="Direct link to Virtual File System">​</a></h3><p>The virtual file system module serves as an intermediary layer for data reading and writing. ByConity has optimized this module to provide a &quot;storage as a service&quot; capability to other modules. The virtual file system offers a unified file system abstraction, shielding the underlying different back-end implementations. It facilitates easy expansion and supports multiple storage systems, such as HDFS or object storage.</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="cache-acceleration">Cache Acceleration<a href="#cache-acceleration" class="hash-link" aria-label="Direct link to Cache Acceleration" title="Direct link to Cache Acceleration">​</a></h3><p>ByConity utilizes caching to accelerate query processing. Under the computing-storage separation architecture, cache acceleration is performed in both the metadata and data dimensions. In the metadata dimension, ByConity caches Table and Partition information in the memory of the server-side (ByConity Server). In the data dimension, cache acceleration occurs on the Worker side within the computing group. This hierarchical caching mechanism utilizes both memory and disk, with Mark collection serving as the cache granularity. These caching strategies effectively enhance query speed and performance.</p><h1>How to Deploy Byconity</h1><p>ByConity currently supports four acquisition and deployment modes. Community developers are welcome to use them and submit issues to us:</p><ul><li>Stand-alone Docker: ByConity provides a Docker deployment option, which can be accessed at <a href="https://github.com/ByConity/byconity-docker" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/byconity-docker</a></li><li>K8s cluster deployment: ByConity also supports deployment on Kubernetes clusters. The deployment guide for Kubernetes can be found at <a href="https://github.com/ByConity/byconity-deploy" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/byconity-deploy</a></li><li>Physical machine deployment: If you prefer to deploy ByConity on physical machines, you can refer to the repository at <a href="https://github.com/ByConity/ByConity/tree/master/packages" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/ByConity/tree/master/packages</a></li><li>Source code compilation: You can compile the ByConity source code yourself. The source code repository can be accessed at <a href="https://github.com/ByConity/ByConity#build-byconity" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/ByConity#build-byconity</a></li></ul><h1>ByConity&#x27;s Future Open-Source Plan</h1><p>ByConity includes several key milestones in its open-source community roadmap through 2023. These milestones are designed to enhance ByConity&#x27;s functionality, performance, and ease of use. Among them, the development of new storage engines, support for more data types, and integration with other data management tools are some important areas of focus. We have listed the following directions, and created an issue on Github: <a href="https://github.com/ByConity/ByConity/issues/26" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/ByConity/issues/26</a>, inviting the community to join us to discuss co-development:</p><ul><li><strong>Performance improvement</strong>: ByConity aims to boost performance through various optimizations. This includes leveraging indexes for acceleration, such as Skip-index optimization, support for new Zorder-index and inverted indexes. ByConity will also focus on the construction and acceleration of external indexes, as well as the automatic recommendation and conversion of indexes. Continuous enhancements to the query optimizer and the implementation of a distributed caching mechanism are also part of the performance improvement efforts.</li><li><strong>Stability improvement</strong>: There are two aspects here.<ul><li>One is to support resource isolation in more dimensions. ByConity is committed to improving stability by extending resource isolation capabilities in multiple dimensions, thereby providing better multi-tenant support. </li><li>The second direction is to enrich metrics and improve observability and problem diagnosis capabilities, ensuring a stable and reliable experience for users.</li></ul></li><li><strong>Enterprise-level feature enhancements</strong>: ByConity aims to introduce finer-grained authority control, improve data security-related functions such as backup, recovery, and data encryption and continue to explore techniques for deep compression of data to save storage costs.</li><li><strong>Ecosystem compatibility improvement</strong>: ByConity plans to expand its compatibility with various storage systems, including popular object storage solutions like S3 and TOS. It plans to enhance the overall compatibility and integration capabilities, facilitating seamless integration with other tools and frameworks. Moreover, it aims to support data lake federation queries, enabling interaction with technologies like Hudi, Iceberg, and more.</li></ul><h1>Working with the Community</h1><p>Since the release of the Beta version, ByConity has received support from numerous enterprise developers, including Huawei, Electronic Cloud, Zhanxinzhanli, Tianyi Cloud, Vipshop, and Transsion Holdings. These organizations have actively contributed by deploying ByConity in their respective environments, undergoing TPC-DS verification, and conducting tests in their business scenarios. The results have been promising, and their feedback has provided valuable insights for improvement, which we greatly appreciate.</p><p>We are delighted to receive the ideas and willingness of community partners to build together. We have already initiated joint development efforts with Huawei Terminal Cloud. Our collaborative endeavors will focus on various areas, such as Kerberos authentication, ORC support, and integration with S3 storage. </p><p>If you are interested in joining our community and participating in the development of ByConity, we invite you to visit our GitHub repository at <a href="https://github.com/ByConity/ByConity" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/ByConity</a>. You can find more information and details about our ongoing projects, contribute your ideas, and collaborate with us to further enhance ByConity. To get involved, simply scan the QR code provided below to join our <a href="https://discord.gg/V4BvTWGEQJ" target="_blank" rel="noopener noreferrer">Discord</a> or follow us on <a href="https://twitter.com/ByConity" target="_blank" rel="noopener noreferrer">Twitter</a>. </p><p><img loading="lazy" alt="ByConity Discord Group" src="/assets/images/f5-ByConity-Discord-QR-Code-3df954930bbe1e0f0f6cb1a197c4de77.jpeg" width="400" height="400" class="img_astN">
ByConity Discord Group</p><p><img loading="lazy" alt="ByConity Twitter" src="/assets/images/f5-ByConity-Twitter-QR-code-1ec0e9ecaae58ba7e5d33acbeadfb84c.jpeg" width="400" height="400" class="img_astN">
ByConity Twitter</p><h1>Summary</h1><p>In summary, ByConity is an open source cloud-native data warehouse that offers features such as read-write separation, elastic expansion and contraction, tenant resource isolation, and strong data read and write consistency. It utilizes a computing-storage separation architecture and leverages optimizations from mainstream OLAP engines to deliver excellent read and write performance. As ByConity continues to evolve and improve, it aims to become a key tool for cloud-native data warehousing in the future.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_HAaO padding--none margin-left--sm"><li class="tag_Ut5_"><a class="tag_sQub tagRegular_dfhI" href="/blog/tags/video-introduction">video introduction</a></li><li class="tag_Ut5_"><a class="tag_sQub tagRegular_dfhI" href="/blog/tags/docusaurus">docusaurus</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_XMSB" itemprop="headline"><a itemprop="url" href="/blog/byconity-an-opensource-cloudnative-data-warehouse-post">ByConity -- An Open source Cloud-native Data Warehouse</a></h2><div class="container_fJtn margin-vert--md"><time datetime="2023-04-10T00:00:00.000Z" itemprop="datePublished">April 10, 2023</time> · <!-- -->10 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_YzU5"><div class="avatar margin-bottom--sm"><a href="https://github.com/hustnn" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/hustnn.png" alt="Zhaojie Niu"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/hustnn" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Zhaojie Niu</span></a></div><small class="avatar__subtitle" itemprop="description">ByConity maintainer</small></div></div></div><div class="col col--6 authorCol_YzU5"><div class="avatar margin-bottom--sm"><a href="https://github.com/tigerwangyb" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/tigerwangyb.png" alt="Yunbo Wang"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/tigerwangyb" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Yunbo Wang</span></a></div><small class="avatar__subtitle" itemprop="description">ByConity maintainer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="introduction-to-byconity">Introduction to ByConity<a href="#introduction-to-byconity" class="hash-link" aria-label="Direct link to Introduction to ByConity" title="Direct link to Introduction to ByConity">​</a></h2><p>ByConity is an open-source cloud-native data warehouse that adopts a storage-computing separation architecture. It supports several key features, including separation of storage and computing, elastic expansion and contraction, isolation of tenant resources, and strong consistency of data read and write. By utilizing mainstream OLAP engine optimizations, such as column storage, vectorized execution, MPP execution, query optimization, etc., ByConity can provide excellent read and write performance.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="byconitys-history">ByConity&#x27;s History<a href="#byconitys-history" class="hash-link" aria-label="Direct link to ByConity&#x27;s History" title="Direct link to ByConity&#x27;s History">​</a></h2><p>The background of ByConity can be traced back to 2018 when ByteDance began to use ClickHouse internally. Due to the development of business, the scale of data has become larger and larger to serve a large number of users. However, because ClickHouse is a Shared-Nothing architecture, each node is independent and does not share storage resources, so computing resources and storage resources are tightly coupled. This leads to a higher cost of expansion and contraction, and involves data migration, which prevents real-time and on-demand expansion and contraction, resulting in a waste of resources. Furthermore, the tightly coupled architecture of ClickHouse will cause multi-tenants to interact with each other in the shared cluster. In addition, because reading and writing are completed on one node, reading and writing are affected. Finally, ClickHouse does not support performance in complex queries such as multi-table join. Based on these pain points, the ByConity project was launched in January 2020.</p><p>The ByConity team hopes to give the project back to the community and improve it through the power of open source. In January 2023, ByConity was officially open-sourced, and the beta version was released.</p><p><img loading="lazy" alt="Figure 1 ByteDance ClickHouse usage" src="/assets/images/f1-byte-clickhouse-usage-cefbe6a269f39c214219df180aef1560.png" width="1652" height="896" class="img_astN"></p><p>Figure 1: ByteDance ClickHouse Usage</p><h1>Features of ByConity</h1><p>ByConity has several key features that make it a powerful open-source cloud-native data warehouse.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="storage-computing-separation">Storage-Computing Separation<a href="#storage-computing-separation" class="hash-link" aria-label="Direct link to Storage-Computing Separation" title="Direct link to Storage-Computing Separation">​</a></h2><p>One of the critical advantages of ByConity is its storage-computing separation architecture, which enables read-write separation and elastic scaling. This architecture ensures that read and write operations do not affect each other, and computing resources and storage resources can be independently expanded and contracted on demand, ensuring efficient resource utilization. ByConity also supports multi-tenant resource isolation, making it suitable for multi-tenant environments.</p><p><img loading="lazy" alt="Figure 2: ByConity storage-computing separation to achieve multi-tenant isolation" src="/assets/images/f2-storage-computing-separation-51a4fd7a3073479cf431a48278204cd5.png" width="1203" height="561" class="img_astN">
Figure 2: ByConity storage-computing separation to achieve multi-tenant isolation</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="elastic-scaling">Elastic Scaling<a href="#elastic-scaling" class="hash-link" aria-label="Direct link to Elastic Scaling" title="Direct link to Elastic Scaling">​</a></h2><p>ByConity supports flexible expansion and contraction, enabling real-time and on-demand expansion and contraction of computing resources, ensuring efficient use of resources.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="resource-isolation">Resource Isolation<a href="#resource-isolation" class="hash-link" aria-label="Direct link to Resource Isolation" title="Direct link to Resource Isolation">​</a></h2><p>ByConity isolates the resources of different tenants, ensuring that tenants are not affected by each other.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="strong-data-consistency">Strong Data Consistency<a href="#strong-data-consistency" class="hash-link" aria-label="Direct link to Strong Data Consistency" title="Direct link to Strong Data Consistency">​</a></h2><p>ByConity ensures strong consistency of data read and write, ensuring that data is always up to date with no inconsistencies between reads and writes.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="high-performance">High Performance<a href="#high-performance" class="hash-link" aria-label="Direct link to High Performance" title="Direct link to High Performance">​</a></h2><p>ByConity adopts mainstream OLAP engine optimizations, such as column storage, vectorized execution, MPP execution, query optimization, etc., ensuring excellent read and write performance.</p><h1>ByConity&#x27;s Technical Architecture</h1><p>ByConity&#x27;s architecture is divided into three layers:</p><ol><li>Service access layer: Responsible for client data and service access, i.e., ByConity Server</li><li>Computing group: ByConity&#x27;s computing resource layer, where each Virtual Warehouse is a computing group</li><li>Data storage: Distributed file system, such as HDFS, S3, etc.</li></ol><p><img loading="lazy" alt="Figure 3: ByConity&amp;#39;s architecture" src="/assets/images/f3-three-layer-architecture-403f695f8cd48cb936283a4c7b86553d.png" width="1483" height="809" class="img_astN">
Figure 3: ByConity&#x27;s architecture</p><h1>Working Principle of ByConity</h1><p>ByConity is a powerful open-source cloud-native data warehouse that adopts a storage-computing separation architecture. In this section, we will examine the working principle of ByConity and the interaction process of each component of ByConity through the complete life cycle of a SQL.</p><p><img loading="lazy" alt="Figure 4: ByConity internal component interaction diagram" src="/assets/images/f4-internal-component-interaction-301b3d1e7d179142c3b1ce535f1ce105.png" width="1280" height="1210" class="img_astN">
Figure 4: ByConity internal component interaction diagram</p><p>Figure 4 above is the interaction diagram of ByConity components. The dotted line in the figure indicates the inflow of a SQL, the two-way arrow in the solid line indicates the interaction within the component, and the one-way arrow indicates the data processing and output to the client.</p><p>ByConity&#x27;s working principle can be divided into three stages:</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="stage-1-query-request">Stage 1: Query Request<a href="#stage-1-query-request" class="hash-link" aria-label="Direct link to Stage 1: Query Request" title="Direct link to Stage 1: Query Request">​</a></h2><p>The client submits a query request to the server, and the server first performs parsing, then analyzes and optimizes through analyzer and optimizer to generate a more efficient executable plan. Here, metadata MetaData is read, which is stored in a distributed KV. ByConity uses FoundationDB and reads the metadata through the Catalog.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="stage-2-plan-scheduling">Stage 2: Plan Scheduling<a href="#stage-2-plan-scheduling" class="hash-link" aria-label="Direct link to Stage 2: Plan Scheduling" title="Direct link to Stage 2: Plan Scheduling">​</a></h2><p>ByConity submits the executable plan generated by the analysis and optimizer to the scheduler (Plan Scheduler). The scheduler obtains idle computing resources by accessing the resource manager and decides which nodes to schedule query tasks for execution.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="stage-3-query-execution">Stage 3: Query Execution<a href="#stage-3-query-execution" class="hash-link" aria-label="Direct link to Stage 3: Query Execution" title="Direct link to Stage 3: Query Execution">​</a></h2><p>Query requests are finally executed on ByConity&#x27;s Worker, and the Worker will read data from the lowest-level Cloud storage and perform calculations by establishing a pipeline. Finally, the calculation results of multiple workers are aggregated by the server and returned to the client.</p><p>In addition to the above components, ByConity also has two main components, Time-stamp Oracle and Deamon Manager. The former ByConity supports transaction processing, and the latter manages and schedules some subsequent tasks.</p><h1>Main Component Library</h1><p>To better understand the working principle of ByConity, let&#x27;s take a look at the main components of ByConity:</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="metadata-management">Metadata Management<a href="#metadata-management" class="hash-link" aria-label="Direct link to Metadata Management" title="Direct link to Metadata Management">​</a></h2><p>ByConity provides a highly available and high-performance metadata read and write service - Catalog Server. And ByConity supports complete transaction semantics (ACID). At the same time, we have made a better abstraction of the Catalog Server, making the back-end storage system pluggable. Currently, we support Apple&#x27;s open-source FoundationDB, which can be expanded to support more back-end storage systems later.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="query-optimizer">Query Optimizer<a href="#query-optimizer" class="hash-link" aria-label="Direct link to Query Optimizer" title="Direct link to Query Optimizer">​</a></h2><p>The query optimizer is one of the cores of the database system. A good optimizer can greatly improve query performance. ByConity&#x27;s self-developed optimizer improves optimization capabilities based on two directions:</p><ul><li>RBO: Rule-Based Optimization capability. Support: column pruning, partition pruning, expression simplification, subquery disassociation, predicate pushdown, redundant operator elimination, outer-JOIN to INNER-JOIN, operator pushdown storage, distributed operator splitting, etc.</li><li>CBO: Cost-Based Optimization capability. Support: Join Reorder, Outer-Join Reorder, Join/Agg Reorder, CTE, Materialized View, Dynamic Filter Push-Down, Magic Set, and other cost-based optimization capabilities. And integrate Property Enforcement for distributed planning.</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="query-scheduling">Query Scheduling<a href="#query-scheduling" class="hash-link" aria-label="Direct link to Query Scheduling" title="Direct link to Query Scheduling">​</a></h2><p>ByConity currently supports two query scheduling strategies: Cache-aware scheduling and Resource-aware scheduling.</p><ul><li>The cache-aware scheduling policy is aimed at scenarios where storage and computing are separated, aiming to maximize the use of the cache and avoid cold reads. The cache-aware scheduling strategy will try to schedule tasks to nodes with corresponding data caches, so that calculations can hit the cache and improve read and write performance.</li><li>Resource-aware scheduling perceives the resource usage of different nodes in the computing group in the entire cluster and performs targeted scheduling to maximize resource utilization. At the same time, it also performs flow control to ensure reasonable use of resources and avoid negative effects caused by overload, such as system downtime.</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="computing-group">Computing Group<a href="#computing-group" class="hash-link" aria-label="Direct link to Computing Group" title="Direct link to Computing Group">​</a></h2><p>ByConity supports different tenants to use different computing resources. Under ByConity&#x27;s new architecture, it is easy to implement features such as multi-tenant isolation and read-write separation. Different tenants can use different computing groups to achieve multi-tenant isolation and support read-write separation. Due to the convenient expansion and contraction, the computing group can be dynamically expanded and contracted on demand to ensure efficient resource utilization. When resource utilization is not high, resource sharing can be carried out, and computing groups can be seconded to other tenants to maximize resource utilization and reduce costs.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="virtual-file-system">Virtual File System<a href="#virtual-file-system" class="hash-link" aria-label="Direct link to Virtual File System" title="Direct link to Virtual File System">​</a></h2><p>The virtual file system module is used as the middle layer of data reading and writing. ByConity has made a better package, exposing storage as a service to other modules to realize &quot;storage as a service&quot;. The virtual file system provides a unified file system abstraction, shields different back-end implementations, facilitates expansion, and supports multiple storage systems, such as HDFS or object storage.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="cache-acceleration">Cache Acceleration<a href="#cache-acceleration" class="hash-link" aria-label="Direct link to Cache Acceleration" title="Direct link to Cache Acceleration">​</a></h2><p>ByConity performs query acceleration through caching. Under the architecture of separating storage and computing, ByConity performs cache acceleration in both metadata and data dimensions. In the metadata dimension, by caching in the memory of ByConity&#x27;s Server side, table, and partition are used as granularity. In the data dimension, ByConity&#x27;s Worker side, that is, the computing group, is used for caching, and the cache on the Worker side is hierarchical. At the same time, memory and disk are used, and the mark set is used as the cache granularity, thereby effectively improving the query speed.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="how-to-obtain-and-deploy">How to Obtain and Deploy<a href="#how-to-obtain-and-deploy" class="hash-link" aria-label="Direct link to How to Obtain and Deploy" title="Direct link to How to Obtain and Deploy">​</a></h2><p>ByConity currently supports four acquisition and deployment modes. Community developers are welcome to use them and submit issues to us:</p><ul><li>Stand-alone version<ul><li>Use docker compose to pull up Reference: <a href="https://github.com/ByConity/byconity-docker" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/byconity-docker</a></li></ul></li><li>K8s cluster version mode<ul><li>Use K8s deployment reference: <a href="https://github.com/ByConity/byconity-deploy" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/byconity-deploy</a></li></ul></li><li>Physical machine deployment mode<ul><li>Deploy on a physical machine using the package manager: <a href="https://github.com/ByConity/ByConity/tree/master/packages" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/ByConity/tree/master/packages</a></li></ul></li><li>Source code compilation method<ul><li>Reference: <a href="https://github.com/ByConity/ByConity#build-byconity" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/ByConity#build-byconity</a></li></ul></li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="byconitys-future-open-source-plan">ByConity&#x27;s Future Open-Source Plan<a href="#byconitys-future-open-source-plan" class="hash-link" aria-label="Direct link to ByConity&#x27;s Future Open-Source Plan" title="Direct link to ByConity&#x27;s Future Open-Source Plan">​</a></h2><p>ByConity includes several key milestones in its open-source community roadmap through 2023. These milestones are designed to enhance ByConity&#x27;s functionality, performance, and ease of use. Among them, the development of new storage engines, support for more data types, and integration with other data management tools are some important areas of focus. We have listed the following directions, and we have created an issue on Github: <a href="https://github.com/ByConity/ByConity/issues/26" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/ByConity/issues/26</a>, inviting community partners to join us to discuss co-construction:</p><ul><li>In terms of performance improvement: ByConity hopes to continue to improve performance, and here are three technical directions. One is to use indexes for acceleration, which includes four aspects:<ul><li>Optimize the existing skip index;</li><li>Explore the implementation of new index research, such as zorder-index and inverted index;</li><li>ByConity builds and accelerates Hive table indexes</li><li>Index recommendation and conversion, lowering the threshold for users to use
The second is the continuous optimization of the query optimizer; the third is that ByConity&#x27;s cache mechanism is local, and each computing group can only access its own cache. In the future, it is hoped to implement a distributed cache mechanism to further improve the cache hit rate.</li></ul></li><li>Stability improvement: There are two aspects here. One is to support resource isolation in more dimensions. Currently, it only supports resource isolation in the computing group dimension. In the next step, resource isolation will also be supported on the server side, providing better end-to-end Guaranteed multi-tenancy capability. The second direction is to enrich metrics and improve observability and problem diagnosis capabilities.</li><li>Enterprise-level feature enhancements: We hope to achieve more detailed permission control, including column-level permission control. The other is to improve the functions related to data security, such as data backup and recovery and data end-to-end encryption. Finally, we continue to explore the deep compression of data to save storage costs.</li><li>Ecological compatibility improvement: This direction is the most important point. ByConity plans to support more types of storage backends, such as AWS&#x27;s S3, Volcano Engine&#x27;s object storage, etc. In terms of improving ecological compatibility, it includes integration with some drivers and some open source software. At the same time, we also hope to support federated queries of data lakes, such as Hudi, Iceberg, etc.</li></ul><p>In short, ByConity is an open source cloud-native data warehouse that provides read-write separation, elastic expansion and contraction, tenant resource isolation, and strong consistency of data read and write. Its storage-computing separation architecture, combined with mainstream OLAP engine optimization, ensures excellent read and write performance. As ByConity continues to develop and improve, it is expected to become an important tool for cloud-native data warehouses in the future.</p><p>We have a video that introduces ByConity in detail, including a demo of ByConity. If you need more information, you can check the following link: <a href="https://www.bilibili.com/video/BV15k4y1b7pw/?spm_id_from=333.999.0.0&amp;vd_source=71f3be2102fec1a0171b49a530cefad0" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV15k4y1b7pw/?spm_id_from=333.999.0.0&amp;vd_source=71f3be2102fec1a0171b49a530cefad0</a></p><p>Scan the QR code to reply <!-- -->[name + QR code]<!-- --> Join the ByConity communication group to get more project dynamics and activity information.</p><p><img loading="lazy" alt="ByConity Community QR Code" src="/assets/images/f5-byconity-community-qrcode-b75b8db64319bf889444dc7b4fd21124.png" width="621" height="627" class="img_astN">
ByConity Community QR Code</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_HAaO padding--none margin-left--sm"><li class="tag_Ut5_"><a class="tag_sQub tagRegular_dfhI" href="/blog/tags/video-introduction">video introduction</a></li><li class="tag_Ut5_"><a class="tag_sQub tagRegular_dfhI" href="/blog/tags/docusaurus">docusaurus</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"></nav></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="row"><div class="col col--3"><img src="/img/footer-logo.svg" alt="ByConity" class="themedImage_DeRy themedImage--light_hbln footer__logo"><img src="/img/footer-logo.svg" alt="ByConity" class="themedImage_DeRy themedImage--dark_qgR1 footer__logo"></div><div class="col"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/introduction/main-principle-concepts">Docs</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/community">Community</a></li><li class="footer__item"><a class="footer__link-item" href="/users">Users</a></li><li class="footer__item"><a href="https://space.bilibili.com/2065226922?spm_id_from=333.1007.0.0" target="_blank" rel="noopener noreferrer" class="footer__link-item">Bilibili<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_LBPb"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/ByConity/ByConity" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_LBPb"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 ByteDance.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.722bd2dd.js"></script>
<script src="/assets/js/main.8d48683e.js"></script>
</body>
</html>