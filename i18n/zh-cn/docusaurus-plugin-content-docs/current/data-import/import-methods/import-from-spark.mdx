---
title: 从Spark导入
sidebar_position: 4
tags:
  - Docs
---


# 导入数据

## 通过 Spark 导入

使用part writer工具可以实现将数据文件转化为part文件，不经过ByConity引擎，从而实现ByConity查询与构建分离，一定程度缓解数据导入和查询的资源竞争，提高查询性能。下面介绍如何使用part writer将数据导入到ByConity。

1. 使用part writer生成part文件

    part writer 接收一个 sql 语句作为参数，用户通过 sql 语句指定源数据文件、数据文件格式、数据 schema、part 文件保存路径等详细信息。具体使用方式如下：

```

./part_writer "load CSV file '/path/to/data/test.csv' as table db.tablename(col1 UInt64, col2 String, col3 Nullable(String)) partition by col1 order by (col2, col3) location '/path/to/dest/'"

```

- `CSV`指定源数据文件格式；此外，part writer 还可使用 CSVWithNames， JSONEachRow 等多种 clickhouse 原生支持的数据文件格式。
- `/path/to/data/test.csv` 指定了源数据文件；支持从本地和 hdfs 读取源数据。如使用 hdfs 数据文件，指定路径为：`hdfs://host:port/path/to/data/file`;
- `/path/to/dest/`指定 part 文件写入的目标文件夹；支持将 part 文件直接写到 hdfs 上，ByConity 可以从 hdfs 上拉取并加载 part 文件。
- `as table` 指定了插入数据的 schema 信息
- `partition by` 和 `order by` 分别指定了数据表的分区键和排序键，多个键之间使用逗号进行分割并且需要用圆括号包裹， 如: `partition by (name, id)`。
- ByConity 特殊选项，`settings cnch=1`，用于将生成的 part 直接 dump 成 ByConity 的 part 格式并写入 location 选项指定的 hdfs 路径。

2. 把part 文件导入 ByConity

    生成好的 part 文件可以直接 copy 到 ByConity 表对应的数据文件路径下，然后通过重启 ByConity server 加载；也可以将 part 文件目录 copy 到表的 detached 目录下，通过 attach 命令加载 part 文件, 如

```
alter table test attach part 'partfile'

```

如果使用 part writer 生成 part 文件时指定了直接上传到 hdfs，可以执行如下命令：

```
system fetch parts into db.table 'hdfs://host:port/path/to/part/'

```

ByConity 将自动从 hdfs 路径下拉取 part 文件并进行加载。同时，还支持以下 ByConity attach 语法，用于将 dump 到 hdfs 的 parts 导入到目标表：

```
alter table test attach parts from '/hdfs/path/to/dumped/dir'

```

这种方式下支持 spark 导入。因为在实际应用场景下，需要往 ByConity 集群导入大量数据，可以考虑使用 spark。首先从外部将数据读入 spark dataset；然后根据 sharding key 对 dataset 进行 repartition，保证将要发送到不同 ByConity 节点到数据落在不同的 partition 上（可能需要根据实际情况，调整 `spark.sql.shuffle.partitions` 参数使 partition 不小于 ByConity 主节点数）；对于每个 partition，首先通过调用 part writer 生成 part 文件，并指定 part 文件上传到 hdfs，然后通过向对应 ByConity 节点发送 http 请求，通知 ByConity 加载 part 文件。
![](../assets/boxcnlSkMX0zkWno7WT7250zU1f.png)
