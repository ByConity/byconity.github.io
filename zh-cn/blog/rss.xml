<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>ByConity Blog</title>
        <link>https://byconity.github.io/zh-cn/blog</link>
        <description>ByConity Blog</description>
        <lastBuildDate>Sun, 10 Sep 2023 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>zh-cn</language>
        <item>
            <title><![CDATA[ByConity ELT原理详解]]></title>
            <link>https://byconity.github.io/zh-cn/blog/byconity-elt</link>
            <guid>https://byconity.github.io/zh-cn/blog/byconity-elt</guid>
            <pubDate>Sun, 10 Sep 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[背景]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="背景">背景<a href="#背景" class="hash-link" aria-label="背景的直接链接" title="背景的直接链接">​</a></h2><p>谈到数据仓库， 一定离不开使用Extract-Transform-Load (ETL)或 Extract-Load-Transform (ELT)。 将来源不同、格式各异的数据提取到数据仓库中，并进行处理加工。传统的数据转换过程一般采用Extract-Transform-Load (ETL)来将业务数据转换为适合数仓的数据模型，然而，这依赖于独立于数仓外的ETL系统，因而维护成本较高。
ByConity 作为云原生数据仓库，从0.2.0版本开始逐步支持 Extract-Load-Transform (ELT)，使用户免于维护多套异构数据系统。本文将介绍 ByConity 在ELT方面的能力规划，实现原理和使用方式等。</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="etl场景和方案">ETL场景和方案<a href="#etl场景和方案" class="hash-link" aria-label="ETL场景和方案的直接链接" title="ETL场景和方案的直接链接">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="elt与etl的区别">ELT与ETL的区别<a href="#elt与etl的区别" class="hash-link" aria-label="ELT与ETL的区别的直接链接" title="ELT与ETL的区别的直接链接">​</a></h3><ul><li>ETL：是用来描述将数据从来源端经过抽取、转置、加载至目的端（数据仓库）的过程。Transform通常描述在数据仓库中的前置数据加工过程。</li></ul><p><img loading="lazy" src="/zh-cn/assets/images/elt1-ccfe04dd9abfa7d620e7c93b024bbc2c.png" width="1080" height="388" class="img_astN"></p><ul><li>ELT 专注于将最小处理的数据加载到数据仓库中，而把大部分的转换操作留给分析阶段。相比起前者（ETL)，它不需要过多的数据建模，而给分析者提供更灵活的选项。ELT已经成为当今大数据的处理常态，它对数据仓库也提出了很多新的要求。</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="资源重复的挑战">资源重复的挑战<a href="#资源重复的挑战" class="hash-link" aria-label="资源重复的挑战的直接链接" title="资源重复的挑战的直接链接">​</a></h3><p><img loading="lazy" src="/zh-cn/assets/images/elt2-caa5b7ef03a16466c984978e04735732.png" width="1048" height="402" class="img_astN"></p><p>典型的数据链路如下：我们将行为数据、日志、点击流等通过MQ/ Kafka/ Flink将其接入存储系统当中，存储系统又可分为域内的HDFS 和云上的 OSS&amp; S3 这种远程储存系统，然后进行一系列的数仓的ETL操作，提供给 OLAP系统完成分析查询。
但有些业务需要从上述的存储中做一个分支，因此会在数据分析的某一阶段，从整体链路中将数据导出，做一些不同于主链路的ETL操作，会出现两份数据存储。其次在这过程中也会出现两套不同的ETL逻辑。
当数据量变大，计算冗余以及存储冗余所带来的成本压力也会愈发变大，同时，存储空间的膨胀也会让弹性扩容变得不便利。</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="业界解决思路">业界解决思路<a href="#业界解决思路" class="hash-link" aria-label="业界解决思路的直接链接" title="业界解决思路的直接链接">​</a></h3><p>在业界中，为了解决以上问题，有以下几类流派：</p><ul><li>数据预计算流派：如Kylin等。如果Hadoop系统中出报表较慢或聚合能力较差，可以去做一个数据的预计算，提前将配的指标的cube或一些视图算好。实际SQL查询时，可以直接用里面的cube或视图做替换，之后直接返回。</li><li>流批一体派：如 Flink、Risingwave。在数据流进时，针对一些需要出报表或者需要做大屏的数据直接内存中做聚合。聚合完成后，将结果写入HBase或MySQL中再去取数据，将数据取出后作展示。Flink还会去直接暴露中间状态的接口，即queryable state，让用户更好的使用状态数据。但是最后还会与批计算的结果完成对数，如果不一致，需要进行回查操作，整个过程考验运维/开发同学的功力。</li><li>湖仓一体&amp;HxxP:将数据湖与数据仓库结合起来。</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="elt-in-byconity">ELT in ByConity<a href="#elt-in-byconity" class="hash-link" aria-label="ELT in ByConity的直接链接" title="ELT in ByConity的直接链接">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="整体执行流程">整体执行流程<a href="#整体执行流程" class="hash-link" aria-label="整体执行流程的直接链接" title="整体执行流程的直接链接">​</a></h3><p><img loading="lazy" src="/zh-cn/assets/images/elt3-9500640cd18aa26d9bc3357e5dd627c9.png" width="1280" height="828" class="img_astN"></p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="elt任务对系统的要求">ELT任务对系统的要求：<a href="#elt任务对系统的要求" class="hash-link" aria-label="ELT任务对系统的要求：的直接链接" title="ELT任务对系统的要求：的直接链接">​</a></h3><ol><li>整体易扩展：导入和转换通常需要大量的资源，系统需要通过水平扩展的方式来满足数据量的快速增长。</li><li>可靠性和容错能力：大量的job能有序调度；出现task偶然失败（OOM）、container失败时，能够拉起重试；能处理一定的数据倾斜</li><li>效率&amp;性能：有效利用多核多机并发能力；数据快速导入；内存使用有效（内存管理）；CPU优化（向量化、codegen）</li><li>生态&amp;可观测性：可对接多种工具；任务状态感知；任务进度感知；失败日志查询；有一定可视化能力</li></ol><p>ByConity 针对ELT任务的要求，以及当前场景遇到的困难，新增了以下特性和优化改进。</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="分阶段执行stage-level-scheduling">分阶段执行（Stage-level Scheduling）<a href="#分阶段执行stage-level-scheduling" class="hash-link" aria-label="分阶段执行（Stage-level Scheduling）的直接链接" title="分阶段执行（Stage-level Scheduling）的直接链接">​</a></h3><p><img loading="lazy" src="/zh-cn/assets/images/elt4-494e04491f26b62b894e7bcea61beb89.png" width="830" height="528" class="img_astN"></p><h4 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="原理解析">原理解析<a href="#原理解析" class="hash-link" aria-label="原理解析的直接链接" title="原理解析的直接链接">​</a></h4><ul><li>当前 ClickHouse的 SQL 执行过程如下：<ul><li>第一阶段，Coordinator 收到分布式表查询后将请求转换为对 local 表查询发送给每个 shard 节点；</li><li>第二阶段，Coordinator 收到各个节点的结果后汇聚起来处理后返回给客户端；</li></ul></li><li>ClickHouse 将Join操作中的右表转换为子查询，带来如下几个问题都很难以解决：<ul><li>复杂的query有多个子查询，转换复杂度高；</li><li>Join表较大时，容易造成worker节点的OOM；</li><li>聚合阶段在Cooridnator，压力大，容易成为性能瓶颈；</li></ul></li></ul><p><img loading="lazy" src="/zh-cn/assets/images/elt5-feb79e6e129f3aeb9ab86aa2b92767f4.png" width="443" height="521" class="img_astN"> <img loading="lazy" src="/zh-cn/assets/images/elt6-8481e54f07ead8899937be2f068395c6.png" width="395" height="536" class="img_astN"> <img loading="lazy" src="/zh-cn/assets/images/elt7-306fde6a5dad86ac0bb435ce6909f7bc.png" width="320" height="231" class="img_astN"></p><p>不同于ClickHouse，我们在ByConity 中实现了对复杂查询的执行优化。通过对执行计划的切分，将之前的两阶段执行模型转换为分阶段执行。在逻辑计划阶段，根据算子类型插入exchange算子。执行阶段根据exchange算子将整个执行计划进行DAG切分，并且分stage进行调度。stage之间的exchange算子负责完成数据传输和交换。
关键节点：</p><ol><li>exchange节点插入</li><li>切分stage</li><li>stage scheduler</li><li>segment executer</li><li>exchange manager</li></ol><p><img loading="lazy" src="/zh-cn/assets/images/elt8-91b30f3fc765792bb11ada8747fd2ec2.png" width="583" height="529" class="img_astN"></p><p>这里重点来讲一下exchange的视线。上图可以看到，最顶层的是query plan。下面转换成物理计划的时候，我们会根据不同的数据分布的要求转换成不同的算子。source层是接收数据的节点，基本都是统一的，叫做ExchangeSource。Sink则有不同的实现，BroadcastSink、Local、PartitionSink等，他们是作为map task的一部分去运行的。如果是跨节点的数据操作，我们在底层使用统一的brpc流式数据传输，如果是本地，则使用内存队列来实现。针对不同的点，我们进行了非常细致的优化：</p><ul><li>数据传输层<ul><li>进程内通过内存队列，无序列化，zero copy</li><li>进程间使用brpc stream rpc，保序、连接复用、状态码传输、压缩等</li></ul></li><li>算子层<ul><li>批量发送</li><li>线程复用，减少线程数量</li></ul></li></ul><h4 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="带来的收益">带来的收益<a href="#带来的收益" class="hash-link" aria-label="带来的收益的直接链接" title="带来的收益的直接链接">​</a></h4><p>因为ByConity 彻底采用了多阶段的查询执行方式，整体有很大的收益：</p><ul><li>Cooridnator更稳定、更高效<ul><li>聚合等算子拆分到worker节点执行</li><li>Cooridnator节点只需要聚合最终结果</li></ul></li><li>Worker OOM减少<ul><li>进行了stage切分，每个stage的计算相对简单</li><li>增加了exchange算子，减少内存压力</li></ul></li><li>网络连接更加稳定、高效<ul><li>exchange算子有效传输</li><li>复用连接池</li></ul></li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="自适应的调度器adaptive-scheduler">自适应的调度器（Adaptive Scheduler）<a href="#自适应的调度器adaptive-scheduler" class="hash-link" aria-label="自适应的调度器（Adaptive Scheduler）的直接链接" title="自适应的调度器（Adaptive Scheduler）的直接链接">​</a></h3><p>Adaptive Scheduler 属于我们在稳定性方面所做的特性。在OLAP场景中可能会发现部分数据不全或数据查询超时等，原因是每个worker是所有的query共用的，这样一旦有一个worker 较慢就会导致整个query的执行受到影响。</p><p><img loading="lazy" src="/zh-cn/assets/images/elt9-5832a9a799604adcf46d919e33a0fd48.png" width="360" height="360" class="img_astN"></p><p>计算节点共用存在的问题：</p><ul><li>Scan 所在的节点负载和不同查询所需的扫描数据量相关，做不到完全平均；</li><li>各 Plan Segment 所需资源差异大；
这就导致worker节点之间的负载严重不均衡。负载较重的worker节点就会影响query整体的进程。因此我们做了以下的优化方案：</li><li>建立 Worker 健康度机制。Server 端建立 Worker 健康度管理类，可以快速获取 Worker Group 的健康度信息，包括CPU、内存、运行Query数量等信息。</li><li>自适应调度：每个SQL 根据 Worker 健康度动态的进行选择以及计算节点并发度控制。</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="查询的队列机制query-queue">查询的队列机制（Query Queue）<a href="#查询的队列机制query-queue" class="hash-link" aria-label="查询的队列机制（Query Queue）的直接链接" title="查询的队列机制（Query Queue）的直接链接">​</a></h3><p><img loading="lazy" src="/zh-cn/assets/images/elt10-00e36ff14589366f30019c9a22871ed0.png" width="1280" height="1024" class="img_astN"> <img loading="lazy" src="/zh-cn/assets/images/elt11-7a76adabaa507e6b8502fef8764e9333.png" width="1280" height="1047" class="img_astN"></p><p>我们的集群也会出现满载情况，即所有的worker都是不健康的或者满载/超载的，就会用查询队列来进行优化。
我们直接在server端做了一个manager。每次查询的时候manager会去check集群的资源，并且持有一个锁。如果资源不够用，则等待资源释放后去唤醒这个锁。这就避免了Server端不限制的下发计算任务，导致worker节点超载，然后崩掉的情况。
当前实现相对简单。server是多实例，每个server实例中都有queue，所持有的是一个局部视角，缺乏全局的资源视角。除此之外，每个queue中的查询状态没有持久化，只是简单的缓存在内存中。
后续，我们会增加server之间的协调，在一个全局的视角上对查询并发做限制。也会对server实例中query做持久化，增加一些failover的场景支持。</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="异步执行async-execution">异步执行（Async Execution）<a href="#异步执行async-execution" class="hash-link" aria-label="异步执行（Async Execution）的直接链接" title="异步执行（Async Execution）的直接链接">​</a></h3><p><img loading="lazy" src="/zh-cn/assets/images/elt12-8734ea2851ceceb9d629659aecd68ac5.png" width="1280" height="708" class="img_astN"></p><p>ELT任务的一个典型特征就是：相对于即时分析，他们的运行时间会相对较长。一般ELT任务执行时长为分钟级，甚至到达小时级。
目前 ClickHouse的客户端查询都采用阻塞的方式进行返回。这样就造成了客户端长期处于等待的情况，而在这个等待过程中还需要保持和服务端的连接。在不稳定的网络情况下，客户端和服务端的连接会断开，从而导致服务端的任务失败。
为了减少这种不必要的失败，以及减少客户端为了维持连接的增加的复杂度。我们开发了异步执行的功能，它的实现如下：</p><ol><li>用户指定异步执行。用户可以通过settings enable_async_query = 1的方式进行per query的指定。也可以通过set enable_async_query = 1的方式进行session级别的指定。</li><li>如果是异步query，则将其放到后台线程池中运行</li><li>静默io。当异步query执行时，则需要切断它和客户端的交互逻辑，比如输出日志等。</li></ol><p>针对query的初始化还是在session的同步线程中进行。一旦完成初始化，则将query状态写入到metastore，并向客户端返回async query id。客户端可以用这个id查询query的状态。async query id返回后，则表示完成此次查询的交互。这种模式下，如果语句是select，那么后续结果则无法回传给客户端。这种情况下我们推荐用户使用async query + select...into outfile的组合来满足需求。</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="未来规划">未来规划<a href="#未来规划" class="hash-link" aria-label="未来规划的直接链接" title="未来规划的直接链接">​</a></h2><p>针对ELT混合负载，ByConity 0.2.0版本目前只是牛刀小试。后续的版本中我们会持续优化查询相关的能力，ELT为核心的规划如下：</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="故障恢复能力">故障恢复能力<a href="#故障恢复能力" class="hash-link" aria-label="故障恢复能力的直接链接" title="故障恢复能力的直接链接">​</a></h3><ul><li>算子Spill<ul><li>Sort、Agg、Join 算子Spill；</li><li>Exchange Spill 能力；</li></ul></li><li>Recoverability 容错恢复<ul><li>算子执行恢复：ELT任务运行时长较长时，中间 Task的偶发失败会导致整个Query失败，支持Task 级别重试可以极大地降低环境原因导致的偶发失败；</li><li>Stage重试：当节点失败时，可以进行 Stage级别的重试；</li><li>保存队列作业状态的能力；</li></ul></li><li>Remote Shuffle Service：当前业界开源的 shuffle service通常为Spark定制，没有通用的客户端，比如c++客户端。后续我们会补充这部分能力。</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="资源">资源<a href="#资源" class="hash-link" aria-label="资源的直接链接" title="资源的直接链接">​</a></h3><ul><li>计算资源可指定：用户可指定query需要的计算资源；</li><li>计算资源预估/预占：可动态预估query需要的计算资源，并通过预占的方式进行调配；</li><li>动态申请资源：当前worker均为常驻进程/节点。动态申请资源可以提高利用率；</li><li>更细粒度的资源隔离：通过worker group或者进程级别的隔离，减少各query之间相互影响；</li></ul>]]></content:encoded>
            <category>video introduction</category>
            <category>docusaurus</category>
        </item>
        <item>
            <title><![CDATA[ByteDance Open Sources Its Cloud Native Data Warehouse ByConity]]></title>
            <link>https://byconity.github.io/zh-cn/blog/2023-05-24-byconity-announcement-opensources-its-cloudnative-data-warehouse</link>
            <guid>https://byconity.github.io/zh-cn/blog/2023-05-24-byconity-announcement-opensources-its-cloudnative-data-warehouse</guid>
            <pubDate>Wed, 24 May 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[ByConity is an open-source cloud-native data warehouse developed by ByteDance. It utilizes a computing-storage separation architecture and offers various essential features, including the separation of computing and storage, elastic scalability, tenant resource isolation, and strong consistency in data reading and writing. By leveraging optimizations from popular OLAP engines like column storage, vectorized execution, MPP execution, and query optimization, ByConity delivers exceptional read and write performance.]]></description>
            <content:encoded><![CDATA[<p>ByConity is an open-source cloud-native data warehouse developed by ByteDance. It utilizes a computing-storage separation architecture and offers various essential features, including the separation of computing and storage, elastic scalability, tenant resource isolation, and strong consistency in data reading and writing. By leveraging optimizations from popular OLAP engines like column storage, vectorized execution, MPP execution, and query optimization, ByConity delivers exceptional read and write performance.</p><h1>History of ByConity</h1><p>The origins of ByConity can be traced back to 2018 when ByteDance initially implemented ClickHouse for internal use. As the business grew, the data volume increased significantly to cater to a large user base. However, ClickHouse's Shared-Nothing architecture, where each node operates independently without sharing storage resources, posed certain challenges during its usage. Here are some of the issues encountered:</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="expansion-and-contraction">Expansion and contraction:<a href="#expansion-and-contraction" class="hash-link" aria-label="Expansion and contraction:的直接链接" title="Expansion and contraction:的直接链接">​</a></h2><p>Due to the tight coupling of computing and storage resources in ClickHouse, scaling the system incurred higher costs and involved data migration. This prevented real-time on-demand scalability, resulting in inefficient resource utilization.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="multi-tenancy-and-shared-cluster-environment">Multi-tenancy and shared cluster environment:<a href="#multi-tenancy-and-shared-cluster-environment" class="hash-link" aria-label="Multi-tenancy and shared cluster environment:的直接链接" title="Multi-tenancy and shared cluster environment:的直接链接">​</a></h2><p>ClickHouse's tightly coupled architecture led to interactions among multiple tenants in a shared cluster environment. Since reading and writing operations were performed on the same node, they often interfered with each other, impacting overall performance.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="performance-limitations">Performance limitations:<a href="#performance-limitations" class="hash-link" aria-label="Performance limitations:的直接链接" title="Performance limitations:的直接链接">​</a></h2><p>ClickHouse's support for complex queries, such as multi-table join operations, was not optimal, which hindered the system's ability to handle such queries efficiently.</p><p>To address these pain points, ByteDance undertook an architectural upgrade of ClickHouse. In 2020, we initiated the ByConity project internally. After releasing the Beta version in January 2023, the project was officially made available to the public at the end of May 2023.</p><p><img loading="lazy" alt="Figure 1 ByteDance ClickHouse usage" src="/zh-cn/assets/images/f1-byte-clickhouse-usage-cefbe6a269f39c214219df180aef1560.png" width="1652" height="896" class="img_astN"></p><p>Figure 1: ByteDance ClickHouse Usage</p><h1>Features of ByConity</h1><p>ByConity implements a computing and storage separation architecture that transforms the original local management of computing and storage on individual nodes. Instead, it adopts a unified management approach for all data across the entire cluster using distributed storage. This transformation results in stateless computing nodes, enabling dynamic expansion and contraction by leveraging the scalability of distributed storage and the stateless nature of computing nodes. ByConity offers several crucial features that enhance its functionality and performance:</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="storage-computing-separation">Storage-Computing Separation<a href="#storage-computing-separation" class="hash-link" aria-label="Storage-Computing Separation的直接链接" title="Storage-Computing Separation的直接链接">​</a></h2><p>One of the critical advantages of ByConity is its storage-computing separation architecture, which enables read-write separation and elastic scaling. This architecture ensures that read and write operations do not affect each other, and computing resources and storage resources can be independently expanded and contracted on demand, ensuring efficient resource utilization. ByConity also supports multi-tenant resource isolation, making it suitable for multi-tenant environments.</p><p><img loading="lazy" alt="Figure 2: ByConity storage-computing separation to achieve multi-tenant isolation" src="/zh-cn/assets/images/f2-storage-computing-separation-51a4fd7a3073479cf431a48278204cd5.png" width="1203" height="561" class="img_astN">
Figure 2: ByConity storage-computing separation to achieve multi-tenant isolation</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="resource-isolation">Resource Isolation<a href="#resource-isolation" class="hash-link" aria-label="Resource Isolation的直接链接" title="Resource Isolation的直接链接">​</a></h2><p>ByConity provides resource isolation, ensuring that different tenants have separate and independent resources. This prevents interference or impact between tenants, promoting data privacy and efficient multi-tenancy support.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="elastic-scaling">Elastic Scaling<a href="#elastic-scaling" class="hash-link" aria-label="Elastic Scaling的直接链接" title="Elastic Scaling的直接链接">​</a></h2><p> ByConity supports elastic expansion and contraction, allowing for real-time and on-demand scaling of computing resources. This flexibility ensures efficient resource utilization and enables the system to adapt to changing workload requirements.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="strong-data-consistency">Strong Data Consistency<a href="#strong-data-consistency" class="hash-link" aria-label="Strong Data Consistency的直接链接" title="Strong Data Consistency的直接链接">​</a></h2><p>ByConity ensures strong consistency of data read and write operations. This ensures that data is always up-to-date and eliminates any inconsistencies between read and write operations, guaranteeing data integrity and accuracy.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="high-performance">High Performance<a href="#high-performance" class="hash-link" aria-label="High Performance的直接链接" title="High Performance的直接链接">​</a></h2><p>ByConity incorporates optimization techniques from mainstream OLAP engines, such as column storage, vectorized execution, MPP execution, and query optimization. These optimizations enhance read and write performance, enabling faster and more efficient data processing and analysis.</p><h1>Technical Architecture of ByConity</h1><p>ByConity follows a three-layer architecture consisting of:</p><ol><li>Service access layer: The service access layer, represented by ByConity Server, handles client data and service access. </li><li>Computing layer: The computing layer comprises multiple computing groups, where each Virtual Warehouse (VW) functions as a computing group.  </li><li>Data storage layer: The data storage layer utilizes distributed file systems like HDFS and S3.</li></ol><p><img loading="lazy" alt="Figure 3: ByConity&amp;#39;s architecture" src="/zh-cn/assets/images/f3-three-layer-architecture-403f695f8cd48cb936283a4c7b86553d.png" width="1483" height="809" class="img_astN">
Figure 3: ByConity's architecture</p><h1>Working Principle of ByConity</h1><p>ByConity is a powerful open-source cloud-native data warehouse that adopts a storage-computing separation architecture. In this section, we will examine the working principle of ByConity and the interaction process of each component of ByConity through the complete life cycle of a SQL.</p><p><img loading="lazy" alt="Figure 4: ByConity internal component interaction diagram" src="/zh-cn/assets/images/f4-internal-component-interaction-301b3d1e7d179142c3b1ce535f1ce105.png" width="1280" height="1210" class="img_astN">
Figure 4: ByConity internal component interaction diagram</p><p>Figure 4 depicts the interaction diagram of ByConity's components. In the figure, the dotted line represents the inflow of a SQL query, the double-headed arrow indicates component interaction, and the one-way arrow represents data processing and output to the client. Let's explore the interaction process of each component in ByConity throughout the complete lifecycle of a SQL query.</p><p>ByConity's working principle can be divided into three stages:</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="stage-1-query-request">Stage 1: Query Request<a href="#stage-1-query-request" class="hash-link" aria-label="Stage 1: Query Request的直接链接" title="Stage 1: Query Request的直接链接">​</a></h2><p>The client submits a Query request to the server. The server initially performs <strong>parsing</strong> and subsequently analyzes and optimizes the query through the <strong>Query Analyzer and Optimizer</strong> to generate an efficient executable plan. To access the required <strong>metadata</strong>, which is stored in a distributed key-value (KV) store, ByConity leverages <strong>FoundationDB</strong> and reads the metadata from the <strong>Catalog</strong>.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="stage-2-plan-scheduling">Stage 2: Plan Scheduling<a href="#stage-2-plan-scheduling" class="hash-link" aria-label="Stage 2: Plan Scheduling的直接链接" title="Stage 2: Plan Scheduling的直接链接">​</a></h2><p>ByConity passes the optimized executable plan to the <strong>Plan Scheduler</strong> component. The scheduler accesses the <strong>Resource Manager</strong> to obtain available computing resources and determines which nodes to schedule the query tasks for execution.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="stage-3-query-execution">Stage 3: Query Execution<a href="#stage-3-query-execution" class="hash-link" aria-label="Stage 3: Query Execution的直接链接" title="Stage 3: Query Execution的直接链接">​</a></h2><p>The Query request is executed on <strong>ByConity's Workers</strong>. The Workers read data from the underlying <strong>Cloud Storage</strong> and perform computations by establishing a Pipeline. The server then aggregates the calculation results from multiple Workers and returns them to the client.</p><p>Additionally, ByConity includes two main components: <strong>Time-stamp Oracle</strong> and <strong>Daemon Manager</strong>. The time-stamp oracle supports transaction processing, while the daemon manager manages and schedules subsequent tasks.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="main-component-library">Main Component Library<a href="#main-component-library" class="hash-link" aria-label="Main Component Library的直接链接" title="Main Component Library的直接链接">​</a></h2><p>To better understand the working principle of ByConity, let's take a look at the main components of ByConity:</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="metadata-management">Metadata Management<a href="#metadata-management" class="hash-link" aria-label="Metadata Management的直接链接" title="Metadata Management的直接链接">​</a></h3><p>ByConity offers a highly available and high-performance metadata read and write service called the Catalog Server. It supports complete transaction semantics (ACID). Furthermore, we have designed the Catalog Server with a flexible architecture, allowing for the pluggability of backend storage systems. Currently, we support Apple's open-source FoundationDB, and there is potential for extending support to other backend storage systems in the future.</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="query-optimizer">Query Optimizer<a href="#query-optimizer" class="hash-link" aria-label="Query Optimizer的直接链接" title="Query Optimizer的直接链接">​</a></h3><p>The query optimizer plays a crucial role in the performance of a database system. A well-designed optimizer can significantly enhance query performance, particularly in complex query scenarios, where it can achieve performance improvements ranging from several times to hundreds of times. ByConity's self-developed optimizer focuses on improving optimization capabilities through two main approaches:</p><ul><li>RBO (Rule-Based Optimization): This capability encompasses various optimizations such as column pruning, partition pruning, expression simplification, subquery dissociation, predicate pushdown, redundant operator elimination, Outer-Join to Inner-Join conversion, operator pushdown storage, distributed operator splitting, and other heuristic optimization techniques.</li><li>CBO (Cost-Based Optimization): ByConity's optimizer also includes cost-based optimization capabilities. This includes support for join reorder, outer-join reorder, join/agg reorder, common table expressions (CTE), materialized views, dynamic filter push-down, magic set optimization, and other cost-based techniques. Additionally, it integrates property enforcement for distributed planning.</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="query-scheduling">Query Scheduling<a href="#query-scheduling" class="hash-link" aria-label="Query Scheduling的直接链接" title="Query Scheduling的直接链接">​</a></h3><p>ByConity currently supports two query scheduling strategies: Cache-aware scheduling and Resource-aware scheduling.</p><ul><li>The <strong>cache-aware scheduling</strong> focuses on scenarios where computing and storage are separated. Its objective is to maximize cache utilization and minimize cold reads. This strategy aims to schedule tasks to nodes that have corresponding data caches, enabling computations to leverage the cache and improve read and write performance. Additionally, considering the dynamic expansion and contraction of the system, cache-aware scheduling strives to minimize the impact of cache failure on query performance when the computing group's topology changes.</li><li><strong>Resource-aware scheduling</strong> analyzes the resource usage of different nodes within the computing group across the entire cluster. It performs targeted scheduling to optimize resource utilization. Moreover, resource-aware scheduling incorporates flow control mechanisms to ensure rational resource utilization and prevent negative effects caused by overload, such as system downtime.</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="computing-group">Computing Group<a href="#computing-group" class="hash-link" aria-label="Computing Group的直接链接" title="Computing Group的直接链接">​</a></h3><p>ByConity enables different tenants to utilize distinct computing resources, as depicted in Figure 5. With ByConity's architecture, implementing features like multi-tenant isolation and read-write separation becomes straightforward. Each tenant can leverage separate computing groups to achieve multi-tenant isolation and support read-write separation. The computing groups can be dynamically expanded and contracted on-demand, ensuring efficient resource utilization. During periods of low resource utilization, resource sharing can be employed, allowing computing groups to be allocated to other tenants to maximize resource utilization and minimize costs.</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="virtual-file-system">Virtual File System<a href="#virtual-file-system" class="hash-link" aria-label="Virtual File System的直接链接" title="Virtual File System的直接链接">​</a></h3><p>The virtual file system module serves as an intermediary layer for data reading and writing. ByConity has optimized this module to provide a "storage as a service" capability to other modules. The virtual file system offers a unified file system abstraction, shielding the underlying different back-end implementations. It facilitates easy expansion and supports multiple storage systems, such as HDFS or object storage.</p><h3 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="cache-acceleration">Cache Acceleration<a href="#cache-acceleration" class="hash-link" aria-label="Cache Acceleration的直接链接" title="Cache Acceleration的直接链接">​</a></h3><p>ByConity utilizes caching to accelerate query processing. Under the computing-storage separation architecture, cache acceleration is performed in both the metadata and data dimensions. In the metadata dimension, ByConity caches Table and Partition information in the memory of the server-side (ByConity Server). In the data dimension, cache acceleration occurs on the Worker side within the computing group. This hierarchical caching mechanism utilizes both memory and disk, with Mark collection serving as the cache granularity. These caching strategies effectively enhance query speed and performance.</p><h1>How to Deploy Byconity</h1><p>ByConity currently supports four acquisition and deployment modes. Community developers are welcome to use them and submit issues to us:</p><ul><li>Stand-alone Docker: ByConity provides a Docker deployment option, which can be accessed at <a href="https://github.com/ByConity/byconity-docker" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/byconity-docker</a></li><li>K8s cluster deployment: ByConity also supports deployment on Kubernetes clusters. The deployment guide for Kubernetes can be found at <a href="https://github.com/ByConity/byconity-deploy" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/byconity-deploy</a></li><li>Physical machine deployment: If you prefer to deploy ByConity on physical machines, you can refer to the repository at <a href="https://github.com/ByConity/ByConity/tree/master/packages" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/ByConity/tree/master/packages</a></li><li>Source code compilation: You can compile the ByConity source code yourself. The source code repository can be accessed at <a href="https://github.com/ByConity/ByConity#build-byconity" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/ByConity#build-byconity</a></li></ul><h1>ByConity's Future Open-Source Plan</h1><p>ByConity includes several key milestones in its open-source community roadmap through 2023. These milestones are designed to enhance ByConity's functionality, performance, and ease of use. Among them, the development of new storage engines, support for more data types, and integration with other data management tools are some important areas of focus. We have listed the following directions, and created an issue on Github: <a href="https://github.com/ByConity/ByConity/issues/26" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/ByConity/issues/26</a>, inviting the community to join us to discuss co-development:</p><ul><li><strong>Performance improvement</strong>: ByConity aims to boost performance through various optimizations. This includes leveraging indexes for acceleration, such as Skip-index optimization, support for new Zorder-index and inverted indexes. ByConity will also focus on the construction and acceleration of external indexes, as well as the automatic recommendation and conversion of indexes. Continuous enhancements to the query optimizer and the implementation of a distributed caching mechanism are also part of the performance improvement efforts.</li><li><strong>Stability improvement</strong>: There are two aspects here.<ul><li>One is to support resource isolation in more dimensions. ByConity is committed to improving stability by extending resource isolation capabilities in multiple dimensions, thereby providing better multi-tenant support. </li><li>The second direction is to enrich metrics and improve observability and problem diagnosis capabilities, ensuring a stable and reliable experience for users.</li></ul></li><li><strong>Enterprise-level feature enhancements</strong>: ByConity aims to introduce finer-grained authority control, improve data security-related functions such as backup, recovery, and data encryption and continue to explore techniques for deep compression of data to save storage costs.</li><li><strong>Ecosystem compatibility improvement</strong>: ByConity plans to expand its compatibility with various storage systems, including popular object storage solutions like S3 and TOS. It plans to enhance the overall compatibility and integration capabilities, facilitating seamless integration with other tools and frameworks. Moreover, it aims to support data lake federation queries, enabling interaction with technologies like Hudi, Iceberg, and more.</li></ul><h1>Working with the Community</h1><p>Since the release of the Beta version, ByConity has received support from numerous enterprise developers, including Huawei, Electronic Cloud, Zhanxinzhanli, Tianyi Cloud, Vipshop, and Transsion Holdings. These organizations have actively contributed by deploying ByConity in their respective environments, undergoing TPC-DS verification, and conducting tests in their business scenarios. The results have been promising, and their feedback has provided valuable insights for improvement, which we greatly appreciate.</p><p>We are delighted to receive the ideas and willingness of community partners to build together. We have already initiated joint development efforts with Huawei Terminal Cloud. Our collaborative endeavors will focus on various areas, such as Kerberos authentication, ORC support, and integration with S3 storage. </p><p>If you are interested in joining our community and participating in the development of ByConity, we invite you to visit our GitHub repository at <a href="https://github.com/ByConity/ByConity" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/ByConity</a>. You can find more information and details about our ongoing projects, contribute your ideas, and collaborate with us to further enhance ByConity. To get involved, simply scan the QR code provided below to join our <a href="https://discord.gg/V4BvTWGEQJ" target="_blank" rel="noopener noreferrer">Discord</a> or follow us on <a href="https://twitter.com/ByConity" target="_blank" rel="noopener noreferrer">Twitter</a>. </p><p><img loading="lazy" alt="ByConity Discord Group" src="/zh-cn/assets/images/f5-ByConity-Discord-QR-Code-3df954930bbe1e0f0f6cb1a197c4de77.jpeg" width="400" height="400" class="img_astN">
ByConity Discord Group</p><p><img loading="lazy" alt="ByConity Twitter" src="/zh-cn/assets/images/f5-ByConity-Twitter-QR-code-1ec0e9ecaae58ba7e5d33acbeadfb84c.jpeg" width="400" height="400" class="img_astN">
ByConity Twitter</p><h1>Summary</h1><p>In summary, ByConity is an open source cloud-native data warehouse that offers features such as read-write separation, elastic expansion and contraction, tenant resource isolation, and strong data read and write consistency. It utilizes a computing-storage separation architecture and leverages optimizations from mainstream OLAP engines to deliver excellent read and write performance. As ByConity continues to evolve and improve, it aims to become a key tool for cloud-native data warehousing in the future.</p>]]></content:encoded>
            <category>video introduction</category>
            <category>docusaurus</category>
        </item>
        <item>
            <title><![CDATA[ByConity -- An Open source Cloud-native Data Warehouse]]></title>
            <link>https://byconity.github.io/zh-cn/blog/byconity-an-opensource-cloudnative-data-warehouse-post</link>
            <guid>https://byconity.github.io/zh-cn/blog/byconity-an-opensource-cloudnative-data-warehouse-post</guid>
            <pubDate>Mon, 10 Apr 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Introduction to ByConity]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="introduction-to-byconity">Introduction to ByConity<a href="#introduction-to-byconity" class="hash-link" aria-label="Introduction to ByConity的直接链接" title="Introduction to ByConity的直接链接">​</a></h2><p>ByConity is an open-source cloud-native data warehouse that adopts a storage-computing separation architecture. It supports several key features, including separation of storage and computing, elastic expansion and contraction, isolation of tenant resources, and strong consistency of data read and write. By utilizing mainstream OLAP engine optimizations, such as column storage, vectorized execution, MPP execution, query optimization, etc., ByConity can provide excellent read and write performance.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="byconitys-history">ByConity's History<a href="#byconitys-history" class="hash-link" aria-label="ByConity's History的直接链接" title="ByConity's History的直接链接">​</a></h2><p>The background of ByConity can be traced back to 2018 when ByteDance began to use ClickHouse internally. Due to the development of business, the scale of data has become larger and larger to serve a large number of users. However, because ClickHouse is a Shared-Nothing architecture, each node is independent and does not share storage resources, so computing resources and storage resources are tightly coupled. This leads to a higher cost of expansion and contraction, and involves data migration, which prevents real-time and on-demand expansion and contraction, resulting in a waste of resources. Furthermore, the tightly coupled architecture of ClickHouse will cause multi-tenants to interact with each other in the shared cluster. In addition, because reading and writing are completed on one node, reading and writing are affected. Finally, ClickHouse does not support performance in complex queries such as multi-table join. Based on these pain points, the ByConity project was launched in January 2020.</p><p>The ByConity team hopes to give the project back to the community and improve it through the power of open source. In January 2023, ByConity was officially open-sourced, and the beta version was released.</p><p><img loading="lazy" alt="Figure 1 ByteDance ClickHouse usage" src="/zh-cn/assets/images/f1-byte-clickhouse-usage-cefbe6a269f39c214219df180aef1560.png" width="1652" height="896" class="img_astN"></p><p>Figure 1: ByteDance ClickHouse Usage</p><h1>Features of ByConity</h1><p>ByConity has several key features that make it a powerful open-source cloud-native data warehouse.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="storage-computing-separation">Storage-Computing Separation<a href="#storage-computing-separation" class="hash-link" aria-label="Storage-Computing Separation的直接链接" title="Storage-Computing Separation的直接链接">​</a></h2><p>One of the critical advantages of ByConity is its storage-computing separation architecture, which enables read-write separation and elastic scaling. This architecture ensures that read and write operations do not affect each other, and computing resources and storage resources can be independently expanded and contracted on demand, ensuring efficient resource utilization. ByConity also supports multi-tenant resource isolation, making it suitable for multi-tenant environments.</p><p><img loading="lazy" alt="Figure 2: ByConity storage-computing separation to achieve multi-tenant isolation" src="/zh-cn/assets/images/f2-storage-computing-separation-51a4fd7a3073479cf431a48278204cd5.png" width="1203" height="561" class="img_astN">
Figure 2: ByConity storage-computing separation to achieve multi-tenant isolation</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="elastic-scaling">Elastic Scaling<a href="#elastic-scaling" class="hash-link" aria-label="Elastic Scaling的直接链接" title="Elastic Scaling的直接链接">​</a></h2><p>ByConity supports flexible expansion and contraction, enabling real-time and on-demand expansion and contraction of computing resources, ensuring efficient use of resources.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="resource-isolation">Resource Isolation<a href="#resource-isolation" class="hash-link" aria-label="Resource Isolation的直接链接" title="Resource Isolation的直接链接">​</a></h2><p>ByConity isolates the resources of different tenants, ensuring that tenants are not affected by each other.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="strong-data-consistency">Strong Data Consistency<a href="#strong-data-consistency" class="hash-link" aria-label="Strong Data Consistency的直接链接" title="Strong Data Consistency的直接链接">​</a></h2><p>ByConity ensures strong consistency of data read and write, ensuring that data is always up to date with no inconsistencies between reads and writes.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="high-performance">High Performance<a href="#high-performance" class="hash-link" aria-label="High Performance的直接链接" title="High Performance的直接链接">​</a></h2><p>ByConity adopts mainstream OLAP engine optimizations, such as column storage, vectorized execution, MPP execution, query optimization, etc., ensuring excellent read and write performance.</p><h1>ByConity's Technical Architecture</h1><p>ByConity's architecture is divided into three layers:</p><ol><li>Service access layer: Responsible for client data and service access, i.e., ByConity Server</li><li>Computing group: ByConity's computing resource layer, where each Virtual Warehouse is a computing group</li><li>Data storage: Distributed file system, such as HDFS, S3, etc.</li></ol><p><img loading="lazy" alt="Figure 3: ByConity&amp;#39;s architecture" src="/zh-cn/assets/images/f3-three-layer-architecture-403f695f8cd48cb936283a4c7b86553d.png" width="1483" height="809" class="img_astN">
Figure 3: ByConity's architecture</p><h1>Working Principle of ByConity</h1><p>ByConity is a powerful open-source cloud-native data warehouse that adopts a storage-computing separation architecture. In this section, we will examine the working principle of ByConity and the interaction process of each component of ByConity through the complete life cycle of a SQL.</p><p><img loading="lazy" alt="Figure 4: ByConity internal component interaction diagram" src="/zh-cn/assets/images/f4-internal-component-interaction-301b3d1e7d179142c3b1ce535f1ce105.png" width="1280" height="1210" class="img_astN">
Figure 4: ByConity internal component interaction diagram</p><p>Figure 4 above is the interaction diagram of ByConity components. The dotted line in the figure indicates the inflow of a SQL, the two-way arrow in the solid line indicates the interaction within the component, and the one-way arrow indicates the data processing and output to the client.</p><p>ByConity's working principle can be divided into three stages:</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="stage-1-query-request">Stage 1: Query Request<a href="#stage-1-query-request" class="hash-link" aria-label="Stage 1: Query Request的直接链接" title="Stage 1: Query Request的直接链接">​</a></h2><p>The client submits a query request to the server, and the server first performs parsing, then analyzes and optimizes through analyzer and optimizer to generate a more efficient executable plan. Here, metadata MetaData is read, which is stored in a distributed KV. ByConity uses FoundationDB and reads the metadata through the Catalog.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="stage-2-plan-scheduling">Stage 2: Plan Scheduling<a href="#stage-2-plan-scheduling" class="hash-link" aria-label="Stage 2: Plan Scheduling的直接链接" title="Stage 2: Plan Scheduling的直接链接">​</a></h2><p>ByConity submits the executable plan generated by the analysis and optimizer to the scheduler (Plan Scheduler). The scheduler obtains idle computing resources by accessing the resource manager and decides which nodes to schedule query tasks for execution.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="stage-3-query-execution">Stage 3: Query Execution<a href="#stage-3-query-execution" class="hash-link" aria-label="Stage 3: Query Execution的直接链接" title="Stage 3: Query Execution的直接链接">​</a></h2><p>Query requests are finally executed on ByConity's Worker, and the Worker will read data from the lowest-level Cloud storage and perform calculations by establishing a pipeline. Finally, the calculation results of multiple workers are aggregated by the server and returned to the client.</p><p>In addition to the above components, ByConity also has two main components, Time-stamp Oracle and Deamon Manager. The former ByConity supports transaction processing, and the latter manages and schedules some subsequent tasks.</p><h1>Main Component Library</h1><p>To better understand the working principle of ByConity, let's take a look at the main components of ByConity:</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="metadata-management">Metadata Management<a href="#metadata-management" class="hash-link" aria-label="Metadata Management的直接链接" title="Metadata Management的直接链接">​</a></h2><p>ByConity provides a highly available and high-performance metadata read and write service - Catalog Server. And ByConity supports complete transaction semantics (ACID). At the same time, we have made a better abstraction of the Catalog Server, making the back-end storage system pluggable. Currently, we support Apple's open-source FoundationDB, which can be expanded to support more back-end storage systems later.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="query-optimizer">Query Optimizer<a href="#query-optimizer" class="hash-link" aria-label="Query Optimizer的直接链接" title="Query Optimizer的直接链接">​</a></h2><p>The query optimizer is one of the cores of the database system. A good optimizer can greatly improve query performance. ByConity's self-developed optimizer improves optimization capabilities based on two directions:</p><ul><li>RBO: Rule-Based Optimization capability. Support: column pruning, partition pruning, expression simplification, subquery disassociation, predicate pushdown, redundant operator elimination, outer-JOIN to INNER-JOIN, operator pushdown storage, distributed operator splitting, etc.</li><li>CBO: Cost-Based Optimization capability. Support: Join Reorder, Outer-Join Reorder, Join/Agg Reorder, CTE, Materialized View, Dynamic Filter Push-Down, Magic Set, and other cost-based optimization capabilities. And integrate Property Enforcement for distributed planning.</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="query-scheduling">Query Scheduling<a href="#query-scheduling" class="hash-link" aria-label="Query Scheduling的直接链接" title="Query Scheduling的直接链接">​</a></h2><p>ByConity currently supports two query scheduling strategies: Cache-aware scheduling and Resource-aware scheduling.</p><ul><li>The cache-aware scheduling policy is aimed at scenarios where storage and computing are separated, aiming to maximize the use of the cache and avoid cold reads. The cache-aware scheduling strategy will try to schedule tasks to nodes with corresponding data caches, so that calculations can hit the cache and improve read and write performance.</li><li>Resource-aware scheduling perceives the resource usage of different nodes in the computing group in the entire cluster and performs targeted scheduling to maximize resource utilization. At the same time, it also performs flow control to ensure reasonable use of resources and avoid negative effects caused by overload, such as system downtime.</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="computing-group">Computing Group<a href="#computing-group" class="hash-link" aria-label="Computing Group的直接链接" title="Computing Group的直接链接">​</a></h2><p>ByConity supports different tenants to use different computing resources. Under ByConity's new architecture, it is easy to implement features such as multi-tenant isolation and read-write separation. Different tenants can use different computing groups to achieve multi-tenant isolation and support read-write separation. Due to the convenient expansion and contraction, the computing group can be dynamically expanded and contracted on demand to ensure efficient resource utilization. When resource utilization is not high, resource sharing can be carried out, and computing groups can be seconded to other tenants to maximize resource utilization and reduce costs.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="virtual-file-system">Virtual File System<a href="#virtual-file-system" class="hash-link" aria-label="Virtual File System的直接链接" title="Virtual File System的直接链接">​</a></h2><p>The virtual file system module is used as the middle layer of data reading and writing. ByConity has made a better package, exposing storage as a service to other modules to realize "storage as a service". The virtual file system provides a unified file system abstraction, shields different back-end implementations, facilitates expansion, and supports multiple storage systems, such as HDFS or object storage.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="cache-acceleration">Cache Acceleration<a href="#cache-acceleration" class="hash-link" aria-label="Cache Acceleration的直接链接" title="Cache Acceleration的直接链接">​</a></h2><p>ByConity performs query acceleration through caching. Under the architecture of separating storage and computing, ByConity performs cache acceleration in both metadata and data dimensions. In the metadata dimension, by caching in the memory of ByConity's Server side, table, and partition are used as granularity. In the data dimension, ByConity's Worker side, that is, the computing group, is used for caching, and the cache on the Worker side is hierarchical. At the same time, memory and disk are used, and the mark set is used as the cache granularity, thereby effectively improving the query speed.</p><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="how-to-obtain-and-deploy">How to Obtain and Deploy<a href="#how-to-obtain-and-deploy" class="hash-link" aria-label="How to Obtain and Deploy的直接链接" title="How to Obtain and Deploy的直接链接">​</a></h2><p>ByConity currently supports four acquisition and deployment modes. Community developers are welcome to use them and submit issues to us:</p><ul><li>Stand-alone version<ul><li>Use docker compose to pull up Reference: <a href="https://github.com/ByConity/byconity-docker" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/byconity-docker</a></li></ul></li><li>K8s cluster version mode<ul><li>Use K8s deployment reference: <a href="https://github.com/ByConity/byconity-deploy" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/byconity-deploy</a></li></ul></li><li>Physical machine deployment mode<ul><li>Deploy on a physical machine using the package manager: <a href="https://github.com/ByConity/ByConity/tree/master/packages" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/ByConity/tree/master/packages</a></li></ul></li><li>Source code compilation method<ul><li>Reference: <a href="https://github.com/ByConity/ByConity#build-byconity" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/ByConity#build-byconity</a></li></ul></li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_l3HI" id="byconitys-future-open-source-plan">ByConity's Future Open-Source Plan<a href="#byconitys-future-open-source-plan" class="hash-link" aria-label="ByConity's Future Open-Source Plan的直接链接" title="ByConity's Future Open-Source Plan的直接链接">​</a></h2><p>ByConity includes several key milestones in its open-source community roadmap through 2023. These milestones are designed to enhance ByConity's functionality, performance, and ease of use. Among them, the development of new storage engines, support for more data types, and integration with other data management tools are some important areas of focus. We have listed the following directions, and we have created an issue on Github: <a href="https://github.com/ByConity/ByConity/issues/26" target="_blank" rel="noopener noreferrer">https://github.com/ByConity/ByConity/issues/26</a>, inviting community partners to join us to discuss co-construction:</p><ul><li>In terms of performance improvement: ByConity hopes to continue to improve performance, and here are three technical directions. One is to use indexes for acceleration, which includes four aspects:<ul><li>Optimize the existing skip index;</li><li>Explore the implementation of new index research, such as zorder-index and inverted index;</li><li>ByConity builds and accelerates Hive table indexes</li><li>Index recommendation and conversion, lowering the threshold for users to use
The second is the continuous optimization of the query optimizer; the third is that ByConity's cache mechanism is local, and each computing group can only access its own cache. In the future, it is hoped to implement a distributed cache mechanism to further improve the cache hit rate.</li></ul></li><li>Stability improvement: There are two aspects here. One is to support resource isolation in more dimensions. Currently, it only supports resource isolation in the computing group dimension. In the next step, resource isolation will also be supported on the server side, providing better end-to-end Guaranteed multi-tenancy capability. The second direction is to enrich metrics and improve observability and problem diagnosis capabilities.</li><li>Enterprise-level feature enhancements: We hope to achieve more detailed permission control, including column-level permission control. The other is to improve the functions related to data security, such as data backup and recovery and data end-to-end encryption. Finally, we continue to explore the deep compression of data to save storage costs.</li><li>Ecological compatibility improvement: This direction is the most important point. ByConity plans to support more types of storage backends, such as AWS's S3, Volcano Engine's object storage, etc. In terms of improving ecological compatibility, it includes integration with some drivers and some open source software. At the same time, we also hope to support federated queries of data lakes, such as Hudi, Iceberg, etc.</li></ul><p>In short, ByConity is an open source cloud-native data warehouse that provides read-write separation, elastic expansion and contraction, tenant resource isolation, and strong consistency of data read and write. Its storage-computing separation architecture, combined with mainstream OLAP engine optimization, ensures excellent read and write performance. As ByConity continues to develop and improve, it is expected to become an important tool for cloud-native data warehouses in the future.</p><p>We have a video that introduces ByConity in detail, including a demo of ByConity. If you need more information, you can check the following link: <a href="https://www.bilibili.com/video/BV15k4y1b7pw/?spm_id_from=333.999.0.0&amp;vd_source=71f3be2102fec1a0171b49a530cefad0" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV15k4y1b7pw/?spm_id_from=333.999.0.0&amp;vd_source=71f3be2102fec1a0171b49a530cefad0</a></p><p>Scan the QR code to reply <!-- -->[name + QR code]<!-- --> Join the ByConity communication group to get more project dynamics and activity information.</p><p><img loading="lazy" alt="ByConity Community QR Code" src="/zh-cn/assets/images/f5-byconity-community-qrcode-b75b8db64319bf889444dc7b4fd21124.png" width="621" height="627" class="img_astN">
ByConity Community QR Code</p>]]></content:encoded>
            <category>video introduction</category>
            <category>docusaurus</category>
        </item>
    </channel>
</rss>